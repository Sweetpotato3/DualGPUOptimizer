{
  "Mixtral 8x7B": {
    "path": "",
    "ctx": 32768,
    "n_layers": 32,
    "n_kv_heads": 8,
    "head_dim": 128,
    "moe_factor": 1.05
  },
  "Llama 2 7B": {
    "path": "",
    "ctx": 16384,
    "n_layers": 32,
    "n_kv_heads": 32,
    "head_dim": 128
  },
  "Llama 2 13B": {
    "path": "",
    "ctx": 8192,
    "n_layers": 40,
    "n_kv_heads": 40,
    "head_dim": 128
  },
  "Mistral 7B": {
    "path": "",
    "ctx": 16384,
    "n_layers": 32,
    "n_kv_heads": 8,
    "head_dim": 128
  }
}
