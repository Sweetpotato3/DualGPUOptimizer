"""
Stateless helpers to turn GPU info → split strings, env files, commands.
"""
from __future__ import annotations

import os
import textwrap
from pathlib import Path
from typing import List

from dualgpuopt.gpu_info import GPU


def split_string(gpus: List[GPU]) -> str:
    """Generate a comma-separated string of GPU memory sizes in GB."""
    return ",".join(str(g.mem_total_gb) for g in gpus)


def tensor_fractions(gpus: List[GPU]) -> list[float]:
    """Calculate tensor parallel fractions based on relative GPU memory sizes."""
    top = max(g.mem_total for g in gpus)
    return [round(g.mem_total / top, 3) for g in gpus]


def make_env_file(gpus: List[GPU], filename: Path) -> Path:
    """
    Create an environment file with optimal GPU configuration.

    Args:
        gpus: List of GPU objects
        filename: Path to save the environment file

    Returns:
        Path to the created file
    """
    env = textwrap.dedent(
        f"""
        # Auto‑generated by dualgpuopt
        CUDA_VISIBLE_DEVICES={','.join(str(g.index) for g in gpus)}
        NCCL_P2P_DISABLE=0
        NCCL_IB_DISABLE=1
        NCCL_NET_GDR_LEVEL=2
        OMP_NUM_THREADS={os.cpu_count()//2}
        PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:512
        """
    ).strip()
    filename.write_text(env, encoding="utf-8")
    return filename


def llama_command(model_path: str, ctx: int, split: str) -> str:
    """
    Generate command for llama.cpp with optimized parameters.

    Args:
        model_path: Path to the model file
        ctx: Context size
        split: GPU memory split string

    Returns:
        Formatted command string
    """
    return (
        f"./main -m {model_path} "
        f"--gpu-split {split} --n-gpu-layers 999 --ctx-size {ctx}"
    )


def vllm_command(model_path: str, tp: int) -> str:
    """
    Generate command for vLLM with optimized parameters.

    Args:
        model_path: Path to the model file
        tp: Tensor parallel size (usually number of GPUs)

    Returns:
        Formatted command string
    """
    return (
        "python -m vllm.entrypoints.openai.api_server "
        f"--model {model_path} --dtype float16 "
        f"--tensor-parallel-size {tp} --gpu-memory-utilization 0.9"
    )