---
description: GPU memory distribution and optimization algorithms for ML model execution across multiple GPUs
globs: **/gpu_info.py,**/optimizer.py,**/telemetry.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithm
**Importance Score: 95**

Core algorithm for distributing model memory across multiple GPUs:

1. GPU Memory Collection
- Collects detailed GPU memory information using NVML
- Tracks total memory, free memory, and utilization per GPU
- File: `gpu_info.py`

2. Tensor Parallel Split Calculation
- Calculates optimal tensor fractions based on available GPU memory sizes
- Generates memory split strings for model distribution
- File: `optimizer.py`

## Real-time GPU Telemetry
**Importance Score: 85**

Telemetry collection system for runtime optimization:

1. Metrics Collection
- Continuously polls GPU metrics including:
  - Memory usage
  - PCIe throughput
  - Temperature
  - Power consumption
  - Clock speeds
- File: `telemetry.py`

2. Data Streaming
- Implements queue-based streaming of telemetry data
- Provides real-time GPU utilization data for optimization decisions
- File: `telemetry.py`

## Command Generation System
**Importance Score: 90**

Optimization command generation for ML frameworks:

1. Framework-Specific Commands
- Generates optimized commands for:
  - llama.cpp with memory splits
  - vLLM with tensor parallel sizes
- File: `optimizer.py`

2. Environment Configuration
- Produces environment files with GPU-specific settings
- Configures tensor parallel parameters based on available GPU memory
- File: `optimizer.py`

$END$