---
description: GPU memory management and tensor parallelism optimization algorithms for multi-GPU LLM deployments
globs: **/optimizer.py,**/gpu_info.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithm (Importance: 95)
- Tensor fraction calculation based on relative GPU memory capacities
- Each GPU's memory allocation weighted against highest capacity GPU
- Automated tensor parallel size determination for vLLM deployments
- Fixed 90% memory utilization policy for vLLM framework

```python
def tensor_fractions(gpus):
    top = max(g.mem_total for g in gpus)
    return [round(g.mem_total / top, 3) for g in gpus]
```

## GPU Resource Validation (Importance: 85)
- Enforces minimum 2-GPU requirement for optimization
- Validates memory override values as integers
- Supports dynamic GPU memory overrides through environment variables
- Implements mock GPU mode with simulated 3090/3080 configurations

## Memory Split Optimization (Importance: 90)
- Custom memory splitting algorithm for model distribution
- Calculates optimal splits based on available VRAM across GPUs
- Handles framework-specific memory requirements (llama.cpp vs vLLM)
- Generates optimized deployment commands with memory parameters

Key Files:
- dual_gpu_optimizer/dualgpuopt/optimizer.py
- dual_gpu_optimizer/dualgpuopt/gpu_info.py

$END$