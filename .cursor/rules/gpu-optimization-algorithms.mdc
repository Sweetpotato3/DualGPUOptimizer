---
description: GPU memory optimization, tensor parallel splits calculation, and memory allocation strategies
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

The GPU optimization logic implements several key algorithms for memory distribution and parallel model execution:

### GPU Memory Distribution
- Calculates optimal memory splits across GPUs based on total VRAM capacity and model size
- Determines tensor parallel ratios per GPU using heuristic bucketing based on layer memory requirements
- Implements adaptive memory quotas that reserve space for activation memory and tensors
- File: `gpu_info.py`

### Layer Distribution Algorithm
- Profiles execution time of model layers to identify compute-heavy vs memory-heavy components
- Uses latency-aware mapping to distribute layers across GPUs while respecting memory constraints
- Generates a JSON device map specifying optimal layer placement
- File: `layer_balance.py`

### Context Size Calculation
- Implements heuristic formula for safe context length based on:
  - Available GPU memory
  - Number of model layers and parameters
  - KV cache requirements
  - Precision bits
  - MOE routing factor
- Reserves memory buffer to prevent OOM errors
- File: `ctx_size.py`

### Telemetry-Based Optimization
- Collects real-time GPU metrics:
  - Memory usage
  - Utilization rates  
  - PCIe bandwidth
  - Temperature
- Uses metrics to dynamically adjust memory splits
- Enables reactive optimization based on actual GPU load
- File: `optimizer.py`

Importance Scores:
- GPU Memory Distribution: 95 (Core distribution logic)
- Layer Distribution: 90 (Critical for model performance)
- Context Size Calculation: 85 (Key for preventing OOM)
- Telemetry Optimization: 75 (Important but supplementary)

$END$