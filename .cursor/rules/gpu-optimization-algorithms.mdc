---
description: Documents GPU optimization logic for machine learning model deployment across multiple GPUs, focusing on memory management and model partitioning
globs: **/gpu/**,**/optimizer/**,**/memory/**,**/batch/**
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Distribution Logic
Key algorithms for partitioning models across multiple GPUs, located in optimizer.py:

```python
def calculate_tensor_splits(gpus, model_size):
    memory_ratios = [gpu.available_memory/total_memory for gpu in gpus]
    return adjust_splits_for_overhead(memory_ratios, model_size) 
```

Determines optimal tensor parallel splits based on:
- Available GPU memory ratios
- Model architecture requirements  
- Memory overhead estimations
- Layer-wise distribution patterns

## Memory Management
Memory allocation and tracking in gpu_info.py:
- Implements dynamic memory quota calculation per GPU
- Tracks memory pressure through watchdog system
- Handles OOM recovery with progressive fallback strategy:
  1. Cache clearing
  2. Batch size reduction  
  3. Layer redistribution
  4. Process restart

## Model Layer Distribution
Layer balancing algorithms in layer_balance.py:
- Multi-sequence profiling (64/1024 token ratios)
- Weighted workload distribution considering:
  - Layer computational intensity
  - Memory transfer costs
  - GPU memory constraints
- Block optimization to minimize cross-GPU transfers

## Smart Batch Processing
Batch optimization system in batch/smart_batch.py:
- Token-aware batch assembly
- Dynamic sizing based on:
  - Available GPU memory
  - Model architecture
  - Sequence length distribution
- Backpressure mechanisms to prevent OOM

The architecture specifically optimizes for language model inference across multiple GPUs, with core focus on memory management and workload distribution.

$END$