---
description: GPU memory allocation and optimization algorithms for multi-GPU model inference
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/mpolicy.py,**/batch/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core GPU Memory Distribution (Importance: 95)
File: dual_gpu_optimizer/dualgpuopt/optimizer.py

- Tensor split calculation based on relative GPU memory capacities
- Adaptive memory distribution for heterogeneous GPU configurations 
- Framework-specific memory allocation strategies for llama.cpp and vLLM
- Dynamic adjustment of splits based on model architecture requirements

## Layer Distribution Algorithm (Importance: 90)
File: dual_gpu_optimizer/dualgpuopt/layer_balance.py

- Latency-aware layer distribution across multiple GPUs
- Dual sequence length profiling (64/1024 tokens) with 20/80 weighting
- Quota-based GPU workload balancing with memory constraints
- Device mapping optimization using layer performance profiles

## Smart Batching System (Importance: 85)
File: dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py

- Token-based dynamic batching with configurable size limits
- Back-pressure mechanism preventing RAM overflow 
- Automatic OOM recovery with cache clearing
- Length-aware scheduling for GPU utilization optimization

## Mixed Precision Policies (Importance: 80)
File: dual_gpu_optimizer/dualgpuopt/mpolicy.py

- Custom autocast implementation for inference optimization
- Specialized handling of LayerNorm/softmax/residual operations
- Smart dtype management for precision requirements
- Gradient scaling policy for training scenarios

## Context Size Calculator (Importance: 75)
File: dual_gpu_optimizer/dualgpuopt/ctx_size.py

- Heuristic algorithm for maximum safe context length
- Factors in layers, KV heads, dimensions, precision bits
- MoE factor adjustment for models like Mixtral
- Dynamic adjustment based on available GPU memory

$END$