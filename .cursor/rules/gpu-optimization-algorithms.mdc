---
description: Specifies GPU optimization algorithms and memory management strategies for dual GPU setups running large language models
globs: dualgpuopt/optimizer.py,dualgpuopt/gpu_info.py,dualgpuopt/memory/*.py,dualgpuopt/engine/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

Core GPU Memory Distribution Logic:

1. Layer Distribution Algorithm (dualgpuopt/optimizer.py):
- Custom layer balancing algorithm for distributing transformer layers across dual GPUs
- Weighted sequence profiling with 20/80 ratio between short (64 tokens) and long (1024 tokens) sequences
- Minimum block size enforcement of 3 layers to minimize cross-GPU communication
- Strategic block merging based on neighbor size ratios
- Device mapping rules:
  * Input embeddings -> first GPU
  * Output components -> last GPU
  * Layer assignments based on weighted performance ratios

2. Memory Split Optimization (dualgpuopt/gpu_info.py):
- Dynamic tensor parallel fraction calculation based on relative GPU memory sizes
- Model-specific memory allocation:
  * 70B models: 70/30 split with 40-layer distribution
  * Mixtral: 60/40 split with 16-layer segments
  * 13B models: 50/50 split with 20-layer segments
- Context size limits per model:
  * 70B: 4096 tokens max
  * Mixtral: 8192 tokens max
  * 13B: 12288 tokens max

3. Memory Management System (dualgpuopt/memory/profiler.py):
- Adaptive memory leak detection algorithm:
  * Analyzes growth patterns using sliding window
  * Leak threshold: 5MB absolute change
  * Cool-down period: 30s between alerts
  * Growth rate conversion to MB/s
- Inference session tracking with:
  * Pre/post memory baselines
  * Per-inference memory deltas
  * Token count correlation
  * Targeted leak analysis

Key components contain custom algorithms for:
- GPU layer distribution optimization
- Memory allocation strategies
- Leak detection and monitoring
- Model-specific resource management

The system focuses on optimizing large language model deployment across dual GPU configurations through intelligent resource allocation and monitoring.

$END$