---
description: Specification for GPU optimization algorithms focusing on memory allocation strategies, tensor parallel splitting, and load distribution across multiple GPUs.
globs: **/gpu_info.py,**/optimizer.py,**/gpu/*.py,**/memory/*.py,**/batch/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution System
**Importance Score: 95**

Core algorithm for distributing model layers and memory across multiple GPUs:

1. Dynamic Split Calculation:
- Calculates optimal tensor fractions based on relative GPU memory capacities
- Implements safety margins to prevent OOM conditions
- Includes 20% overhead factor for activation memory
- Handles asymmetric GPU configurations with minimum 20% allocation per GPU

2. Model-Specific Memory Estimation:
- Per-token memory calculation based on:
  - Number of layers and attention heads
  - Hidden size dimensions
  - KV cache requirements
  - MoE overhead for expert models
- Supports specialized architectures:
  - Mixtral models: 1.05x memory factor for gating
  - LLaMA 2: Dynamic KV head scaling
  - Mistral: Fixed 32 layer configuration

## Batch Processing Optimization
**Importance Score: 90**

Smart batching system with length-aware scheduling:
- Groups sequences by length similarity
- Implements dynamic batch sizing based on:
  - Available GPU memory
  - Model size requirements
  - 5% per-sequence memory allocation
  - Maximum 64 sequences per batch
- Backpressure system reduces batch sizes after OOM events

File paths:
```
dualgpuopt/optimizer.py
dualgpuopt/memory/profiler.py
dualgpuopt/batch/smart_batch.py
dualgpuopt/gpu_info.py
```

## Layer Balance Optimization
**Importance Score: 85**

Adaptive layer distribution algorithm:
- Two-phase profiling approach:
  - Short sequence (64 tokens): 20% weight
  - Long sequence (1024 tokens): 80% weight
- Dynamic quota system for VRAM allocation
- Minimum block size of 3 layers to prevent fragmentation
- Strategic merging of neighboring blocks based on performance metrics

Memory recovery system:
- Progressive memory reclamation:
  1. Cache clearing (CUDA cache)
  2. Batch size reduction (25% steps)
  3. Layer rebalancing
  4. Process termination as last resort

## Memory Growth Prediction
**Importance Score: 80**

Memory usage projection system:
- Linear regression on recent memory patterns
- Token-based growth rate calculation
- Model-specific overhead factors:
  - Base model memory (1GB per billion parameters)
  - KV cache scaling (2 bytes per token per billion parameters)
  - 20% activation buffer
- Adaptive threshold adjustment based on model architecture

$END$