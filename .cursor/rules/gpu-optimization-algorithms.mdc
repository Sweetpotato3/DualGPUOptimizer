---
description: Specification for GPU memory distribution and optimization algorithms for multi-GPU LLM deployments
globs: **/optimizer.py,**/gpu_info.py,**/telemetry.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Memory Distribution Algorithm (Importance: 95)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`

The GPU split calculation algorithm determines optimal memory distribution across multiple GPUs:
- Computes tensor fractions based on available GPU memory capacities
- Generates split configuration strings for framework-specific tensor parallelism
- Adjusts memory allocation ratios based on detected GPU capabilities

## GPU Detection and Query System (Importance: 85)
`dual_gpu_optimizer/dualgpuopt/gpu_info.py`

Parallelized GPU information retrieval system:
- Concurrent querying of multiple GPUs using ThreadPoolExecutor
- GPU data encapsulation in structured format including index, name, memory stats
- Direct NVML integration for accurate hardware-level information

## Real-time Memory Monitoring (Importance: 80)
`dual_gpu_optimizer/dualgpuopt/telemetry.py`

GPU telemetry system for memory optimization:
- NVML-based polling of GPU metrics for memory usage patterns
- Memory threshold monitoring for optimization adjustments
- PCIe throughput tracking to validate memory distribution effectiveness

## Memory Allocation Strategy (Importance: 90)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`

Framework-specific memory allocation strategies:
- Generates environment configurations for CUDA device visibility
- Computes optimal tensor parallel splits based on model requirements
- Adjusts memory allocation based on framework-specific requirements (llama.cpp vs vLLM)

## Memory Configuration Output (Importance: 75)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`

Configuration generation system:
- Environment file generation with GPU-specific memory settings
- Framework-specific command generation incorporating memory splits
- Dynamic adjustment of memory configurations based on model context size

$END$