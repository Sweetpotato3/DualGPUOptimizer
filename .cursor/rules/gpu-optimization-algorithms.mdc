---
description: GPU memory allocation algorithms and optimization strategies for deep learning model inference on dual GPU setups
globs: **/optimizer.py,**/gpu_info.py,**/gpu/memory/*.py,**/gpu/telemetry/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core GPU Memory Management

The memory distribution system implements specialized algorithms for allocating model layers and tensors across dual GPU configurations:

### Memory Split Calculation
- Uses relative VRAM sizes of GPUs to determine optimal tensor splits
- Implements safety margins (2GB system, 2x KV cache, 20% tensor parallel)
- Dynamically adjusts splits based on model architecture and quantization level
- Minimum 20% allocation per GPU to maintain stable tensor parallelism

### Layer Distribution Algorithm
- Balances transformer layers across GPUs based on memory and latency profiles
- Weights layer performance by sequence length (80% long sequences, 20% short)
- Block-aware distribution to minimize cross-GPU communication
- Respects position-dependent performance variations (±20%)

### Memory Usage Calculation 
- Model-specific base memory estimation
- KV cache scaling with context length and batch size  
- Dynamic overhead factoring for different attention mechanisms:
  - MQA: 0.25x overhead factor
  - GQA: 0.5x overhead factor 
  - Standard: 1.0x overhead factor
- Mixture of Experts (MoE) models: Expert count scaling

### Context Length Optimization
- Maximum safe context calculation based on:
  - Available GPU memory
  - Model architecture (layers, heads, dimensions) 
  - Precision level (FP16/INT8/INT4)
  - MoE scaling factors
- Power-of-2 rounding for large contexts (≥4096)
- 128-token multiples for smaller contexts

## File Paths
- `dualgpuopt/optimizer.py`: Core memory split algorithms
- `dualgpuopt/layer_balance.py`: Layer distribution logic
- `dualgpuopt/ctx_size.py`: Context length calculation

$END$