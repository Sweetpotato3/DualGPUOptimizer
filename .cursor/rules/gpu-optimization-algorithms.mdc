---
description: Specification for GPU memory optimization, tensor parallel splits, and memory allocation algorithms
globs: **/gpu_info.py,**/optimizer.py,**/layer_balance.py,**/ctx_size.py,**/batch/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithms
- Calculates optimal tensor splits between GPUs based on available VRAM
- Implements parallel GPU probing to gather real-time memory metrics
- Dynamically adjusts memory quotas based on model size and precision settings

File: `dualgpuopt/gpu_info.py`

## Layer Balancing System
- Redistributes model layers across GPUs for optimal latency
- Profiles layer execution times to determine optimal splits
- Generates adaptive layer-to-GPU mapping saved as JSON
- Respects configured VRAM quotas while balancing

File: `dualgpuopt/layer_balance.py`

## Context Size Management
- Calculates maximum safe context length based on:
  - Available GPU memory
  - Model parameters
  - Chosen precision settings
- Adjusts context dynamically based on current GPU load

File: `dualgpuopt/ctx_size.py`

## Smart Batching Logic
- Length-aware batching for inference requests
- Implements configurable backlog limits to prevent RAM overflow
- Handles back-pressure and automatic retries on GPU OOM errors
- Uses pluggable bucket policies:
  - Power of 2 bucketing
  - Token ratio bucketing

File: `dualgpuopt/batch/smart_batch.py`

## Memory Distribution Commands
- Applies overclocking settings with rollback capability
- Handles errors with centralized error service
- Saves applied settings to persistent configuration
- Enables mock GPU mode for testing

File: `dualgpuopt/commands/gpu_commands.py`

$END$