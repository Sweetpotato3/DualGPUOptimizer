---
description: For analyzing GPU optimization algorithms including memory management, tensor splits, and CUDA process coordination
globs: 
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithms
File: dualgpuopt/optimizer.py

Primary GPU memory distribution logic:
- Calculates optimal tensor splits based on VRAM ratios between GPUs
- Dynamically adjusts splits based on base model size and KV cache requirements
- Maintains 10% safety margin for system overhead

Memory distribution formula:
```python
split_ratio = gpu0_vram / (gpu0_vram + gpu1_vram)
gpu0_layers = int(total_layers * split_ratio)
gpu1_layers = total_layers - gpu0_layers
```

## Tensor Parallel Split Optimization 
File: dualgpuopt/memory/profiler.py

Optimizes model layer distribution:
- Maps transformer layers to GPUs based on memory availability 
- Implements backpressure system for OOM handling
- Progressive batch size reduction (25% steps) on OOM events
- Gradual recovery after 5 successful batches

## GPU Memory Reset Logic
File: dualgpuopt/vram_reset.py

Memory reclamation strategies:
- CACHE_ONLY: Basic cache clearing
- CLOCK_RESET: GPU clock manipulation
- FULL_RESET: Complete tensor cleanup 
- SYSTEM_CMD: OS-specific memory commands

## Smart Batch Sizing
File: dualgpuopt/batch/smart_batch.py

Batch optimization logic:
- Length-aware sequence grouping
- Dynamic batch size calculation based on memory profiles
- Automatic backpressure on OOM events
- Progressive recovery with successful batch tracking

## Memory Event Classification
File: dualgpuopt/memory/profiler.py

Event categorization system:
- ALLOCATION/DEALLOCATION: Memory changes >1MB
- GROWTH_SPIKE: Rapid memory increases  
- LEAK_DETECTED: Sustained growth patterns
- INFERENCE_START/END: Session boundaries

$END$