---
description: Specification for GPU memory optimization algorithms and tensor parallel calculations for dual-GPU LLM inference
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/smart_batch.py
alwaysApply: false
---


# gpu-optimization-algorithms

GPU Memory Distribution Algorithm (optimizer.py):
- Tensor split calculation based on relative GPU memory capacities
- Dynamic memory fraction allocation: tensor_fraction = gpu_memory / max_gpu_memory
- GPU-specific command generation for llama.cpp and vLLM frameworks
- Automatic tensor parallelism degree calculation

Layer Balancing System (layer_balance.py):
Importance Score: 95
- Adaptive latency-based layer distribution across GPUs
- Dual sequence length profiling (64/1024 tokens) with weighted scoring
- Layer placement optimization using 0.2 * short_seq + 0.8 * long_seq formula
- VRAM quota enforcement during distribution

Context Size Calculator (ctx_size.py):
Importance Score: 90
- Maximum safe context length calculation considering:
  - n_layers, n_kv_heads, head_dim parameters
  - Precision type adjustments (16-bit, 8-bit QLoRA, 4-bit GPTQ)
  - MoE factor handling for Mixtral-type models
  - Dynamic safety margin calculation

Smart Batching Algorithm (smart_batch.py):
Importance Score: 85
- Length-aware GPU inference scheduling
- Token-based dynamic batch size optimization
- Automatic OOM recovery with cache clearing
- Power-of-two bucketing strategy for sequence grouping
- Token ratio maintenance between buckets (1.5x sizing)

File paths containing core algorithms:
- dualgpuopt/optimizer.py
- dualgpuopt/layer_balance.py
- dualgpuopt/ctx_size.py
- dualgpuopt/batch/smart_batch.py

$END$