---
description: Specification for GPU memory distribution algorithms, tensor parallel splits, and memory allocation strategies
globs: **/optimizer.py,**/gpu_info.py,**/telemetry.py
alwaysApply: false
---


# gpu-optimization-algorithms

### GPU Memory Distribution (Importance: 95)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`
- Calculates optimal memory splits across multiple GPUs for LLM workloads
- Generates split configuration strings representing total memory allocation per GPU
- Computes tensor fraction distributions relative to highest-memory GPU
- Creates environment configurations for balanced GPU memory utilization

### GPU Detection and Monitoring (Importance: 90)
`dual_gpu_optimizer/dualgpuopt/gpu_info.py`
- Integrates with NVML for real-time GPU discovery and information retrieval
- Implements fallback mock GPU data system for testing environments
- Tracks device names and memory usage across detected GPUs

### Telemetry Collection System (Importance: 85)
`dual_gpu_optimizer/dualgpuopt/telemetry.py`
- Gathers real-time GPU metrics including load, memory usage, and PCIe throughput
- Maintains continuous telemetry stream for dynamic resource monitoring
- Provides data for GPU utilization optimization decisions

### Machine Learning Framework Integration (Importance: 80)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`
- Generates framework-specific commands for llama.cpp with GPU split configurations
- Creates vLLM commands incorporating model paths and tensor parallel sizes
- Adjusts command parameters based on detected GPU count and memory distribution

### GPU Resource Management (Importance: 75)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`
- Sets CUDA visible devices based on optimization calculations
- Manages environment variables for GPU memory allocation
- Coordinates tensor parallel splits across available GPUs

$END$