---
description: Specifies GPU memory optimization algorithms, tensor parallel splits, and model deployment strategies across multiple GPUs.
globs: **/gpu_info.py,**/optimizer.py,**/gpu/**,**/memory/**,**/batch/**
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution System
- Implements dynamic memory allocation across multiple GPUs based on model architecture and available VRAM
- Calculates optimal tensor splits using GPU memory ratios and model layer characteristics
- Supports asymmetric memory configurations with different GPU capacities
- File: `dualgpuopt/optimizer.py`

## Layer Distribution Engine
- Distributes model layers across GPUs based on profiled performance metrics
- Handles MoE (Mixture of Experts) models with specialized expert distribution logic
- Implements weighted scoring system:
  * 20% weight for short sequences (64 tokens)
  * 80% weight for long sequences (1024 tokens)
- File: `dualgpuopt/layer_balance.py`

## Context Size Calculator
- Calculates maximum safe context length based on:
  * Available GPU memory
  * Model architecture parameters (layers, heads, dimensions)
  * MQA/GQA attention patterns
  * Memory overhead for MoE models
- File: `dualgpuopt/ctx_size.py`

## Batch Processing System
- Length-aware batching queue for inference optimization
- Implements bucket policies for sequence length grouping:
  * Power-of-two bucketing
  * Token ratio-based bucketing
- Provides back-pressure mechanisms to prevent RAM overflow
- File: `dualgpuopt/batch/smart_batch.py`

## Memory Recovery System
- Progressive memory recovery with multiple strategies:
  * CACHE_ONLY: Lightweight GPU cache clearing
  * CLOCK_RESET: GPU clock management
  * FULL_RESET: Comprehensive memory recovery
  * SYSTEM_CMD: OS-specific reclamation
- File: `dualgpuopt/memory/recovery.py`

## Memory Usage Predictor
- Implements memory profiling for common model architectures
- Projects memory growth using linear regression on usage history
- Calculates maximum batch sizes and sequence lengths based on available memory
- File: `dualgpuopt/memory/predictor.py`

## Model Deployment Strategies
Two primary approaches implemented:
1. llama.cpp:
- GPU layer splitting with context size considerations
- Memory mapping for efficient VRAM utilization
- Custom split configurations based on GPU capabilities

2. vLLM:
- Tensor parallel deployment with memory utilization controls
- Dynamic parallel size determination
- Memory-aware worker allocation

$END$