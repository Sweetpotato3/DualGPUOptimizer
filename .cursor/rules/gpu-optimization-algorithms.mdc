---
description: GPU memory distribution algorithms, parallel splits calculation, and memory allocation strategies used by optimizer and monitoring components
globs: *.py,dualgpuopt/memory/*.py,dualgpuopt/gpu/*.py,dualgpuopt/optimizer.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithm (optimizer.py)
Importance: 95

Implements GPU memory splitting and tensor parallel allocation:
- Calculates optimal memory splits based on model architecture and GPU VRAM ratios 
- Determines per-token memory requirements from model parameters (layers, heads, hidden size)
- Applies tensor parallelism overhead factors based on split configs
- Maintains memory safety margins and reserves
- Handles KV cache sizing based on attention head configurations

## Layer Balancing (layer_balance.py) 
Importance: 90

Layer distribution algorithm for transformer models across GPUs:
- Uses performance profiling with short (64 tokens) and long (1024 tokens) sequences
- Weights sequence performance (20% short, 80% long) for real-world optimization
- Maintains contiguous block grouping where possible
- Ensures critical components remain on designated GPUs:
  - Input embeddings on first GPU
  - Output components on last GPU
- Applies dynamic programming for block merging optimization

## Memory Monitoring (memory/profiler.py)
Importance: 85

Memory leak detection and profiling system:
- Session-based memory tracking with inference counters
- Two-tier leak detection:
  - Spike detection for rapid memory growth
  - Pattern analysis for steady memory leaks
- Calculates growth rates using sliding window analysis
- Filters false positives with minimum thresholds (5MB) and cooldown periods (30s)

## GPU Split Optimization (batch/smart_batch.py)
Importance: 85 

Length-aware batch scheduling for GPU inference:
- Dynamic batch size optimization based on:
  - Available GPU memory
  - Token memory requirements 
  - Model architecture parameters
  - Sequence length patterns
- Backpressure system for OOM prevention:
  - Reduces batch sizes after OOM events
  - Uses scale factor (0.25-0.95) for adjustment
  - Gradually recovers sizes when stable

## Memory Recovery (memory/recovery.py)
Importance: 80

Implements tiered memory recovery strategies:
- Progressive actions based on severity:
  1. Cache clearing
  - Tensor cache cleanup
  - CUDA memory flushing 
  2. Batch size reduction
  - Scale factor: 0.75
  - Minimum threshold enforcement
  3. Emergency deallocation
  - Forces tensor deallocation
  - Triggers garbage collection
- Maintains recovery history to prevent thrashing

File paths containing core algorithms:
```
dualgpuopt/optimizer.py
dualgpuopt/layer_balance.py
dualgpuopt/memory/profiler.py
dualgpuopt/batch/smart_batch.py
dualgpuopt/memory/recovery.py
```

$END$