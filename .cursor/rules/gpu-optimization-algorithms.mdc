---
description: GPU memory distribution and tensor parallelism algorithms for optimizing large language models across multiple GPUs
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

## GPU Memory Distribution Algorithm
Located in optimizer.py:
- Dynamic tensor parallel fraction calculation based on relative GPU memory sizes
- Generates memory splits across multiple GPUs with safety margins
- Framework-specific command generation for llama.cpp and vLLM
- Environment configuration optimization for multi-GPU setups
Importance Score: 95

## Layer Distribution System
Located in layer_balance.py:
- Adaptive latency-aware layer redistribution across GPUs
- Dual-pass profiling with sequence lengths 64 and 1024
- Weighted averaging (0.2 short + 0.8 long) for attention scaling compensation
- Dynamic layer allocation based on execution time and memory constraints
Importance Score: 90

## Context Size Management
Located in ctx_size.py:
- Heuristic algorithm for maximum safe context length calculation
- Model-specific parameter handling:
  - Layer count
  - KV heads
  - Head dimensions
  - Precision bits
  - MoE factor
- Reserved memory buffer calculation
Importance Score: 85

## GPU Resource Management
Located in gpu_info.py:
- Memory utilization ratio tracking between GPUs
- Performance characteristics profiling for load balancing
- Power and thermal constraint monitoring
- Architecture-specific CUDA core calculations
Importance Score: 80

$END$