---
description: Specifications for GPU memory allocation, tensor parallelism, and layer distribution algorithms in the DualGPUOptimizer project.
globs: dualgpuopt/optimizer.py,dualgpuopt/gpu_info.py,dualgpuopt/ctx_size.py,dualgpuopt/layer_balance.py,dualgpuopt/memory/**
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Split and Layer Distribution

The optimizer implements specialized algorithms for GPU memory allocation and model layer distribution across multiple GPUs:

1. Memory Split Calculator:
- Calculates optimal memory splits between GPUs based on relative VRAM capacity
- Implements weighted ratios to account for NVLink/PCIe bandwidth differences  
- Handles asymmetric GPU configurations with different VRAM sizes
- Produces tensor parallel fractions for framework-specific commands

2. Layer Distribution Engine:
- Performs latency-aware layer distribution across GPUs
- Uses dual sequence length profiling (64 and 1024 tokens)
- Applies weighted scoring (20% short, 80% long sequences)
- Creates contiguous layer blocks to minimize communication overhead

3. Dynamic Context Size Optimizer:
- Calculates safe context lengths based on:
  - Model architecture (MQA/GQA factors)
  - KV cache scaling requirements
  - MoE expert count overhead
  - Reserved memory buffer (2GB default)
- Implements model-specific memory estimation heuristics
- Handles different precision requirements (FP16/INT8/INT4)

4. Memory Usage Predictor:
- Maintains rolling window of memory usage patterns
- Projects future memory requirements using linear regression
- Applies architecture-specific growth factors
- Triggers recovery strategies at configurable thresholds

## Critical Paths

```
dualgpuopt/
  optimizer.py         - Memory split and tensor parallel calculations
  layer_balance.py    - Layer distribution algorithms
  ctx_size.py         - Context length optimization
  memory/
    predictor.py      - Memory usage prediction
    recovery.py       - OOM recovery strategies
```

The core optimization algorithms focus on efficient distribution of model layers and memory across heterogeneous GPU configurations while maintaining optimal performance characteristics.

$END$