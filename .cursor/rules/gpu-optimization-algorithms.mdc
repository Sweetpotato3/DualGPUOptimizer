---
description: GPU memory distribution algorithms, tensor parallel splits calculation, and memory allocation strategies from optimizer.py and gpu_info.py
globs: 
alwaysApply: false
---


# gpu-optimization-algorithms

### Memory Distribution Algorithm (optimizer.py)
Calculates optimal GPU memory splits for dual GPU setups based on:
- Relative GPU memory capacities
- Model architecture requirements
- KV cache overhead
- Safety margins (10% buffer)
- Tensor parallel slicing ratios

Core business logic splits model across GPUs using:
```python
def tensor_split(gpus):
    total = sum(g.mem_free for g in gpus)
    return [g.mem_free/total for g in gpus]
```

### Layer Balancing System (layer_balance.py)
- Profiles each transformer layer's performance on different GPUs
- Uses weighted profiling with short (64 token) and long (1024 token) sequences 
- Allocates layers to GPUs based on empirical performance data
- Optimizes for contiguous layer blocks to minimize cross-GPU communication

### Batch Size Optimization (batch/smart_batch.py)
- Dynamic batch sizing based on:
  - Available GPU memory
  - Model architecture
  - Input sequence lengths
  - Current memory pressure
- Implements backpressure when approaching memory limits
- Recovery logic to gradually increase batch size after OOM events

### Memory Monitoring (memory_monitor.py)
- Multi-tier alert system:
  - WARNING: 80% memory usage
  - CRITICAL: 90% memory usage  
  - EMERGENCY: 95% memory usage
- Memory usage projection using rolling history
- Model-specific memory profiles for common architectures
- Proactive OOM prevention with tiered recovery:
  1. Cache clearing
  2. Batch reduction
  3. Memory offloading
  4. Process termination

### GPU Information Tracking (gpu_info.py)
- Real-time monitoring of:
  - Memory utilization 
  - Power consumption
  - Temperature
  - PCIe bandwidth
- Provides mock data generation for testing
- Correlates metrics for realistic simulation

$END$