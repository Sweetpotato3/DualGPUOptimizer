---
description: GPU memory optimization algorithms and parallel processing strategies for ML model execution
globs: **/*.py,**/dualgpuopt/optimizer.py,**/dualgpuopt/gpu_info.py,**/dualgpuopt/memory/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

The GPU optimization system implements specialized algorithms for memory distribution and parallel processing across multiple GPUs.

Core Memory Management Components:

1. Memory Split Calculation (dualgpuopt/optimizer.py):
- Calculates optimal memory distribution across GPUs using:
  * Per-token KV cache requirements
  * Tensor parallel processing overhead
  * Model-specific safety margins
  * Memory reservation factors for cache growth
- Implements split ratio determination:
  * Analyzes available VRAM per GPU
  * Calculates proportional splits based on memory capacity
  * Adjusts for minimum viable splits (20% threshold)

2. Memory Growth Prediction (dualgpuopt/memory/predictor.py):
- Projects memory growth patterns using:
  * Historical allocation patterns
  * Model-specific cache scaling factors
  * Token count projections
  * Activation memory requirements
- Provides dynamic adjustment of memory allocations

3. Layer Distribution Algorithm (dualgpuopt/layer_balance.py):
- Optimizes transformer layer placement across GPUs:
  * Profiles layer execution times
  * Uses weighted sequence length sampling (20% short, 80% long)
  * Ensures balanced memory usage
  * Minimizes cross-GPU communication

4. Memory Profiling System (dualgpuopt/memory/profiler.py):
- Tracks real-time memory patterns:
  * Spike detection with configurable thresholds
  * Leak detection using sliding windows
  * Per-inference memory deltas
  * Token processing efficiency metrics

5. Recovery Mechanisms (dualgpuopt/memory/recovery.py):
- Implements tiered recovery strategies:
  * Cache clearing
  * Batch size reduction
  * Memory defragmentation
  * Process termination as last resort

6. Batch Optimization (dualgpuopt/batch/smart_batch.py):
- Length-aware batch scheduling:
  * Groups similar length sequences
  * Adjusts batch sizes based on memory availability
  * Implements backpressure when approaching limits
  * Recovers batch sizes after stability periods

The system provides comprehensive memory management for dual-GPU ML model execution, with particular focus on preventing OOM conditions while maximizing throughput.

$END$