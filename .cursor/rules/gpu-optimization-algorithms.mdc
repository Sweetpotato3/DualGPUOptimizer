---
description: Specialized GPU memory distribution and optimization algorithms for LLM inference across multi-GPU setups
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/model_profiles.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

Core Memory Distribution System:

1. Tensor Parallel Split Calculator
- Handles optimal memory distribution across multiple GPUs based on relative VRAM sizes:
  * Base memory per model (GB per billion parameters)
  * KV cache overhead per token
  * Safety margins for system overhead (20%)
  * Expert overhead for MoE models (50%)

2. Context Length Optimizer
- Dynamic calculation of maximum safe sequence length:
```python
context_size = (available_vram - base_memory) / (bytes_per_token * n_layers * kv_heads)
```
- Model-specific parameter detection:
  * LLaMA: 32/40 heads, 128 head dimension
  * Mistral: 32 heads, 128 dimension
  * Mixtral: 8 experts, 32 heads each

3. Layer Balance Algorithm
- Weighted profiling system for layer distribution:
  * Short sequence profiling (64 tokens, 20% weight)
  * Long sequence profiling (1024 tokens, 80% weight)
  * Contiguous block optimization for minimal transfers
  * Prioritizes high-latency layers on faster GPU

File Paths:
/dualgpuopt/optimizer.py
/dualgpuopt/gpu_info.py
/dualgpuopt/layer_balance.py

Importance Score: 95 - Core GPU optimization algorithms directly impacting model performance and resource utilization

$END$