---
description: GPU optimization algorithms and memory distribution logic for large language models across multiple GPUs
globs: dualgpuopt/optimizer.py,dualgpuopt/gpu_info.py,dualgpuopt/gpu/*,dualgpuopt/memory/*,dualgpuopt/engine/*,dualgpuopt/services/*,dualgpuopt/telemetry/*
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Memory Distribution Logic

The GPU optimization system implements specialized algorithms for distributing large language models across multiple GPUs:

1. Memory Split Calculation
- Dynamic tensor fraction calculation based on available VRAM ratios
- Safety margin enforcement (default 10%) for stable operation
- Overhead compensation for tensor parallelism (default 20%) 
- KV cache scaling based on context length and model architecture

2. Layer Distribution Optimization
- Adaptive layer distribution using profiling at two sequence lengths (64/1024 tokens)
- Real-time latency profiling for balanced workload distribution
- Block-based optimization to minimize cross-device transfers
- Cost-based model for layer placement decisions

3. VRAM Management
- Token-based memory calculation using model parameters:
```python
bytes_per_token = model.kv_hidden_size * model.num_layers * 2
mb_per_token = (bytes_per_token / (1024 * 1024)) * kv_cache_factor
```
- Dynamic context length optimization based on available memory
- Automatic tensor split ratio calculation for heterogeneous GPU configurations

4. Framework-Specific Optimizations
- llama.cpp: GPU split string generation with memory quota enforcement
- vLLM: Tensor parallelism configuration with memory utilization controls
- Custom layer reordering for optimal parallel execution

Relevant file paths:
- dualgpuopt/optimizer.py: Core optimization algorithms
- dualgpuopt/gpu_info.py: GPU capability detection
- dualgpuopt/memory/profiler.py: Memory usage profiling
- dualgpuopt/engine/pool/core.py: Engine instance management

The system focuses on optimal resource utilization while maintaining model performance through intelligent workload distribution and memory management strategies.

$END$