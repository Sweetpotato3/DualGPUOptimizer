---
description: Specification for GPU memory optimization algorithms, tensor parallel processing, and resource management for ML model inference
globs: **/gpu/**,**/optimizer/**,**/memory/**,**/telemetry/**
alwaysApply: false
---


# gpu-optimization-algorithms

## Core GPU Memory Management

The system implements specialized algorithms for managing and optimizing GPU memory allocation across dual GPU setups:

1. Memory Split Calculator:
- Calculates optimal tensor parallel splits based on VRAM availability and model size
- Allocation strategy considers:
  * Base model memory requirements 
  * KV cache scaling factor (2.0x)
  * System overhead buffer (2GB default)
  * Safety margin (10% buffer)

2. Layer Distribution Optimizer:
- Adaptive latency-aware layer distribution algorithm
- Uses weighted profiling combining:
  * 20% weight for short sequences (64 tokens)
  * 80% weight for long sequences (1024 tokens) 
- Enforces minimum block size of 3 layers for stability

## Memory Telemetry System

1. Real-time Memory Analysis:
- Memory pressure classification:
  * EMERGENCY: Usage ≥95% or Temperature ≥90°C
  * CRITICAL: Usage ≥90% or Temperature ≥80°C or Power ≥98%
  * WARNING: Usage ≥75% or Temperature ≥70°C or Power ≥90%
  * NORMAL: Below all thresholds

2. Differential Metrics:
- Per-GPU tracking:
  * VRAM usage percentages
  * Temperature patterns
  * Power consumption trends
- Cross-GPU correlation analysis

## Recovery Strategies

1. Memory Pressure Handler:
- Cascading recovery system:
  * Level 1: Cache clearing (CUDA/PyTorch)
  * Level 2: Batch size reduction (25% reduction)
  * Level 3: Force VRAM reset
  * Level 4: Process termination

2. Auto-scaling:
- Dynamic batch size adjustment
- Context length optimization
- Tensor parallel fraction updates

File Paths:
```
dualgpuopt/optimizer.py
dualgpuopt/memory/profiler.py 
dualgpuopt/memory/recovery.py
dualgpuopt/telemetry.py
dualgpuopt/layer_balance.py
```

$END$