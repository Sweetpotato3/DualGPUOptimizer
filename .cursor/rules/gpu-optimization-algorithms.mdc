---
description: Documentation of GPU memory distribution algorithms, tensor parallel splits calculation, and memory allocation strategies for model optimization.
globs: **/gpu_info.py,**/optimizer.py,**/layer_balance.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

Core algorithms for GPU memory optimization and workload distribution:

1. Context Size Calculator (ctx_size.py)
- Custom formula for maximum safe context length:
  MAX_CTX = (available_memory * safety_margin) / (layers * kv_heads * dim * precision)
- Model-specific architecture factors:
  - Mixtral: 1.05x memory multiplier for MoE overhead
  - Llama-2: Base memory calculation
  - Mistral: Adjusted for smaller head dimensions
- Memory reservation logic:
  - System: 2GB base reservation
  - KV Cache: 2x multiplier for bi-directional context
  - Safety margin: 10% overhead buffer

2. Layer Balancing Algorithm (layer_balance.py)
- Dual-phase layer distribution system:
  - Short sequence test (64 tokens) weighted at 20%
  - Long sequence test (1024 tokens) weighted at 80%
- Memory quota enforcement per GPU with configurable reserve ratio
- Layer mapping optimization prioritizes fastest GPU within memory constraints

3. GPU Split Calculator (optimizer.py)
- Memory split ratio generation for multi-GPU setups:
  - Base split on relative VRAM capacities
  - 20% overhead allocation for tensor parallelism
  - Dynamic adjustment based on model architecture
- Framework-specific command generation:
  - llama.cpp: --gpu-split parameter optimization
  - vLLM: tensor-parallel-size configuration

4. Memory Policy Engine (mpolicy.py)
- FP32 preservation for critical operations:
  - LayerNorm computations
  - Softmax calculations
  - Residual additions
- Dynamic precision switching based on operation type
- Safety thresholds for memory-intensive operations

Critical Paths:
/dualgpuopt/ctx_size.py
/dualgpuopt/layer_balance.py  
/dualgpuopt/optimizer.py
/dualgpuopt/mpolicy.py

Importance Score: 95
The algorithms represent core optimization logic for GPU memory management and workload distribution in ML model execution.

$END$