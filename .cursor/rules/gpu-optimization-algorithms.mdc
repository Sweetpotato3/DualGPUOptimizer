---
description: Documents core GPU optimization algorithms, memory management, and tensor parallel processing logic for dual GPU systems
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

Core GPU Memory Distribution Algorithm:
- Calculates optimal memory splits across multiple GPUs based on total available VRAM
- Dynamically adjusts tensor parallel fractions relative to highest capacity GPU
- Implements model-aware scaling for MoE architectures with extra memory overhead
- Reserve ratio maintains 10% buffer for system operations

Context Size Calculation:
```python
bytes_per_token = n_layers * n_kv_heads * head_dim * (precision_bits/8) * 2 * moe_factor
max_context = (available_gpu_mem * reserve_ratio) / bytes_per_token
```

Layer Balancing System:
- Dual-pass profiling strategy:
  - Short sequence pass (64 tokens) establishes baseline performance
  - Long sequence pass (1024 tokens) determines scaling behavior
  - Weighted combination (20/80) provides realistic workload assessment
- Dynamic GPU allocation adjusts based on real-time profiling metrics
- Maintains memory constraints while optimizing for computational latency

Tensor Parallel Split Calculator:
- Generates specialized command parameters for different ML frameworks:
  - llama.cpp: GPU split string based on relative VRAM ratios
  - vLLM: Tensor parallelism configuration with balanced distribution
- Handles architecture-specific CUDA core calculations
- Implements failover to mock GPU mode for testing scenarios

Relevant Files:
- dual_gpu_optimizer/dualgpuopt/optimizer.py
- dual_gpu_optimizer/dualgpuopt/gpu_info.py
- dual_gpu_optimizer/dualgpuopt/layer_balance.py
- dual_gpu_optimizer/dualgpuopt/ctx_size.py

Business Impact Scores:
- GPU Memory Distribution: 95 (Core optimization logic)
- Context Size Algorithm: 90 (Critical for model deployment)
- Layer Balancing: 85 (Key performance optimization)
- Tensor Split Calculator: 80 (Framework integration)

$END$