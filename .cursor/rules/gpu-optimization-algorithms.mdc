---
description: Specification for GPU memory optimization, tensor parallel splits, and memory allocation strategies for multi-GPU LLM deployment.
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py,**/metrics.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution System
- GPU memory split calculation based on available VRAM ratios
- Safety buffer allocation (2GB default) for operational overhead
- Model-specific memory scaling factors:
  - Mixtral MoE: 1.05x multiplier
  - Standard transformers: 1.0x multiplier

## Layer Distribution Optimization
Location: layer_balance.py
Importance: 95

- Weighted profiling methodology:
  - Short sequences (64 tokens): 20% weight
  - Long sequences (1024 tokens): 80% weight
- Dynamic layer redistribution based on real-time execution profiles
- Latency-aware balancing respecting per-GPU VRAM quotas 

## Context Size Calculation
Location: ctx_size.py
Importance: 90

- Maximum safe context length formula:
  `max_ctx = (free_memory * 1024Â²) / (n_layers * n_kv_heads * head_dim * precision_factor * 2 * moe_factor)`
- Model architecture parameter mapping for:
  - Mixtral MoE variants
  - Llama 2 architectures
  - Mistral models

## Tensor Parallel Splits
Location: optimizer.py
Importance: 85

- Split ratio calculation based on relative GPU memory sizes
- Framework-specific command generation:
  - llama.cpp GPU split strings
  - vLLM tensor parallel configurations
- Dynamic tensor distribution factoring in:
  - Available VRAM per GPU
  - Model layer counts
  - Attention head configurations

## Memory Management Pipeline
Location: gpu_info.py
Importance: 80

- Real-time GPU telemetry collection
- Multi-GPU coordination system
- Asymmetric GPU configuration handling
- Dynamic memory quota adjustments
- Safety thresholds enforcement

## Metrics Collection
Location: metrics.py
Importance: 75

- GPU utilization tracking
- Memory usage monitoring
- Temperature and power metrics
- PCIe bandwidth measurement
- Clock speed monitoring

$END$