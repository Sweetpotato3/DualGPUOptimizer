---
description: Analyzes GPU optimization algorithms and memory allocation strategies for multi-GPU model inference
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py,**/batch/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Optimization Components

### Memory Distribution (90)
- Calculates optimal GPU memory splits based on total available VRAM
- Generates tensor parallel fractions for each GPU to balance model layers
- Dynamic adjustment of context sizes based on model architecture and GPU capabilities
- Memory mapping strategies consider PCIe bandwidth and GPU interconnect speeds

### Layer Balancing (95)
- Adaptive latency-aware layer redistribution across multiple GPUs
- Profiles layer execution times to identify fast vs slow layers
- Dynamic device mapping adjusts layer assignments based on real-time performance
- Handles asymmetric GPU configurations by rebalancing computational load

### Batch Processing (85)
- Length-aware inference scheduler groups similar sequence lengths
- Implements bucket policies for optimal batch size determination
- Automatic retry logic for GPU OOM errors with CUDA cache clearing
- Dynamic adjustment of batch sizes based on GPU utilization metrics

### Context Management (80)
- Calculates maximum safe context lengths per GPU
- Considers model parameters, head dimensions, and MoE factors
- Dynamically adjusts context windows based on available VRAM
- Handles model-specific requirements for Mixtral, Llama 2, and Mistral

## Key Files
- `optimizer.py`: Core optimization algorithms and command generation
- `gpu_info.py`: GPU probing and memory analysis
- `layer_balance.py`: Layer distribution and rebalancing logic
- `ctx_size.py`: Context size calculations and adjustments
- `batch/smart_batch.py`: Batch optimization and scheduling

$END$