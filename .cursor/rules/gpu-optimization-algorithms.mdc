---
description: GPU optimization algorithms and memory management strategies for multi-GPU language model inference
globs: **/optimizer.py,**/gpu_info.py,**/memory/*.py,**/batch/*.py,**/layer_balance.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Memory Management Algorithms (optimizer.py)

Memory distribution and tensor splitting calculations optimize deployment of large language models across multiple GPUs:

1. Split Ratio Calculation:
- Uses relative VRAM sizes to determine optimal GPU allocation percentages
- Accounts for model architecture overhead and KV cache requirements 
- Enforces minimum 20% per-GPU allocation rule
- Adjusts splits for tensor parallelism overhead (20% buffer)

2. Context Length Optimization:
- Calculates maximum safe context window based on:
  * Available GPU memory
  * Model architecture (layers, heads, dimensions)
  * KV cache scaling factors
  * MoE overhead multipliers (1.2x for expert models)

## Layer Distribution Logic (layer_balance.py)

1. Performance-Based Layer Assignment:
- Profiles layer execution across sequence lengths:
  * Short sequences (64 tokens): 20% weight
  * Long sequences (1024 tokens): 80% weight
- Distributes transformer layers based on:
  * GPU performance ratios
  * Memory capacity limits
  * Contiguous block optimization

2. Block Optimization:
- Merges isolated layers into contiguous blocks
- Input embeddings assigned to first GPU
- Output components (norm, LM head) to second GPU
- Minimizes cross-device transfer overhead

## Memory Recovery System (memory/recovery.py)

Hierarchical recovery strategy for out-of-memory prevention:
1. Cache clearing
2. Batch size reduction
3. Model offloading
4. Process termination

Memory safety rules:
- 15% reserved recovery buffer
- Minimum batch size: 1
- Minimum sequence length: 128 tokens

## Batch Optimization (batch/smart_batch.py)

Length-aware inference scheduling:
- Dynamic batch sizing based on token counts
- Back-pressure system for RAM protection
- Sequence length bucketing with two strategies:
  * Power-of-two scaling
  * Token ratio preservation
- OOM protection with automatic retry mechanism

## Model Distribution (memory/predictor.py)

Memory usage prediction per model type:
- Base memory requirements
- Per-batch scaling factors
- Token-based growth rates
- Architecture-specific overhead:
  * Standard models: 1.0x
  * MoE models: 1.2x per active expert

$END$