---
description: Documents GPU optimization algorithms, memory management, tensor parallelism, and resource allocation for dual-GPU systems
globs: **/optimizer.py,**/gpu_info.py,**/ctx_size.py,**/layer_balance.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core GPU Memory Management

The system implements specialized GPU memory optimization algorithms with business logic focused on dual-GPU workload distribution:

1. Context Size Optimization
- Calculates maximum safe context length based on model parameters:
  - Layers, KV heads, dimensions 
  - Available GPU memory with configurable safety margins
  - Mixture of Experts (MoE) factor for models like Mixtral
- Implements model-specific parameter mapping for common LLMs
- Uses heuristic memory calculation incorporating:
  - 20% tensor parallelism overhead
  - 2GB system overhead allowance
  - 10% safety margin

2. Layer Distribution Algorithm
- Balances model layers across GPUs using weighted profiling:
  - 20% weight for short sequences (64 tokens)
  - 80% weight for long sequences (1024 tokens)
- Prioritizes performance-critical layers on faster GPU
- Respects per-GPU memory quotas
- Handles tensor parallelism overhead

3. Memory Split Optimization
- Calculates optimal tensor parallel fractions based on relative GPU sizes
- Implements KV cache estimation with 2.0x overhead factor
- Generates framework-specific configurations for:
  - llama.cpp GPU split ratios
  - vLLM tensor parallel deployment
  - Environmental configurations

Relevant Files:
- dualgpuopt/optimizer.py
- dualgpuopt/layer_balance.py 
- dualgpuopt/ctx_size.py

Business Impact Score: 95
The algorithms directly impact model performance and resource utilization across dual-GPU setups, representing core optimization capabilities.

$END$