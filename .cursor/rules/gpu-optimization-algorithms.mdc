---
description: GPU memory distribution algorithms and tensor parallel splits calculation for optimizing dual-GPU model inference
globs: **/optimizer.py,**/gpu_info.py,**/memory/*.py,**/telemetry.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core Memory Distribution Logic

1. GPU Memory Split Algorithms:
- Calculates optimal memory distribution across dual GPUs based on:
  * Available VRAM capacity per GPU 
  * Model architecture requirements
  * KV cache memory needs
  * Tensor parallel overhead
- Implements safety margins and reserved memory buffers
- Enforces minimum allocation of 20% model per GPU
- Dynamic overhead adjustment based on quantization
File: dualgpuopt/optimizer.py

2. Model-Specific Memory Calculation:
- Per-token memory requirements based on:
  * Number of layers and heads
  * Hidden dimension size
  * MoE architecture factors
  * Attention mechanism type (MQA/GQA)
- Base memory calculation formula:
  bytes_per_token = kv_hidden_size * num_layers * 2
File: dualgpuopt/memory/profiler.py

3. Layer Distribution Strategy:
- Profile-based layer distribution between GPUs
- Performance weighted scoring (20% short sequences, 80% long)
- Minimum block size enforcement (3 layers)
- Special handling for embeddings and output layers
File: dualgpuopt/layer_balance.py

4. Memory Recovery System:
- Multi-tiered recovery approach:
  * Cache clearing (PyTorch/CUDA)
  * Clock/compute mode reset
  * Full tensor cleanup
  * OS-level resource management
- Platform-specific optimizations for Windows/Linux
- Reclaimed memory tracking across GPUs
File: dualgpuopt/vram_reset.py

5. GPU Telemetry Integration:
- Real-time monitoring of:
  * Memory utilization
  * Temperature thresholds  
  * Power consumption
  * PCIe bandwidth
- 60-second rolling metric history
- Automatic failover to mock data
File: dualgpuopt/telemetry.py

## Critical Memory Parameters

Memory Alert Thresholds:
- EMERGENCY: ≥95% memory usage or ≥90°C
- CRITICAL: ≥90% memory or ≥80°C 
- WARNING: ≥75% memory or ≥70°C

Context Size Rules:
- Minimum: 128 tokens
- Maximum: Based on available GPU memory
- Rounding: Nearest 128 tokens
- Safety margin: 20% reserved memory

Tensor Parallel Default Split:
- Primary/Secondary: 60%/40%
- Minimum per GPU: 20%
- Maximum per GPU: 80%

$END$