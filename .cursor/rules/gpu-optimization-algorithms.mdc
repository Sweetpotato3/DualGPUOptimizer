---
description: Specification for GPU memory distribution algorithms, tensor parallel splits calculation and memory allocation strategies
globs: **/gpu_info.py,**/optimizer.py,**/gpu_commands.py,**/layer_balance.py,**/memory_monitor.py
alwaysApply: false
---


# gpu-optimization-algorithms

## GPU Memory Split Optimization

Core algorithm for distributing model layers and memory across multiple GPUs:

1. Memory Distribution Algorithm:
```python
def calculate_gpu_splits(gpus):
  total_mem = sum(g.mem_total for g in gpus)
  splits = [round(g.mem_total/total_mem, 3) for g in gpus]
  return splits
```

2. Tensor Parallel Size Calculation:
- For N GPUs, calculates optimal tensor parallel size based on:
  - Total available VRAM across GPUs
  - Model size and parameter count 
  - Communication overhead between GPUs
  - Framework-specific requirements (vLLM vs llama.cpp)

3. Layer Balancing:
- Uses weighted performance profiling with two sequence lengths:
  - Short sequences (64 tokens) weighted at 20%
  - Long sequences (1024 tokens) weighted at 80%
- Groups layers into contiguous blocks to minimize GPU switching
- Accounts for temperature and power limits when distributing compute

4. Memory Reservation:
- Reserves fixed 2GB buffer per GPU for system overhead
- Additional 5% reservation for dynamic allocations
- Framework-specific memory optimizations:
  - vLLM: 90% memory utilization target
  - llama.cpp: Variable based on quantization

5. Model-Specific Optimizations:
- MoE models: Additional 5% memory for expert routing
- Attention patterns: Adjusts for sliding window vs full attention
- Quantization: Accounts for precision (FP16/INT8/INT4)

File paths containing core GPU optimization logic:
- dualgpuopt/optimizer.py 
- dualgpuopt/memory_monitor.py
- dualgpuopt/layer_balance.py
- dualgpuopt/gpu_info.py

$END$