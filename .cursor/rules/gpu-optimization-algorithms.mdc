---
description: GPU memory distribution algorithms, tensor parallel split calculations, and memory allocation strategies for optimizing dual GPU setups
globs: **/optimizer.py,**/gpu_info.py,**/layer_balance.py,**/ctx_size.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution Algorithm
- Split ratio calculation based on relative GPU capacities
- Dynamic tensor parallel fraction computation:
  ```python
  split_ratio = gpu_memory[1] / gpu_memory[0]
  tensor_fraction = min(1.0, split_ratio * 0.9)
  ```
- Memory reservation threshold of 10% for system overhead
- Automatic rebalancing when memory pressure exceeds 90%

## Context Size Optimization 
- Token memory requirement calculation:
  ```python
  bytes_per_token = n_layers * n_kv_heads * head_dim * (precision/8) * 2 * moe_factor
  ```
- Model-specific parameter mapping for Mixtral, Llama 2, Mistral
- MoE factor adjustment (~1.05) for mixture-of-experts architectures
- Minimum 2GB reserved memory buffer enforcement

## Layer Distribution Strategy
- Latency-aware layer balancing across GPUs using:
  - Short sequence profiling (64 tokens)
  - Long sequence profiling (1024 tokens)
  - 20/80 weighted average for final distribution
- Memory quota enforcement per GPU based on capacity ratio
- Real-time redistribution based on layer execution metrics

## GPU Memory Management
Files: optimizer.py, gpu_info.py
- Memory split strategies for multi-GPU configurations
- Framework-specific deployment parameters:
  - llama.cpp: GPU split ratios and context limits
  - vLLM: Tensor parallel sizing and memory quotas
- Precision-aware allocation for:
  - FP16 base operations
  - 8-bit QLoRA fine-tuning
  - 4-bit GPTQ inference

Importance Score: 95
Rationale: Core GPU optimization algorithms directly impact model performance and hardware utilization

$END$