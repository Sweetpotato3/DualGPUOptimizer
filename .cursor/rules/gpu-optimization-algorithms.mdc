---
description: GPU memory distribution and tensor parallel optimization for large language models across multiple GPUs
globs: **/optimizer.py,**/gpu_info.py,**/*gpu*.py
alwaysApply: false
---


# gpu-optimization-algorithms

### GPU Memory Distribution (Importance: 95)
Located in `optimizer.py`, the core GPU memory distribution logic:
- Calculates optimal memory splits across detected GPUs for model distribution
- Determines tensor parallel fractions based on available VRAM
- Generates configuration strings for CUDA device visibility and memory allocation

### GPU Detection System (Importance: 90)
Implemented in `gpu_info.py`:
- Probes NVIDIA Management Library (NVML) for GPU hardware information
- Retrieves per-device memory capacity and utilization metrics
- Supports mock GPU data generation for testing environments via `DGPUOPT_MOCK_GPUS`

### Tensor Parallel Split Calculator (Importance: 85)
Core algorithm in `optimizer.py`:
- Computes optimal tensor parallel splits for model layers
- Adjusts split ratios based on available GPU memory
- Generates split configuration strings for model runners

### Memory Allocation Strategy (Importance: 80)
Memory management system in `optimizer.py`:
- Allocates model weights across multiple GPUs
- Handles CUDA visible devices configuration
- Manages P2P communication settings between GPUs
- Generates environment variables for GPU memory configuration

### Model Execution Command Generation (Importance: 75)
Command generation logic in `optimizer.py`:
- Creates optimized commands for llama.cpp and vLLM implementations
- Incorporates GPU splits and tensor fractions into command parameters
- Handles different model architectures and context size requirements

$END$