---
description: Specification for GPU memory distribution algorithms, tensor parallel splits and memory allocation optimization strategies
globs: **/*optimizer*.py,**/gpu_info.py,**/gpu_split*.py,**/model_distribution.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Core GPU Memory Distribution Logic 

The optimizer implements specialized algorithms for distributing large language models across multiple GPUs. Core components include:

### Memory Split Calculator
- Implements dynamic memory split ratios based on model architecture:
  - 70B models: 70/30 GPU split, max 4096 context 
  - Mixtral: 60/40 GPU split, max 8192 context
  - 13B models: 50/50 GPU split, max 12288 context
  - Other models: 40/60 GPU split, unlimited context

### Tensor Parallel Layer Distribution
- Dynamic layer distribution based on model architecture parameters:
  - Calculates optimal tensor parallel fractions per GPU
  - Balances memory requirements with bandwidth constraints
  - Special handling for MoE (Mixture of Experts) models
  - Preserves residual connections across GPU boundaries

### Memory Allocation Strategies
- Progressive memory allocation steps:
  1. Base model memory estimation
  2. KV cache sizing with configurable overhead
  3. Tensor parallel memory requirements
  4. Safety margin calculation
  - Automatic fallback to mock data if hardware monitoring unavailable

### Framework-Specific Command Generation
- `llama.cpp`: Configures GPU layer distribution and precision
- `vLLM`: Sets tensor parallelism and model length parameters
- Custom backends for PyTorch and other frameworks

## Domain-Specific Memory Profiles

- Model-specific memory consumption patterns:
```python
MEMORY_PROFILES = {
  "llama-70b": {
    "base_memory": 140*1024,  # MB
    "per_token": 320,         # bytes
    "kv_scale": 2.0          # multiplier
  },
  "mixtral-8x7b": {
    "base_memory": 120*1024,
    "per_token": 256,
    "kv_scale": 1.8
  }
}
```

## File Paths
- `dualgpuopt/optimizer.py`  
- `dualgpuopt/gpu_info.py`

$END$