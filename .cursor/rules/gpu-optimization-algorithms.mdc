---
description: Defines GPU memory allocation strategies, tensor parallel distribution mechanisms, and optimization algorithms for distributed model deployment
globs: **/gpu_info.py,**/optimizer.py,**/gpu/**,**/memory/**,**/batch/**
alwaysApply: false
---


# gpu-optimization-algorithms

Core Memory Distribution Logic:
- Calculates optimal GPU splits using transformer layer distribution algorithms
- Memory allocation per token: KV hidden size * layers * 2 * precision factor
- Tensor parallel fraction determination based on relative GPU memory sizes
- Dynamic context length optimization with safety margin enforcement
- Batch scheduling with automatic size reduction after OOM events

Key Memory Management Components:

1. Memory Profiling:
- Token-based memory allocation tracking across inference sessions
- Two-tier leak detection using linear regression for rapid spikes and sustained growth
- Memory timeline analysis categorizing ALLOCATION, DEALLOCATION, GROWTH_SPIKE events
- Memory retention pattern analysis specific to inference operations

2. Batch Processing:
- Length-aware batch scheduling for GPU inference workloads 
- Adaptive backpressure system reducing batch size 25% after OOM
- Gradual batch size recovery after 5 successful processes
- Maintains scaling factor between 0.25-0.95
- Prioritizes fastest GPU for compute-intensive layers

3. Memory Recovery:
- Multi-stage memory reclamation:
  - CACHE_ONLY: Conservative PyTorch cache reset
  - CLOCK_RESET: GPU clock manipulation 
  - FULL_RESET: Complete tensor cleanup
  - SYSTEM_CMD: OS-level resource release

Core File Paths:
- dualgpuopt/optimizer.py
- dualgpuopt/memory/profiler.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/memory/recovery.py
- dualgpuopt/gpu/info.py

Business Impact Scores:
- Memory Distribution Logic: 95
- Memory Profiling System: 85
- Batch Processing: 80
- Recovery Mechanisms: 75

$END$