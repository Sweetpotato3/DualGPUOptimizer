---
description: Specification for GPU memory distribution algorithms, tensor parallel splits, and allocation strategies
globs: /dualgpuopt/optimizer.py,/dualgpuopt/gpu_info.py,/dualgpuopt/services/*.py
alwaysApply: false
---


# gpu-optimization-algorithms

## Memory Distribution System
Located in `dualgpuopt/optimizer.py`
Importance: 95

1. GPU Memory Splitting Algorithm
- Calculates optimal memory distribution across available GPUs
- Assigns tensor fractions based on relative GPU memory capacities 
- Handles asymmetric GPU configurations (different memory sizes)
- Generates split ratios specific to LLaMA and vLLM frameworks

2. Tensor Parallel Distribution
- Determines parallel split configurations based on:
  - Individual GPU memory capacity
  - Total available system memory
  - Model requirements and context sizes
- Creates framework-specific tensor configurations

## GPU Architecture Management
Located in `dualgpuopt/gpu_info.py`
Importance: 85

1. Hardware Capability Assessment
- Maps compute capabilities to specific GPU architectures
- Calculates CUDA core counts using architecture-specific formulas
- Determines memory bandwidth and allocation limits
- Provides architecture-aware optimization parameters

2. Multi-GPU Configuration Handler
- Manages resource coordination between GPUs
- Implements mock GPU simulation for testing
- Handles architecture-specific memory constraints

## Optimization Strategies
Located in `dualgpuopt/services/state_service.py`
Importance: 90

1. GPU Profile Management
- Maintains per-GPU optimization profiles
- Handles memory allocation strategies per profile
- Preserves optimization states between sessions

2. Resource Allocation Rules
- Power limit thresholds: 50% to 120%
- Memory clock offset ranges: -1000 to +1500 MHz
- Core clock offset ranges: -200 to +200 MHz
- Temperature-based throttling controls

$END$