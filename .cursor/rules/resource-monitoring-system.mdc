---
description: Specialized monitoring system for tracking GPU resources, memory usage, and performance metrics in ML workloads
globs: dualgpuopt/telemetry.py,dualgpuopt/gpu/monitor.py,dualgpuopt/memory/monitor.py,dualgpuopt/batch/smart_batch.py
alwaysApply: false
---


# resource-monitoring-system

The GPU resource monitoring system consists of several core components:

## Real-time Telemetry Collection (telemetry.py)
- Custom alert level system (NORMAL, WARNING, CRITICAL, EMERGENCY) based on GPU metrics
- Alert threshold rules:
  * Memory: Emergency >95%, Critical >90%, Warning >75%
  * Temperature: Emergency >90°C, Critical >80°C, Warning >70°C
  * Power: Critical >98%, Warning >90% of power limit
- Maintains 60-second rolling history of GPU metrics
- Implements mock data generation for different GPU tiers:
  * High-end: 24GB memory, 350W power limit, 2100MHz base clock
  * Mid-range: 12GB memory, 200W power limit, 1800MHz base clock

## Smart Batch Processing (batch/smart_batch.py)
- Implements length-aware batch scheduling for GPU inference
- Dynamic batch size optimization based on model memory profiles
- Token budget management (max 16384 tokens per batch)
- Memory pressure monitoring with automatic backpressure
- Reduces to 25% capacity after OOM events, gradually recovers to 95%

## Memory Monitoring (memory/monitor.py)
- Real-time memory allocation tracking with process granularity
- Predictive analysis for impending OOM conditions
- Memory growth rate calculation with linear regression
- Memory leak detection through sliding window analysis
- Event classification for allocation/deallocation patterns

## GPU Resource Monitor (gpu/monitor.py) 
- Multi-GPU synchronization for resource tracking
- PCIe bandwidth utilization monitoring
- Clock speed synchronization across GPUs
- Temperature correlation with workload intensity
- Power draw tracking with dynamic limits

## Key Business Rules
- Alert thresholds scale based on GPU generation and capabilities
- Memory recovery triggered at 95% utilization
- Batch size adjusts dynamically based on memory pressure
- Memory leaks identified through sustained growth patterns
- Temperature thresholds adjust based on GPU model specifications

$END$