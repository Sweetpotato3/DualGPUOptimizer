---
description: Rules and guidelines for documenting data flow patterns between components in GPU optimization systems
globs: **/dualgpuopt/**/*.py,**/services/**/*.py,**/commands/**/*.py
alwaysApply: false
---


# data-flow-architecture

Core Data Flow Paths:

1. GPU Telemetry Collection Pipeline
- Source: telemetry.py → event_bus.py → metrics.py
- Raw GPU metrics flow from NVML interfaces through middleware processors
- Normalized metrics published to application event bus
- Performance data routed to Prometheus collectors

2. Optimization Parameter Flow
- Source: optimizer.py → mpolicy.py → gpu_commands.py
- GPU memory splits calculated based on available VRAM
- Mixed precision policies generated for inference
- Command parameters propagated to framework-specific launchers

3. Resource Monitoring Chain
- Source: gpu_info.py → telemetry.py → dashboard.py
- Real-time GPU utilization metrics
- Temperature/power/clock data streams
- PCIe bandwidth monitoring flow

4. Configuration Distribution
- Source: config_service.py → state_service.py → optimizer.py
- GPU overclock profiles
- Idle detection thresholds
- Model context sizes
- Framework-specific parameters

Key Integration Points:

1. Metrics Collection Hub (metrics.py)
- Centralizes GPU performance data
- Distributes to monitoring systems
- Feeds optimization decision engine

2. Event Distribution System (event_bus.py)
- Routes GPU state changes
- Propagates optimization decisions
- Handles telemetry broadcasts

3. Command Generation Pipeline (gpu_commands.py)
- Receives optimization parameters
- Generates framework commands
- Routes to execution services

The data flow architecture emphasizes real-time GPU metric collection and distribution, with optimization parameters flowing from analysis components to execution services through a centralized event bus system.

$END$