---
description: Data flow documentation for GPU resource management and optimization across distributed components
globs: dualgpuopt/**/*.py,integrated_app/**/*.py,dual_gpu_optimizer/**/*.py
alwaysApply: false
---


# data-flow-architecture

The dual GPU optimization system implements a multi-layered data flow architecture:

1. Core Telemetry Flow
- GPU metrics originate in telemetry.py through NVML/mock providers
- Metrics flow through EventBusMiddleware for standardization 
- Dashboard components consume metrics via event subscriptions
- Historical metrics retained in 60-sample rolling buffers

2. Model Parameter Flow
- Model configurations enter through optimizer_tab.py
- Parameter resolver validates and transforms configurations
- Launch controller generates framework-specific commands
- Commands flow to process execution layer

3. Memory Management Flow
- Memory predictor analyzes model requirements
- Layer balancer calculates optimal distributions
- Memory monitor tracks utilization metrics
- Recovery system triggers staged interventions

4. Resource Optimization Flow
- GPU split calculations originate in optimizer.py
- Context size optimizer determines safe limits
- Layer distribution flows to execution environment
- Real-time metrics flow back for validation

Key Integration Points:
- Event bus provides centralized metric distribution
- Memory monitor coordinates with recovery system
- Parameter resolver feeds launch controller
- Telemetry system supplies all monitoring components

The architecture emphasizes GPU resource coordination through event-driven communication patterns, with metrics flowing from hardware interfaces through processing middleware to visualization and control components.

$END$