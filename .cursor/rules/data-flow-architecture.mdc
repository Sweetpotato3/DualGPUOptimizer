---
description: Documents data flow between components in a dual-GPU optimization system for machine learning workloads.
globs: **/services/**/*.py,**/telemetry/**/*.py,**/engine/**/*.py,**/memory/**/*.py
alwaysApply: false
---


# data-flow-architecture

Core Data Flow Components:

1. GPU Telemetry Pipeline (dualgpuopt/telemetry.py)
- Metric Collection Flow:
  - GPU utilization/memory/temperature/power metrics collected via NVML
  - 60-second rolling history buffer maintained per GPU
  - Synchronous data feed to visualization components through event bus
  - Failover to mock data generation on hardware access failures

2. Memory Event Flow (dualgpuopt/memory/profiler.py)
- Memory Event Processing:
  - Capture allocation/deallocation/growth events
  - Associate events with specific inference sessions
  - Channel memory metrics to monitoring dashboards
  - Feed historical data to leak detection system

3. Model Optimization Flow (dualgpuopt/engine/backend.py)
- Model Processing Pipeline:
  - Initial model format detection (.gguf, .safetensors, HF)
  - Route to appropriate engine implementation
  - Stream inference results through non-blocking queue
  - Monitor engine health via status checks

4. Cross-Component Data Exchange:
- GPU Metrics → Memory Monitor → Optimizer
- Model States → Engine Pool → Launch Controller
- Memory Events → Alert System → Dashboard
- Telemetry Data → Event Bus → UI Components

5. Resource Management Flow:
- Engine Pool → Memory Monitor → Recovery System
- GPU States → Optimizer → Command Generator
- Memory Alerts → Event Bus → Status Display

Key Integration Points:
- Telemetry Service interfaces with NVML for hardware metrics
- Memory Profiler feeds data to leak detection system
- Engine Pool coordinates with Memory Monitor for resource constraints
- Event Bus provides central nervous system for metric distribution

The architecture emphasizes reliable metric collection and distribution while maintaining system stability through graceful degradation and failover mechanisms.

$END$