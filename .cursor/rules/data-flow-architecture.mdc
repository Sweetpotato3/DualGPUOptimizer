---
description: Documents data flow patterns for GPU optimization and model inference across dual GPU setups
globs: **/dualgpuopt/**/*.py,**/dual_gpu_optimizer/**/*.py
alwaysApply: false
---


# data-flow-architecture

Core Data Flow Components:

1. GPU Metrics Pipeline (Importance: 95)
- Location: dualgpuopt/telemetry.py
- Collects real-time GPU performance data:
  * Memory utilization
  * Temperature
  * Power consumption
  * Clock speeds
  * PCIe bandwidth
- Implements event-driven metrics distribution through EventBus
- Features telemetry middleware for data transformation and normalization
- Handles graceful degradation with mock data generation

2. Model Layer Distribution Flow (Importance: 90)
- Location: dualgpuopt/layer_balance.py 
- Directs transformer layer assignments across GPUs based on:
  * Per-layer performance profiles
  * Memory quotas
  * GPU capability differentials
- Employs weighted performance metrics:
  * Short sequences (64 tokens): 20% weight
  * Long sequences (1024 tokens): 80% weight
- Optimizes data movement through contiguous block formation

3. Memory Management Pipeline (Importance: 85)
- Location: dualgpuopt/memory_monitor.py
- Implements tiered alert system for memory pressure:
  * NORMAL → WARNING → CRITICAL → EMERGENCY
- Tracks per-process memory consumption
- Provides hierarchical recovery flows:
  1. Cache clearing
  2. Batch reduction
  3. Memory offloading
  4. Process termination

4. Optimization Parameter Flow (Importance: 80)
- Location: dualgpuopt/optimizer.py
- Calculates and distributes:
  * Tensor parallel fractions
  * GPU memory splits
  * Context length limits
  * Framework-specific parameters
- Routes optimization parameters to framework-specific deployment handlers

5. Framework Command Generation (Importance: 75)
- Location: dualgpuopt/commands/gpu_commands.py
- Generates framework-specific deployment commands
- Routes environment variables and GPU configurations
- Handles command validation and error reporting flow

The system implements a multi-layered data flow architecture optimizing GPU resource utilization for large language model inference, with particular emphasis on dual GPU configurations and dynamic resource management.

$END$