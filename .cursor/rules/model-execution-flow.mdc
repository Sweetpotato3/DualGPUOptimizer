---
description: Defines execution workflow and command generation for LLM models across multiple GPU configurations
globs: **/*.py,**/commands/*,**/batch/*,**/services/*
alwaysApply: false
---


# model-execution-flow

Core LLM Execution Components:

1. Command Generation Engine
File: dual_gpu_optimizer/dualgpuopt/commands/command_base.py
Importance: 95

- Framework-specific command construction:
```python
llama_command = f"--model {model_path} --gpu-split {split} --n-gpu-layers 999"
vllm_command = f"--model {model_path} --tensor-parallel-size {tp_size}"
```

- Dynamic parameter injection based on:
  - Available GPU memory ratios
  - Model architecture type
  - Context window requirements
  - Framework compatibility

2. Layer Distribution System
File: dual_gpu_optimizer/dualgpuopt/layer_balance.py 
Importance: 90

- Adaptive layer allocation across GPUs using:
  - Hardware capability profiling
  - Memory availability tracking
  - Layer performance scoring
- Weighted sequence testing:
  - 20% weight for short sequences (64 tokens)
  - 80% weight for long sequences (1024 tokens)

3. Memory Management Pipeline
File: dual_gpu_optimizer/dualgpuopt/optimizer.py
Importance: 85

Key allocation rules:
- System memory reserve: 2GB
- KV cache multiplier: 2x
- Tensor parallel overhead: 20%
- Safety margin: 10%

4. Batch Processing Workflow
File: dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py
Importance: 80

- Length-aware sequence batching
- Dynamic batch size calculation using:
  - Available GPU memory
  - Model architecture parameters
  - Sequence length distribution
- Back-pressure management with configurable queue depths

5. GPU State Coordination
File: dual_gpu_optimizer/dualgpuopt/services/state_service.py
Importance: 75

- Manages execution state across GPUs:
  - Active model tracking
  - Resource allocation state
  - GPU assignment mapping
  - Idle state detection

6. Telemetry Integration
File: dual_gpu_optimizer/dualgpuopt/telemetry.py
Importance: 70

- Real-time execution metrics:
  - GPU utilization monitoring
  - Memory usage tracking
  - Temperature thresholds
  - Power consumption limits

The system orchestrates LLM execution across multiple GPUs by managing command generation, layer distribution, memory allocation, and batch processing while maintaining real-time performance monitoring.

$END$