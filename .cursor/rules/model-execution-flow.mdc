---
description: Defines execution workflow for LLM models across multiple GPUs, including command generation and resource allocation
globs: **/dualgpuopt/engine/**,**/dualgpuopt/gpu/**,**/dualgpuopt/launcher/**
alwaysApply: false
---


# model-execution-flow

The LLM execution workflow consists of several key components:

## Command Generation Flow
1. Framework Detection & Command Generation:
- Detects model format (GGUF/AWQ/HF) and selects appropriate framework
- Generates framework-specific commands:
  - llama.cpp: GPU split and layer balancing commands
  - vLLM: Tensor parallelism and memory allocation commands

2. Resource Allocation:
- Calculates optimal memory splits between GPUs based on:
  - Model size and architecture
  - Available GPU VRAM
  - Memory overhead requirements
- Determines tensor parallel configurations
- Applies fractional GPU allocation for smaller models

## Execution Pipeline
1. Pre-execution Setup:
- VRAM reset sequence if enabled 
- Cache clearing
- Memory monitoring initialization
- Layer balancing calculation

2. Process Management:
- Launches model process with configured parameters
- Monitors process health and memory pressure
- Handles OOM recovery through cache clearing
- Manages concurrent model instances

3. Resource Monitoring:
- Tracks real-time GPU metrics
- Publishes telemetry events
- Monitors memory usage thresholds
- Triggers cleanup on memory pressure

## Recovery Flow
1. Progressive Recovery Strategy:
- Stage 1: Clear CUDA cache
- Stage 2: Reduce batch size
- Stage 3: Reload model with reduced parameters
- Stage 4: Failover to mock mode

2. Automatic Retries:
- Implements exponential backoff
- Maximum 3 retry attempts
- Falls back to mock data after failures
- Maintains operation logs for diagnostics

Key Paths:
- `dualgpuopt/engine/pool/core.py`: Engine pool management
- `dualgpuopt/gpu/monitor.py`: GPU monitoring
- `dualgpuopt/launcher/process_monitor.py`: Process management

$END$