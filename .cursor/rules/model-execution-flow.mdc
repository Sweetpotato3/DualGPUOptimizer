---
description: Documents the model execution workflow and GPU optimization for large language model inference across multiple GPUs.
globs: **/*.py,**/dualgpuopt/**
alwaysApply: false
---


# model-execution-flow

The system implements a comprehensive model execution pipeline optimized for multi-GPU language model inference:

1. Model Parameter Distribution System (optimizer.py, 90)
- Dynamically splits model layers across GPUs based on memory availability
- Calculates optimal tensor parallel sizes for different model architectures
- Generates environment-specific launch configurations for multiple frameworks (llama.cpp, vLLM)
- Handles specialized architectures like MoE (Mixture of Experts) models

2. Layer Balancing System (layer_balance.py, 85)
- Profiles layer performance using dual-pass strategy:
  * Short sequences (64 tokens) weighted at 20%
  * Long sequences (1024 tokens) weighted at 80%
- Distributes layers across GPUs based on performance characteristics and memory constraints
- Applies optimized block assignments with minimum block size constraints

3. Command Generation Engine (commands/gpu_commands.py, 80)
- Creates framework-specific execution commands
- Configures tensor parallel parameters for multi-GPU deployments
- Manages GPU split ratios and memory allocations
- Handles specialized quantization parameters

4. Memory Management Pipeline (memory/profiler.py, 85)
- Tracks memory patterns across GPU devices
- Implements leak detection using sliding window analysis
- Maintains token count and memory efficiency metrics
- Provides real-time memory alerts and optimization recommendations

5. Process Execution Controller (launcher/launch_controller.py, 75)
- Manages model process lifecycle across GPUs
- Handles OOM detection and recovery
- Implements graceful termination with configurable timeouts
- Coordinates between framework-specific launch configurations

6. Context Size Optimizer (ctx_size.py, 80)
- Calculates maximum safe context length based on:
  * Available GPU memory
  * Model architecture parameters
  * KV cache requirements
  * Overhead factors
- Implements model-specific memory estimation

7. Recovery System (memory/recovery.py, 70)
- Provides tiered recovery strategies:
  * Cache clearing
  * Batch size reduction
  * Memory offloading
  * Process termination
- Implements cascading recovery attempts with escalating impact

$END$