---
description: Specifies the execution flow and orchestration of LLM models across multiple GPUs, including command generation and memory management.
globs: **/run_optimizer.py,**/run_standalone.py,**/runner.py,**/optimizer.py,**/execution/*.py,**/launcher/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow in DualGPUOptimizer centers around orchestrating LLM inference across multiple GPUs with specialized memory management and command generation.

Core Model Execution Components:

1. Command Generation System
- Generates optimized commands for different ML frameworks:
```python
llama.cpp: '--gpu-split 7:3 --n-gpu-layers 32' 
vLLM: '--tensor-parallel-size 2 --max-gpu-memory 0.9'
```
- Automatically determines optimal tensor parallel size based on GPU count
- Calculates memory fraction values per GPU based on relative VRAM sizes
- Custom environment variable generation for proper GPU allocation

2. Memory Management Flow
- Per-model memory profiling:
```python
llama2-70b:
  base_usage: 35GB
  per_batch: 350MB
  per_token: 18KB
```
- Dynamic memory projections for out-of-memory prevention
- Multi-tier recovery system:
  1. Cache clearing
  2. Batch reduction
  3. Memory offloading
  4. Process termination
- Context size validation against available VRAM

3. Layer Distribution 
- Balanced layer assignment across GPUs based on:
  - Relative GPU memory capacity
  - Layer compute intensity
  - GPU bandwidth characteristics
- Custom heuristics for MoE model layer distribution
- Automatic rebalancing on memory pressure

4. Execution Orchestration
- Framework-specific process launching with optimized parameters
- Non-blocking output handling for model processes
- GPU resource monitoring and throttling
- Process lifecycle management with graceful termination

5. Resource Allocation Flow
- Automatic GPU capability detection
- Memory split ratio calculations based on free VRAM
- Dynamic adjustment of tensor parallel parameters
- Framework-specific optimization parameters

Key Domain Rules:
- Minimum 2GB free memory required per GPU
- Context size must not exceed available VRAM
- Tensor parallel size limited by GPU count
- MoE models require special layer distribution

File Paths:
- dualgpuopt/optimizer.py (command/memory optimization)
- dualgpuopt/runner.py (process management)
- dualgpuopt/services/gpu_commands.py (command generation)
- dualgpuopt/launcher/launch_controller.py (execution orchestration)

$END$