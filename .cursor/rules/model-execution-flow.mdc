---
description: Specification for GPU-accelerated model execution flows, command generation, and resource allocation in ML inference
globs: **/dualgpuopt/**/*.py,**/dual_gpu_optimizer/**/*.py,**/integrated_app/**/*.py
alwaysApply: false
---


# model-execution-flow

Core Execution Flow Components:

1. Model Launch Orchestration (`dualgpuopt/gui/launcher.py`):
- Generates optimized commands for different ML frameworks:
```python
def get_llama_command(model_path, ctx_size, split):
    return f"--model {model_path} --gpu-split {split} --ctx-size {ctx_size}"

def get_vllm_command(model_path, tensor_parallel):
    return f"--model {model_path} --tensor-parallel-size {tensor_parallel}"
```
- Handles model parameter detection from filenames
- Implements dynamic context size optimization
- Controls GPU resource allocation

2. Memory Management (`dualgpuopt/memory_monitor.py`):
- Tracks per-token and per-batch memory patterns
- Defines model-specific memory profiles:
  - Llama2: Base + KV cache scaling
  - Mixtral: Expert gating overhead
  - Mistral: Sliding window attention
- Implements four-level alert system (Normal/Warning/Critical/Emergency)

3. GPU Layer Distribution (`dualgpuopt/layer_balance.py`):
- Calculates optimal layer distribution across GPUs
- Uses weighted performance profiling (20% short sequences, 80% long sequences)
- Maintains contiguous layer blocks to minimize cross-GPU communication
- Assigns input embeddings to first GPU, output components to second GPU

4. Model Parameter Configuration (`dualgpuopt/ctx_size.py`):
- Calculates safe context lengths based on:
  - Available GPU memory
  - Model architecture (layers, heads, hidden size)
  - MoE factor for expert models
  - Quantization settings
- Implements model-specific overrides for popular architectures

5. Command Generation and Resource Allocation:
- Produces framework-specific launch commands
- Sets environment variables for GPU visibility
- Configures tensor parallel sizes
- Manages GPU memory thresholds

Critical execution paths include model initialization, layer distribution calculation, memory monitoring, and dynamic resource reallocation based on monitoring alerts.

$END$