---
description: Defines how language models are executed across multiple GPUs, including command generation, resource allocation, and monitoring flows
globs: **/optimizer.py,**/gpu/*.py,**/launcher/*.py,**/memory/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

Core execution components manage LLM deployment across multiple GPUs:

1. Model Configuration Resolution
- Context length calculation based on model architecture and available GPU memory
- Model size detection from name patterns (7B, 13B, 70B, etc.)
- Parameter validation including layer count, attention heads, and KV ratios
- Memory requirement calculation incorporating:
  * KV cache overhead (default 2x)
  * Tensor parallelism factors (20%)
  * System buffer (2GB)
  * Safety margin (10%)

2. GPU Resource Allocation
- Dynamic GPU split calculation based on available memory
- Tensor parallel size determination for vLLM
- Layer distribution optimization across GPUs
- Memory monitoring and OOM prevention through:
  * Cache clearing
  * Batch size reduction
  * Process termination

3. Execution Command Generation
`dualgpuopt/optimizer.py`:
```python
def generate_commands():
    # Generate framework-specific commands
    llama_cmd = f"--split {gpu_split} --ctx-size {ctx_size}"
    vllm_cmd = f"--tensor-parallel-size {tp_size} --max-length {ctx_size}"
```

4. Batch Processing Management
`dualgpuopt/batch/smart_batch.py`:
- Length-aware sequence batching
- Adaptive batch size based on GPU memory
- OOM recovery with 25% reduction steps
- Backpressure mechanism for overload prevention

5. Process Monitoring and Recovery
- Real-time GPU metrics tracking
- Automatic VRAM reset on high memory pressure
- Exponential backoff for failed operations
- Graceful process termination handling

Key execution paths:
- Model parameter resolution → GPU allocation → Command generation
- Batch sequence processing → Memory monitoring → Recovery actions
- Metrics collection → Resource optimization → Process management

$END$