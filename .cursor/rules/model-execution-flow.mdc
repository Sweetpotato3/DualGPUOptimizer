---
description: Defines LLM model execution workflows, command generation, GPU split configuration, and backend selection logic.
globs: */*/model/*,*/*/engine/*,*/*/batch/*,*/*/commands/*,*/*/services/*
alwaysApply: false
---


# model-execution-flow

## Command Generation and Model Execution
The system implements framework-specific command generation for both llama.cpp and vLLM backends:

```python
# Framework-specific command generation 
llama_cmd = f"--model {model_path} --ctx-size {ctx_size} --gpu-split {split}"
vllm_cmd = f"--model {model_path} --tensor-parallel-size {gpu_count}"
```

## Model Distribution Strategy
- Calculates optimal GPU split ratios based on available VRAM and model size
- Adjusts tensor parallelism based on number of available GPUs
- Handles memory calculations with safety margins:
  - System overhead: 2GB reserved
  - Tensor parallel overhead: 20%
  - KV cache scaling: 2x multiplier

## Backend Selection Logic
1. Selection based on model format:
   - GGUF files -> llama.cpp backend
   - AWQ files -> vLLM backend
   - HF models -> HFBackend
   
2. Process management:
   - Non-blocking process execution for inference
   - Output streaming with token processing
   - Resource cleanup on process termination

## Batch Processing
Smart batching implementation with:
- Length-aware sequence batching
- Token-based backpressure 
- Memory quota enforcement
- OOM recovery and retry

## Engine Pool Management
- LRU caching of loaded models
- Health monitoring with auto-recovery
- Backend-specific validation
- Resource cleanup

Key file paths:
- dualgpuopt/engine/backend.py
- dualgpuopt/engine/pool.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/commands/gpu_commands.py

$END$