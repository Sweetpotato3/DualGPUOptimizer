---
description: Model execution and optimization rules for large language models across multiple GPUs, handling tensor parallelism and memory management.
globs: **/optimizer.py,**/commands/*.py,**/runner.py,**/ctx_size.py,**/layer_balance.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized logic for optimizing large language model inference across multiple GPUs:

## Core Model Execution Components

1. **Framework-Specific Command Generation**
- Generates optimized commands for llama.cpp and vLLM frameworks
- Configures tensor parallel size based on available GPUs 
- Implements memory split ratios based on relative GPU VRAM sizes
- Handles specialized quantization parameters and context lengths

2. **Dynamic Context Size Management**
- Calculates maximum safe context lengths based on:
  - Available GPU VRAM
  - Model architecture (MQA/GQA/Standard)
  - KV cache requirements
  - MoE (Mixture of Experts) configurations
- Automatically detects model architecture from model names
- Implements safety margins to prevent OOM conditions

3. **Layer Distribution Optimization** 
- Profiles layer execution times across sequence lengths
- Distributes transformer layers between GPUs based on:
  - GPU memory capacity
  - Layer performance characteristics 
  - Memory quotas per GPU
- Optimizes for contiguous layer blocks to reduce inter-GPU communication

4. **Memory Recovery Strategies**
- Implements tiered recovery approaches:
  - CUDA cache clearing
  - Batch size reduction
  - Model offloading
  - Process termination
- Tracks memory pressure and triggers recovery before OOM errors

## Key Files

- `dualgpuopt/ctx_size.py`: Context size calculations and model detection
- `dualgpuopt/layer_balance.py`: Layer distribution optimization
- `dualgpuopt/optimizer.py`: Command generation and GPU split optimization
- `dualgpuopt/memory/recovery.py`: Memory recovery strategies
- `dualgpuopt/commands/gpu_commands.py`: Framework-specific command builders

The execution flow focuses on optimizing model inference across multiple GPUs while maintaining stability and preventing out-of-memory conditions. It handles framework-specific requirements, dynamic context sizing, and intelligent layer distribution.

$END$