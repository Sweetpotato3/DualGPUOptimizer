---
description: Documents how LLM model execution is orchestrated across GPUs including command generation and resource management
globs: **/optimizer.py,**/runner.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

Key components of the model execution flow:

## Command Generation
- `llama_command()` generates execution commands for llama.cpp models with:
  - GPU memory split configuration
  - Context size settings
  - Model path and parameters

- `vllm_command()` generates commands for vLLM server execution with:
  - Tensor parallel configuration 
  - GPU device mapping

## Batch Processing
The SmartBatcher handles asynchronous inference scheduling:
- Groups requests by sequence length into buckets
- Implements back-pressure via queue depth limits
- Handles OOM errors by clearing GPU cache and retrying
- Supports configurable bucketing policies (power-of-2, token ratio)

## Layer Distribution
Adaptive layer distribution across GPUs:
- Profiles layer execution times
- Redistributes layers based on latency metrics
- Generates device mapping JSON for layer placement
- Handles mixed precision policies per layer

## GPU Resource Management
- Calculates tensor fractions based on available GPU memory
- Creates environment configurations for CUDA/NCCL
- Provides mock GPU mode for testing
- Monitors GPU metrics during execution

## Command Pattern Implementation
Commands for GPU operations:
- ApplyOverclockCommand: Manages GPU overclocking settings
- EnableMockGpuCommand: Controls mock GPU mode
- Command history and undo/redo support
- Event publishing for command execution results

Key files:
- `optimizer.py`: Command generation and resource allocation
- `batch/smart_batch.py`: Batch processing implementation
- `commands/gpu_commands.py`: GPU operation commands
- `layer_balance.py`: Layer distribution logic

$END$