---
description: Technical specification for model execution flow patterns and GPU resource management in ML inference systems
globs: **/gpu/**,**/gpu_info.py,**/optimizer.py,**/runner.py,**/launcher/**
alwaysApply: false
---


# model-execution-flow

The model execution flow system implements specialized logic for managing large language model inference across dual GPU configurations:

1. Framework-Specific Command Generation
```python
# Generates optimized commands for different ML frameworks
llama_command(model_path, ctx, split) # GPU-split layer distribution
vllm_command(model_path, tp_size) # Tensor parallel optimization
```

2. GPU Memory Management Flow
- Pre-inference validation of available VRAM
- Dynamic batch size adjustment based on token counts
- Automatic OOM recovery with cache clearing
- Memory monitoring with leak detection
- Progressive fallback mechanisms:
  * REDUCE_BATCH: Scale down batch size 
  * CLEAR_CACHE: Clear GPU caches
  * OFFLOAD: Move to CPU/disk
  * TERMINATE: Kill process

3. Model Parameter Resolution
- Context length calculation based on:
  * Available GPU memory
  * Model architecture (layers, heads)
  * Quantization method
- Split ratio determination for dual GPUs
- Layer distribution optimization

4. Execution Control Flow
- Model initialization with memory validation
- Background telemetry monitoring
- Asynchronous token streaming
- Real-time memory profiling
- Automatic error recovery

Key Paths:
- dualgpuopt/commands/gpu_commands.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/runner.py
- dualgpuopt/memory/recovery.py

$END$