---
description: Documents the execution flow and orchestration of language models across multiple GPUs, including command generation and resource allocation
globs: **/optimizer.py,**/runner.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

## Core Execution Components

### Command Generation System
Located in `dualgpuopt/optimizer.py`:
- Generates framework-specific deployment commands:
  - llama.cpp: GPU split ratios and memory allocation parameters
  - vLLM: Tensor parallelism configuration and GPU distribution
- Implements dynamic parameter calculation based on available GPU memory
- Handles model-specific configuration (context size, layer distribution)

### Batch Processing Pipeline 
Located in `dualgpuopt/batch/smart_batch.py`:
- Length-aware sequence batching with dynamic sizing
- Implements backpressure mechanism for memory management
- Automatic retry logic for OOM conditions
- Token-based batch composition with configurable limits

### Model Runner Orchestration
Located in `dualgpuopt/runner.py`:
- Controls model lifecycle across multiple GPUs
- Manages process execution and monitoring
- Handles framework-specific launch configurations:
  - Memory splits for llama.cpp
  - Tensor parallel sizes for vLLM
- Real-time output monitoring with non-blocking queues

### Resource Allocation Flow
Located in `dualgpuopt/layer_balance.py`:
- Adaptive layer distribution based on GPU capabilities
- Dual-length profiling (64/1024 tokens) for balanced optimization
- Memory quota management with configurable reserve ratios
- Dynamic redistribution based on performance metrics

### GPU Command Pipeline
Located in `dualgpuopt/commands/gpu_commands.py`:
- Framework-specific command string generation
- Environment configuration for multi-GPU setups
- Tensor parallelism parameter calculation
- Model preset handling for common configurations

### Execution Safety Controls
- Minimum 2 GPU requirement enforcement
- 90% memory utilization cap for stability
- Automatic failover for hardware errors
- GPU capability validation before execution

Importance Scores:
- Command Generation: 95 (core optimization logic)
- Batch Processing: 90 (critical for execution efficiency)
- Runner Orchestration: 85 (key execution control)
- Resource Allocation: 90 (essential optimization)
- GPU Command Pipeline: 80 (framework integration)
- Safety Controls: 75 (operational stability)

$END$