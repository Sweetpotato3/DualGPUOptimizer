---
description: Documents the core model execution workflows for LLM inference, including GPU allocation and command generation
globs: **/optimizer.py,**/runner.py,**/telemetry.py
alwaysApply: false
---


# model-execution-flow

## Core Execution Components (Score: 95)

The model execution flow consists of three main components:

1. GPU Allocation System
- Calculates optimal tensor splits across multiple GPUs based on memory capacity
- Generates device allocation strings for frameworks
- Produces environment configurations with CUDA_VISIBLE_DEVICES settings

2. Command Generation Engine
- Creates framework-specific execution commands for:
  - llama.cpp inference
  - vLLM serving
- Incorporates GPU split configurations into command parameters
- Handles model path and context size parameters

3. Process Management
- Non-blocking subprocess execution for LLM servers
- Real-time log streaming from model processes
- Process lifecycle management

## Runtime Flow (Score: 85)

1. GPU Configuration
```
Load GPU Details -> Calculate Split String -> Generate Environment File
```

2. Model Launch
```
Parse Model Settings -> Generate Framework Command -> Execute via Runner
```

3. Monitoring Loop
```
Start Telemetry -> Monitor GPU Usage -> Handle Idle Detection
```

## Framework Integration (Score: 80)

Specific command generation logic for:

1. llama.cpp
- GPU split parameter generation
- Context size configuration
- Model path handling

2. vLLM
- Tensor parallel configuration
- GPU device mapping
- Server configuration parameters

$END$