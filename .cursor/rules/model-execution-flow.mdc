---
description: Defines and documents how LLM model execution is handled, including process launching, parameter management, and GPU resource allocation.
globs: '**/*.py','**/gui/launcher/**','**/gpu/**','**/optimizer/**'
alwaysApply: false
---


# model-execution-flow

Core Model Execution Components:

1. Framework-Specific Command Generation
- Generates optimized command strings for llama.cpp and vLLM
- Handles parameter distribution between GPUs based on memory capacity
- Built-in presets for common models (Llama-2, Mixtral, Mistral)
- Dynamic tensor parallel size calculation for vLLM
- GPU split ratio optimization for llama.cpp
- File: dualgpuopt/commands/gpu_commands.py

2. Smart Batch Processing
- Length-aware batch scheduling with profiled memory constraints
- Adaptive batch size adjustment based on OOM events
- Backpressure mechanism for preventing memory exhaustion
- Token count tracking and optimization
- File: dualgpuopt/batch/smart_batch.py

3. GPU Memory Management 
- Real-time memory monitoring across multiple GPUs
- Memory leak detection with configurable thresholds
- Pre/post inference memory baseline tracking
- Custom memory pattern analysis for growth detection
- Files: dualgpuopt/memory/profiler.py, dualgpuopt/memory/monitor.py

4. Model Parameter Validation
- Validates model architectures and configurations
- Enforces constraints on head counts and dimensions
- Handles specialized MoE model requirements
- Implements validation rules for tensor parallel setups
- File: dualgpuopt/optimizer.py

5. Resource Allocation Flow
- Dynamic GPU split calculation based on available memory
- Tensor parallel processing configuration
- KV cache allocation optimization
- Safety margin enforcement for stable execution
- Files: dualgpuopt/optimizer.py, dualgpuopt/layer_balance.py

6. Launch Process Management
- Non-blocking execution with output streaming
- Process lifecycle tracking and cleanup
- Error recovery and automatic retries
- Environment variable configuration
- File: dualgpuopt/runner.py

The system implements sophisticated model execution workflows with particular focus on memory management, resource allocation, and process control across multiple GPUs. Key aspects include dynamic resource distribution, intelligent batch processing, and framework-specific optimizations.

$END$