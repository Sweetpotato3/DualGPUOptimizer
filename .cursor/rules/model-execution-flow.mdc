---
description: Manages execution flows and resource allocation for large language model deployment across multiple GPUs.
globs: **/gpu/*.py,**/gpu/**/*.py,**/optimizer/*.py,**/runner/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

Core LLM Execution Components:

1. GPU Split Configuration (optimizer.py)
- Generates optimal GPU memory splits based on:
  * Model architecture parameters 
  * Available GPU memory
  * Tensor parallel configurations
  * Requested context length
- Dynamically adjusts splits between 0.25 and 0.75 ratios
- Handles asymmetric GPU memory scenarios

2. Execution Flow Management (runner.py)
- Pipeline orchestration for model loading and inference:
  1. Initial GPU memory validation
  2. Model weight distribution
  3. KV cache allocation
  4. Inference process initialization
- Provides framework-specific launch parameters for:
  * llama.cpp: GPU split, layer distribution
  * vLLM: Tensor parallel size, memory fraction

3. Resource Management (batch/smart_batch.py)
- Length-aware batch scheduling with:
  * Dynamic batch size adjustment
  * Token count tracking per sequence
  * Memory pressure monitoring
  * OOM prevention through backpressure
- Recovery logic with 25% batch reduction on OOM

4. Framework-Specific Command Generation
- llama.cpp execution:
```python
command = [
    f"--tensor-split {split_ratio}",
    f"--ctx-size {context_length}",
    f"--batch-size {batch_size}"
]
```
- vLLM deployment:
```python
command = [
    f"--tensor-parallel-size {gpu_count}",
    f"--max-num-batched-tokens {tokens_per_batch}",
    f"--gpu-memory-utilization {mem_fraction}"
]
```

File Paths:
```
dualgpuopt/optimizer.py - Split calculation logic
dualgpuopt/runner.py - Execution orchestration 
dualgpuopt/batch/smart_batch.py - Batch management
```

Importance Scores:
- GPU Split Configuration: 95 (Core resource allocation)
- Execution Flow Management: 90 (Critical orchestration)
- Resource Management: 85 (Key optimization)
- Command Generation: 75 (Framework integration)

$END$