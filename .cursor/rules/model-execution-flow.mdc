---
description: Defines the execution workflow and orchestration of LLM models across multiple GPUs and frameworks
globs: **/*.py,**/model/**,**/execution/**,**/runner/**
alwaysApply: false
---


# model-execution-flow

## Core Model Execution Components (Importance: 95)

1. Framework-Specific Model Deployment
- Specialized command generation for llama.cpp and vLLM frameworks
- Dynamic tensor parallelism configuration for vLLM
- GPU layer splitting logic for llama.cpp
- Environment variable orchestration based on GPU configuration

Files:
- dualgpuopt/optimizer.py
- dualgpuopt/runner.py

## Smart Batching System (Importance: 90)

1. Dynamic Batch Scheduling
- Length-aware inference queue management
- Token count based batch optimization
- OOM recovery with cache clearing mechanism
- Two specialized bucketing strategies:
  - Power-of-two sequence grouping
  - Token ratio maintenance between buckets

Files:
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/batch/heuristics.py

## Layer Distribution Engine (Importance: 85)

1. Adaptive Layer Balancing 
- Latency-aware neural network layer distribution
- Dual sequence length profiling (64/1024 tokens)
- 90% memory reservation quota enforcement
- Dynamic workload distribution based on execution time

Files:
- dualgpuopt/layer_balance.py

## Context Management (Importance: 80)

1. Model Context Calculation
- Maximum safe context length determination
- Model-specific parameter handling:
  - Layer count
  - KV heads configuration  
  - Head dimensions
  - Precision bits
  - MoE factor handling
- Multi-GPU memory buffer allocation

Files:
- dualgpuopt/ctx_size.py

## Execution State Management (Importance: 75)

1. Model Process Lifecycle
- Resource allocation orchestration
- Process state transitions
- GPU configuration state tracking
- Framework initialization sequencing

Files:
- dualgpuopt/gui/launcher.py
- dualgpuopt/services/state_service.py

$END$