---
description: Guidelines for model execution workflow including LLM command generation, GPU management, and resource allocation
globs: **/launcher.py,**/gpu_info.py,**/optimizer.py,**/runner.py,**/layer_balance.py
alwaysApply: false
---


# model-execution-flow

Core Execution Components:

1. Model Runner System (dualgpuopt/gui/launcher.py)
Importance Score: 95
- Manages model deployment across dual GPUs with memory optimization
- Implements specialized command generation for llama.cpp and vLLM frameworks
- Handles OOM recovery through staged memory reclamation
- Coordinates tensor parallelism based on GPU capabilities

2. Layer Distribution Engine (dualgpuopt/layer_balance.py)
Importance Score: 90
- Distributes model layers across GPUs using weighted profiling:
  * Short sequence profiling (64 tokens) - 20% weight
  * Long sequence profiling (1024 tokens) - 80% weight
- Forces input embeddings to first GPU and output layers to last GPU
- Creates contiguous layer blocks to minimize cross-GPU communication

3. Context Size Calculator (dualgpuopt/ctx_size.py)
Importance Score: 85
- Implements model-specific context length calculation:
  * Base memory requirements
  * Per-token KV cache scaling
  * MoE architecture overhead
  * Quantization impact factors
- Enforces safety margins through 20% memory reservation

4. Framework Command Generation (dualgpuopt/optimizer.py)
Importance Score: 80
- Creates optimized execution commands for supported frameworks:
  * llama.cpp: GPU split configuration, layer distribution
  * vLLM: Tensor parallel settings, batch size optimization
- Manages environment variables for GPU memory allocation
- Handles quantization parameters and precision settings

5. Memory Monitoring System (dualgpuopt/memory_monitor.py)
Importance Score: 75
- Tracks real-time memory utilization across GPUs
- Implements progressive memory recovery:
  * Cache clearing
  * Batch size reduction
  * Process termination
- Maintains model-specific memory profiles with pre-configured parameters

Key Execution Paths:
1. Model Loading:
- Architecture detection from model name
- Memory requirement calculation
- GPU split ratio determination
- Layer distribution optimization

2. Execution Flow:
- Command generation for target framework
- Resource allocation across GPUs
- Memory monitoring and recovery
- Performance metric collection

$END$