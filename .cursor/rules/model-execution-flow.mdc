---
description: Defines GPU model execution flow including command generation for llama.cpp and vLLM, configuration parameters, and resource allocation logic.
globs: **/dualgpuopt/**/*.py,**/gpu/**/*.py,**/runner.py,**/optimizer.py
alwaysApply: false
---


# model-execution-flow

## Command Generation and GPU Split Logic
- Generates framework-specific commands with tensor parallelism considerations
- llama.cpp commands include: GPU split ratios, context size, layers per GPU
- vLLM commands configure: tensor parallel degree, max length, memory utilization
- Supports flexible GPU distribution through split ratios (e.g. 70/30, 60/40)

File: `dualgpuopt/commands/gpu_commands.py`

## Model Resource Allocation
- Calculates memory requirements based on model architecture:
  - Base model size
  - KV cache per token
  - Attention mechanism overhead
  - MoE overhead for expert models
- Implements custom per-token memory estimation:
  `bytes_per_token = kv_hidden_size * num_layers * 2`
- Considers BF16 and AWQ quantization impacts

File: `dualgpuopt/optimizer.py`

## GPU Resource Management
- Dynamic GPU split optimization based on:
  - Available VRAM per GPU
  - Model architecture requirements
  - Minimum 20% allocation per GPU policy
  - MoE expert distribution constraints
- Layer distribution strategies:
  - Single GPU fallback mode
  - Dual GPU balanced distribution
  - Multi-GPU load balancing

File: `dualgpuopt/gpu_info.py`

## Execution Flow Control
- Non-blocking subprocess execution with log streaming
- Real-time output processing for model status
- Framework-specific environment variable configuration:
  - CUDA device connections
  - NCCL P2P settings
  - Visible devices configuration

File: `dualgpuopt/runner.py`

## Process Recovery and Monitoring
- Implements adaptive error handling:
  - Memory pressure detection
  - Batch size reduction by 25% on OOM
  - Automatic cache clearing
  - Process restart on critical failures
- GPU state monitoring during execution

File: `dualgpuopt/memory/recovery.py`

$END$