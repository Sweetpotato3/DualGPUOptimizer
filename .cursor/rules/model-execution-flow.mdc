---
description: Manages the execution flow and orchestration of LLM models across GPU resources using llama.cpp and vLLM
globs: **/*optimizer.py,**/*runner.py,**/*gpu_info.py
alwaysApply: false
---


# model-execution-flow

### Command Generation (Importance: 95)
- `/dualgpuopt/optimizer.py`
  - Generates executable commands for both llama.cpp and vLLM runtimes
  - Constructs GPU split configurations based on available memory
  - Sets environment variables for optimal tensor parallel execution
  - Configures CUDA device visibility and NCCL parameters

### GPU Resource Allocation (Importance: 90)
- `/dualgpuopt/optimizer.py`
  - Calculates tensor fractions for memory distribution across GPUs
  - Determines optimal GPU splits based on model requirements
  - Maps model parameters to available GPU resources

### Execution Environment Setup (Importance: 85)
- `/dualgpuopt/optimizer.py`
  - Creates environment files with GPU-specific configurations
  - Sets up CUDA device mappings
  - Configures OpenMP and NCCL parameters for multi-GPU execution

### Model Runtime Flow (Importance: 80)
- `/dualgpuopt/runner.py`
  - Orchestrates model execution across selected GPUs
  - Manages context size allocation
  - Handles runtime switching between llama.cpp and vLLM backends

### Resource Monitoring (Importance: 75)
- `/dualgpuopt/telemetry.py`
  - Tracks GPU utilization during model execution
  - Monitors memory allocation and PCIe throughput
  - Provides real-time execution metrics

### Resource State Management (Importance: 70)
- `/dualgpuopt/gpu_info.py`
  - Maintains current GPU allocation state
  - Tracks available memory across devices
  - Provides device capability information for execution planning

$END$