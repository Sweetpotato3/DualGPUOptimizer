---
description: Defines how LLM models are loaded and executed, including command generation, batching, and GPU resource handling
globs: **/services/*.py,**/batch/*.py,**/commands/*.py,**/launcher/*.py
alwaysApply: false
---


# model-execution-flow

Core Model Execution Components (Importance: 95):

1. Framework-Specific Launch Orchestration
- Specialized command generation for LLM frameworks:
  * llama.cpp: GPU split optimization with memory fraction calculations
  * vLLM: Tensor parallel size configuration and memory allocation
- Model-specific parameter optimization:
  * Context length based on model architecture
  * Layer distribution for multi-GPU deployments
  * Memory per token allocation tracking

File paths:
- dualgpuopt/commands/gpu_commands.py
- dualgpuopt/gui/launcher/launch_controller.py

2. Smart Batching System (Importance: 90):
- Length-aware sequence batching for optimized throughput
- Dynamic batch size calculation considering:
  * Available GPU memory per device
  * Model architecture requirements
  * Token sequence lengths
  * Safety margins for OOM prevention
- Backpressure mechanism with configurable queue depths

File paths:
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/batch/heuristics.py

3. Model Loading Pipeline (Importance: 85):
- Progressive loading states with validation
- Memory prediction for model requirements
- Resource allocation verification across GPUs
- Layer distribution optimization for multi-GPU setups

File paths:
- dualgpuopt/services/event_service.py
- dualgpuopt/gui/launcher/parameter_resolver.py

4. Execution Monitoring (Importance: 80):
- Real-time token processing rate tracking
- GPU memory utilization monitoring
- Framework-specific error handling
- OOM detection and recovery strategies

File paths:
- dualgpuopt/memory/profiler.py
- dualgpuopt/memory/monitor.py

The system implements a comprehensive model execution pipeline optimized for dual-GPU configurations, with specialized handling for different ML frameworks and dynamic resource management based on model architectures and GPU capabilities.

$END$