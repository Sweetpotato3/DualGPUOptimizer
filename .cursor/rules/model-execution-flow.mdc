---
description: Used for analyzing and documenting LLM model execution workflows, command generation, and GPU resource management
globs: **/gpu_info.py,**/optimizer.py,**/runner.py,**/launcher.py
alwaysApply: false
---


# model-execution-flow

The model execution flow consists of several key components that manage the LLM execution pipeline:

### Command Generation (Importance: 95)
- Generates optimized command strings for running models with llama.cpp and vLLM
- Takes into account GPU memory splits and tensor fractions based on available GPU resources
- Located in `dualgpuopt/optimizer.py`

### GPU Resource Allocation (Importance: 90)
- Calculates optimal GPU memory splits based on detected GPU configurations
- Generates environment files with CUDA and NCCL configurations specific to GPU setup
- Dynamically adjusts tensor fractions based on available GPU memory
- Located in `dualgpuopt/gpu_info.py`

### Model Execution Pipeline (Importance: 85)
- Handles model initialization and execution across multiple GPUs
- Manages process logging and execution state
- Enables starting/stopping model execution with selected frameworks
- Located in `dualgpuopt/gui/launcher.py`

### Framework-Specific Workflows (Importance: 80)
Two primary execution paths:

1. llama.cpp flow:
   - Generates split configuration strings
   - Sets up multi-GPU environment variables
   - Configures memory and compute parameters

2. vLLM flow:
   - Manages tensor parallel configurations
   - Sets up GPU device mapping
   - Configures framework-specific parameters

Located in `dualgpuopt/runner.py`

### Telemetry Integration (Importance: 75)
- Streams real-time GPU metrics during model execution
- Monitors memory utilization and GPU load
- Provides feedback for execution optimization
- Located in `dualgpuopt/telemetry.py`

### Mock Execution Mode (Importance: 65)
- Simulates GPU presence for testing execution flows
- Generates mock telemetry data
- Enables workflow testing without physical GPUs
- Located in `dualgpuopt/gpu_info.py`

$END$