---
description: Defines model execution workflows, command generation and GPU resource allocation for LLM inference
globs: **/runner.py,**/launcher/*.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

The model execution system handles two primary LLM frameworks (llama.cpp and vLLM) with specialized workflow management:

1. Command Generation System
- Generates optimized framework-specific commands:
```python
# llama.cpp format
./main -m {model_path} --gpu-split {split} --n-gpu-layers {layers} --ctx-size {ctx}

# vLLM format  
python -m vllm.entrypoints.api_server --model {model_path} --tensor-parallel-size {gpus}
```

- Dynamically calculates GPU split ratios based on available VRAM
- Handles tensor parallel configuration for multi-GPU deployments
- Manages context size limits based on available memory

2. Batch Processing Pipeline
- Implements length-aware sequence batching
- Uses token-based batch sizing with configurable limits
- Handles OOM recovery through cache clearing and batch size reduction
- Maintains token throughput tracking per batch

3. Resource Allocation Flow
- Pre-execution GPU memory validation
- Dynamic layer distribution across GPUs
- Custom error handling for memory allocation failures
- Environmental variable generation for GPU configurations

Core Execution Rules:
- Memory validation before model loading
- Progressive batch size reduction on OOM (25% reduction)
- Minimum 5 successful batches before size increase
- Tensor parallel size must match available GPU count

Relevant Files:
- dualgpuopt/commands/gpu_commands.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/runner.py

$END$