---
description: Specification for LLM execution workflows, command generation, and GPU resource allocation patterns
globs: **/optimizer.py,**/runner.py,**/gpu_info.py,**/telemetry.py
alwaysApply: false
---


# model-execution-flow

### GPU Resource Management Flow (Importance: 85)
`dualgpuopt/optimizer.py` and `dualgpuopt/gpu_info.py`
- Parallel GPU discovery using NVML to detect available devices
- Memory capacity analysis for each GPU to calculate split ratios
- Dynamic tensor fraction computation based on available GPU memory
- Generation of CUDA device visibility configuration

### Model Execution Pipeline (Importance: 95)
`dualgpuopt/optimizer.py`
1. Command Generation
   - llama.cpp command construction with GPU split parameters
   - vLLM command generation with resource allocation configs
   - Environment file creation with CUDA device mappings

2. Runtime Configuration
   - Context size determination for model loading
   - GPU memory split string calculation
   - Model path validation and parameter mapping

### Resource Monitoring (Importance: 75)
`dualgpuopt/telemetry.py`
- Real-time GPU utilization tracking through NVML interface
- Memory usage monitoring for tensor allocation
- PCIe throughput measurement for data transfer optimization
- Idle state detection for resource optimization

### Execution Control Flow (Importance: 80)
`dualgpuopt/runner.py`
- Model launch orchestration across multiple GPUs
- Runtime parameter adjustment based on GPU availability
- Execution log capture and processing
- Command validation before execution

$END$