---
description: Defines execution flows and resource allocation for GPU-accelerated LLM inference using llama.cpp and vLLM frameworks
globs: **/services/*.py,**/gui/launcher.py,**/optimizer.py,**/runner.py
alwaysApply: false
---


# model-execution-flow

Core command generation and execution workflow for LLM inference:

1. Model Execution Pipeline (dualgpuopt/optimizer.py, dualgpuopt/runner.py)
- Generates framework-specific launch commands:
  ```
  llama.cpp: --n-gpu-layers split --split [ratio] --ctx-size [context]
  vLLM: --tensor-parallel-size [num_gpus] --max-model-len [context]
  ```
- Memory calculation logic:
  - Computes GPU split ratios based on available VRAM
  - Adjusts context sizes according to model requirements
  - Determines tensor parallel size based on GPU count

2. Resource Allocation Flow (dualgpuopt/services/state_service.py)
- Maintains resource state:
  - Active model paths and context window sizes
  - Current GPU memory allocations
  - Tensor parallel configurations
- Tracks idle thresholds for optimization

3. Launch Orchestration (dualgpuopt/gui/launcher.py)
- Framework-specific launch strategies:
  - llama.cpp: GPU splits and context management
  - vLLM: Tensor parallelism configuration
- Non-blocking process management with output handling

4. Configuration Management (dualgpuopt/services/config_service.py)
- Manages execution parameters:
  - Model context sizing
  - GPU memory thresholds
  - Framework-specific optimizations

Importance Scores:
- Command Generation Pipeline: 95 (Core execution logic)
- Resource Allocation: 90 (Critical for multi-GPU coordination)
- Launch Orchestration: 85 (Framework integration)
- Configuration Management: 75 (Execution parameters)

$END$