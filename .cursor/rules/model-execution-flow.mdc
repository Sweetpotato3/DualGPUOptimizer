---
description: Specifies the LLM model execution workflow, command generation, memory management and GPU allocation patterns
globs: **/dualgpuopt/**/*.py,**/integrated_app/**/*.py,**/hooks/**/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized handling of LLM inference across dual GPU configurations:

## Command Generation
Located in `dualgpuopt/commands/gpu_commands.py`:

1. Framework-Specific Command Generation:
- Generates optimized execution commands for:
  - llama.cpp: GPU split ratios, context sizes, thread counts
  - vLLM: Tensor parallel sizes, memory allocations
- Handles environment variable configurations for each framework
- Importance Score: 95

2. Memory Split Calculator:
- Calculates optimal memory distribution across GPUs based on:
  - Model architecture requirements (layers, attention heads)
  - Available GPU memory ratios
  - KV cache requirements per token
- Importance Score: 90

## Runtime Execution Management
Located in `dualgpuopt/runner.py`:

1. Model Server Lifecycle:
- Non-blocking process management for AI model servers
- Real-time output streaming with process isolation
- Graceful termination with two-phase shutdown
- Importance Score: 85

2. Memory Recovery System:
- Hierarchical recovery strategies:
  - Batch size reduction
  - Cache clearing
  - Memory offloading
  - Process termination
- GPU-specific recovery targeting
- Importance Score: 80

## Resource Optimization
Located in `dualgpuopt/batch/smart_batch.py`:

1. Length-Aware Batching:
- Dynamic batch size calculation based on:
  - Sequence lengths
  - GPU memory constraints
  - Model architecture
- Token-based optimization with configurable limits
- Importance Score: 85

2. Memory Pressure Management:
- OOM prevention through:
  - Proactive batch size adjustment
  - Cache purging
  - Memory reclamation
- Memory usage projection for preventive action
- Importance Score: 75

## GPU Resource Management
Located in `dualgpuopt/gpu_info.py`:

1. Resource Monitoring:
- Real-time tracking of:
  - GPU utilization
  - Memory allocation
  - Temperature thresholds
  - Power consumption
- Importance Score: 70

2. Load Distribution:
- Layer distribution optimization across GPUs
- Tensor parallel configuration management
- Memory quota enforcement per GPU
- Importance Score: 85

$END$