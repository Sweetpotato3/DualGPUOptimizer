---
description: Documents LLM model execution strategies, command generation and GPU resource management for optimized inference
globs: **/gpu_commands.py,**/launch_controller.py,**/optimizer.py,**/runner.py
alwaysApply: false
---


# model-execution-flow

The system implements specialized execution flows for running large language models across multiple GPUs:

1. Command Generation Pipeline (dualgpuopt/commands/gpu_commands.py)
- Framework-specific command construction for llama.cpp and vLLM backends
- Tensor parallel configuration based on available GPU count
- GPU memory split ratio calculations for model distribution
- Environment variable setup for each framework's requirements

2. Launch Controller Logic (dualgpuopt/gui/launcher/launch_controller.py)
- Model deployment orchestration with configurable parameters
- GPU memory reservation and validation before model loading
- OOM recovery logic with cache clearing and retry mechanisms
- Dynamic batch size adjustment based on available resources

3. Model Resource Calculator (dualgpuopt/optimizer.py)
- Context window size calculation based on model architecture
- KV cache estimation for attention mechanisms
- Memory requirement projections for different model sizes
- Load balancing logic for dual GPU configurations

Key Command Generation Templates:

llama.cpp:
```
./main -m {model} --gpu-split {split} --n-gpu-layers 999 --ctx-size {ctx}
```

vLLM:
```
--model {model} --dtype float16 --tensor-parallel-size {tp} --gpu-memory-utilization 0.9
```

Notable Domain-Specific Elements:
- Model-aware resource estimation
- Framework-specific parameter optimization
- Multi-GPU workload distribution
- Memory pressure monitoring and recovery

The implementation focuses on optimizing model execution across multiple GPUs while maintaining stability through intelligent resource management and error recovery.

$END$