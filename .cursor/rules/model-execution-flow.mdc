---
description: Defines execution flows and command generation for AI model deployment across multiple GPUs
globs: 
alwaysApply: false
---


# model-execution-flow

MODEL EXECUTION WORKFLOW SYSTEM
Importance Score: 95

Core Components:

1. Context Length Management
File: `dualgpuopt/ctx_size.py`

- Dynamic context window calculation based on model architecture:
  - Number of layers
  - KV heads configuration
  - Head dimensions 
  - MoE factor adjustments
- Model-specific parameter lookups for:
  - Mixtral, Llama-2, Mistral, Phi-2
  - Configuration presets with optimized defaults

2. Layer Distribution Engine
File: `dualgpuopt/layer_balance.py`

- Layer allocation across GPUs using weighted profiling:
  - Short sequence profiling (64 tokens, 20% weight)
  - Long sequence profiling (1024 tokens, 80% weight)
- Memory quota enforcement with reserve ratios
- Layer rebalancing based on runtime performance metrics

3. Command Generation System
File: `dualgpuopt/optimizer.py`

- Framework-specific command synthesis:
  - llama.cpp: GPU split parameters, context sizing
  - vLLM: Tensor parallel configuration
- Environment variable generation for GPU allocations
- Split ratio calculation based on available memory

4. Batch Processing Manager 
File: `dualgpuopt/batch/smart_batch.py`

- Length-aware sequence batching:
  - Groups similar length sequences
  - Manages memory overhead (20% reserve)
  - Dynamic batch size bounds (1-64 sequences)
  - Prevents mixing of short/long sequences beyond threshold

5. Memory Policy System
File: `dualgpuopt/mpolicy.py`

- Mixed precision management:
  - FP32 for critical ops (LayerNorm, softmax, residuals)
  - Lower precision for bulk compute
  - Dynamic dtype selection based on operation type

Key Workflows:

1. Model Launch Sequence
- Context size calculation
- Layer distribution optimization
- Command generation with framework detection
- Environment configuration

2. Runtime Optimization
- Batch size adjustment
- Memory policy enforcement
- Layer rebalancing based on metrics

3. Resource Management
- GPU memory splitting
- Tensor parallel sizing
- Overhead reservation

$END$