---
description: Documents the unique model execution flows, command generation, and GPU resource allocation in dual-GPU optimizer solutions.
globs: **/optimizer.py,**/launcher*.py,**/gpu_info.py,**/gpu/**,**/commands/**
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized GPU management and model deployment across multiple GPUs:

## Command Generation System (dualgpuopt/optimizer.py)
- Generates framework-specific commands for llama.cpp and vLLM execution
- Computes optimal tensor splits based on relative GPU memory capacities and model requirements
- Calculates appropriate memory fractions for balanced distribution between GPUs
- Configures framework-specific parameters like context length, batch size and tensor parallelism

## Launcher Orchestration (dualgpuopt/gui/launcher/launch_controller.py)
- Manages model deployment processes across GPUs
- Validates model compatibility with available GPU configurations
- Handles launch sequence orchestration with failure recovery
- Monitors execution metrics like GPU utilization and memory usage
- Implements graceful shutdown and error handling

## Resource Allocation Flow
- Analyzes GPU capabilities including VRAM size and compute capability
- Generates optimal layer distribution across GPUs based on profiling
- Calculates safe maximum context sizes per device
- Manages VRAM quotas and overflow handling
- Monitors memory pressure with warning thresholds

## Framework-Specific Optimizations
- llama.cpp:
  - GPU layer split ratios based on relative memory capacity
  - Layer offloading between devices
  - Tensor parallel size configuration
  
- vLLM: 
  - Memory utilization percentage allocation
  - Tensor parallel configuration
  - KV cache management across GPUs

Key file paths:
```
dualgpuopt/optimizer.py
dualgpuopt/gui/launcher/launch_controller.py
dualgpuopt/gui/launcher/parameter_resolver.py
dualgpuopt/gpu/monitor.py
```

$END$