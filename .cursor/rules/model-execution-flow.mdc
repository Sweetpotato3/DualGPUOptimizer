---
description: Defines LLM execution workflows, command generation, and GPU resource allocation patterns
globs: **/runner.py,**/optimizer.py,**/gpu_commands.py,**/layer_balance.py,**/launcher.py
alwaysApply: false
---


# model-execution-flow

## Core LLM Execution Components

### Model Launch Orchestration (Importance: 95)
- Framework-specific command generation for llama.cpp and vLLM
- Dynamic GPU split calculation and tensor parallelism sizing
- Process lifecycle management with configurable context windows (default 65536 tokens)
- Files: `dualgpuopt/gui/launcher.py`

### Layer Distribution System (Importance: 90)
- Latency-aware layer balancing across GPUs using dual sequence profiling (64/1024 tokens)
- Weighted profiling with 20% short sequences, 80% long sequences
- Quota-based load distribution respecting GPU memory constraints
- Files: `dualgpuopt/layer_balance.py`

### Memory Split Optimization (Importance: 85) 
- Calculates optimal memory distribution across GPUs for LLM workloads
- Generates tensor fractions based on relative GPU capacities
- Produces specialized configs for llama.cpp and vLLM frameworks
- Files: `dualgpuopt/optimizer.py`

### Smart Batching Engine (Importance: 80)
- Dynamic token-based batching with size limits
- Back-pressure mechanism for RAM overflow prevention
- OOM recovery with cache clearing and retry logic
- Files: `dualgpuopt/batch/smart_batch.py`

### Context Size Management (Importance: 75)
- Heuristic algorithm for safe context length calculation
- Considers layers, KV heads, dimensions, precision bits
- Dynamically adjusts for MoE models and memory buffers
- Files: `dualgpuopt/ctx_size.py`

## Execution Flow
1. GPU capability detection and validation
2. Memory split calculation based on GPU capabilities
3. Layer distribution profiling and optimization
4. Framework-specific command generation
5. Process launch with optimized parameters
6. Runtime batching and context management

$END$