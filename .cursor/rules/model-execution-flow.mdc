---
description: Defines LLM execution workflows, command generation, and GPU resource allocation patterns
globs: **/optimizer.py,**/launcher/**,**/executor/**,**/gpu_commands.py,**/gpu_info.py
alwaysApply: false
---


# model-execution-flow

Key execution workflow components:

1. LLM Launch Pipeline (dualgpuopt/gui/launcher/launch_controller.py):
- Model size detection based on name patterns (70B, 13B, 7B, Mixtral)
- Framework-specific command generation for llama.cpp and vLLM
- Dynamic GPU memory allocation and tensor parallelism configuration
- OOM recovery through CUDA cache clearing

2. Resource Management (dualgpuopt/memory/monitor.py):
- KV cache size calculation: context_size * 8 bytes * layers * kv_heads * hidden_size
- Model weight requirement estimates based on parameter count
- Memory split optimization across multiple GPUs
- Memory allocation safety buffer maintains 10% headroom

3. Batch Processing (dualgpuopt/batch/smart_batch.py):
- Length-aware inference scheduling with backpressure mechanism
- Dynamic batch size adjustments based on sequence length distribution
- OOM recovery reduces batch size by 25% after failures
- Gradual recovery increases batch size by 10% after 5 successful batches

4. Layer Distribution (dualgpuopt/layer_balance.py):
- Performance-based layer assignment across GPUs
- Block optimization creates contiguous layer groups
- Special handling for embedding and output layers
- Layer distribution validation ensures split ratios match assignments

5. Framework Integration:
- llama.cpp: GPU split ratios and layer distribution 
- vLLM: Tensor parallel size and GPU device mapping
- Mixed precision policies for LayerNorm and attention components
- Framework-specific parameter validation and constraints

The execution flow prioritizes efficient resource utilization while maintaining stable model inference across multiple GPUs. Core logic emphasizes memory management, layer distribution optimization, and framework-specific deployment patterns.

$END$