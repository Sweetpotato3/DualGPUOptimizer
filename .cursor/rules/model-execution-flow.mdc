---
description: Specification for managing LLM inference execution, GPU resource allocation, and command generation across multiple ML frameworks
globs: **/optimizer.py,**/commands/*,**/gpu/**,**/memory/**,**/batch/**
alwaysApply: false
---


# model-execution-flow

## Core Execution Components

1. GPU Memory Split Generation:
```python
def calculate_split(model_size, gpu_memory):
    split_ratio = min(0.85, gpu_memory[0] / sum(gpu_memory))
    return f"{split_ratio:.2f},{1-split_ratio:.2f}"
```

2. Command Generation Pipeline:
- Generates framework-specific commands for model execution:
  * llama.cpp: GPU layer distribution and memory splits 
  * vLLM: Tensor parallel configuration and memory thresholds

3. Smart Batch Management:
- Length-aware batch scheduling with throttling
- Memory pressure detection with dynamic batch size reduction
- Token budget enforcement (max 16384 tokens/batch)
- Backpressure mechanism for OOM prevention

4. Memory Recovery Protocol:
```python
RECOVERY_STEPS = [
  CLEAR_CACHE,    # Clear CUDA cache
  REDUCE_BATCH,   # Cut batch size by 50%
  OFFLOAD,        # Move data to CPU
  TERMINATE       # Kill low priority processes
]
```

## Resource Allocation

1. GPU Memory Budget:
- Dynamic VRAM allocation based on model architecture:
  * Base model weights
  * KV cache per token
  * Attention overhead
  * Safety margins

2. Layer Distribution:
- Model-specific layer placement across GPUs:
  * 70B models: 0-39 on GPU0, 40-79 on GPU1
  * Mixtral: 0-15 on GPU0, 16-31 on GPU1 
  * 13B models: Even distribution

3. Context Length Management:
```python
max_ctx = min(
    available_memory / bytes_per_token,
    model.max_sequence_length
)
```

## File Paths
- dualgpuopt/optimizer.py: Core optimization logic
- dualgpuopt/commands/gpu_commands.py: Command generation
- dualgpuopt/memory/recovery.py: OOM handling
- dualgpuopt/batch/smart_batch.py: Batch management

Importance Score: 95 (Core model execution flow and resource management)

$END$