---
description: Defines the execution flow and resource allocation for running LLM models across dual GPUs
globs: **/optimizer.py,**/runner.py,**/commands/**,**/batch/**
alwaysApply: false
---


# model-execution-flow

### Core Execution Components

1. **Command Generation and Optimization**
- Generates optimized command strings for llama.cpp and vLLM execution
- Calculates GPU memory splits and tensor fractions based on available GPU memory
- Creates environment files with CUDA device mappings and runtime parameters

2. **Asynchronous Batch Processing**
The SmartBatcher handles inference request queueing and execution:
- Groups requests into buckets based on input sequence length
- Implements back-pressure via queue depth limits
- Retries inference on OOM errors after cache clearing
- Uses pluggable heuristics for bucket sizing

3. **Layer Distribution System**
- Profiles layer execution times to identify fast/slow layers
- Redistributes layers across GPUs based on memory quotas and latency
- Saves device maps as JSON for model inference

4. **Context Size Management**
- Calculates safe maximum context length based on:
  - Available GPU memory
  - Model parameters (layers, heads, dimensions)
  - MOE factors
  - Precision requirements
  
5. **Command Pattern for GPU Operations**
- Encapsulates GPU operations in command objects
- Tracks command history for undo/redo
- Publishes execution events for monitoring
- Handles overclocking and mock GPU mode commands

### Key File Paths:
```
dual_gpu_optimizer/dualgpuopt/
├── optimizer.py         # Core optimization logic
├── runner.py           # Model execution
├── commands/
│   ├── gpu_commands.py # GPU operation commands
└── batch/
    ├── smart_batch.py  # Batching system
    └── heuristics.py   # Batching policies
```

$END$