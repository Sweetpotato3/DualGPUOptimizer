---
description: Provides guidance on AI model execution workflows, optimization strategies, and launch parameter configuration across multiple GPUs.
globs: **/launcher.py,**/gpu_commands.py,**/runner.py,**/model_profiles.py,**/mpolicy.py
alwaysApply: false
---


# model-execution-flow

The model execution flow encompasses several key subsystems:

## Launch Parameter Generation

The launcher system generates optimal launch parameters for both llama.cpp and vLLM frameworks:

```python
def llama_command(model_path: str, ctx: int, split: str):
    return f"./main -m {model_path} --gpu-split {split} --ctx-size {ctx}"

def vllm_command(model_path: str, tp: int):
    return f"--model {model_path} --tensor-parallel-size {tp}"
```

## GPU Resource Management

The system actively manages GPU resources during model execution:

1. Dynamic GPU Split Calculation
- Analyzes available GPU memory
- Determines optimal tensor split ratios
- Configures memory pressure thresholds
- Maintains reserves for system overhead

2. Layer Distribution
- Profiles layer performance on each GPU
- Creates optimized device maps for model distribution
- Handles load balancing across devices
- Maintains monitoring of layer execution stats

## Recovery System

1. OOM Prevention
- Implements tiered recovery approaches:
  * Cache clearing
  * Batch size reduction
  * Memory offloading
  * Process termination

2. Memory Pressure Monitoring
- Configurable alert levels (WARNING/CRITICAL/EMERGENCY)
- Memory usage projection system
- Rolling history analysis
- Growth rate modeling with safety factors

## Smart Batch Processing

The batch processing system optimizes workload distribution:

1. Length-aware batch scheduling
- Groups similar length sequences
- Adapts batch sizes based on GPU capacity
- Implements backpressure for OOM prevention
- Gradual batch size recovery after stability

2. Token Management
- Maximum token sum per batch (16384)
- Length threshold-based splitting
- Backpressure scaling (25-95%)
- 5-batch stability window

## Framework Integration

1. Mixed Precision Policies
- Specialized handling for LayerNorm operations
- Custom autocast context management
- Framework-specific dtype configurations

2. Model-Specific Optimizations
- Parameter detection from model names
- Architecture-based context size calculation
- Automatic batch size optimization
- Memory overhead calculation

$END$