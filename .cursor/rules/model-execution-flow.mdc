---
description: Defines comprehensive LLM execution workflows for GPU optimized inference including command generation and resource allocation.
globs: **/*.py,**/run*.py,**/launch*.py,**/model*.py,**/gpu/**
alwaysApply: false
---


# model-execution-flow

Core LLM Execution Workflows:

1. Framework-Specific Command Generation
- Implements specialized command builders for different LLM frameworks:
  * llama.cpp: GPU split configurations and layer distribution
  * vLLM: Tensor parallel sizing and memory utilization
- Handles dynamic parameter generation based on:
  * Available GPU memory 
  * Model architecture requirements
  * Layer balancing configurations

2. Model Launch Pipeline 
- Pre-launch validation and resource allocation
- Dynamic batch size configuration based on:
  * Token length distribution
  * Available GPU memory
  * Model architecture parameters
- Smart batching system with:
  * Length-aware grouping
  * Memory pressure monitoring
  * Automatic batch size adjustment

3. Memory Management 
- VRAM allocation strategy with dual GPU optimization:
  * Base model allocation
  * KV cache memory estimates
  * Per-token memory tracking
  * Dynamic memory reclamation
- Smart memory prediction system for:
  * MoE model scaling
  * Attention mechanism overhead
  * Model-specific heuristics

4. Resource Optimization
- Layer balancing across GPUs using:
  * Short sequence profiling (64 tokens)
  * Long sequence profiling (1024 tokens)
  * 20/80 weighted performance metrics
- Automatic tensor parallel configuration:
  * GPU memory ratio calculations
  * Layer distribution optimization
  * Block size constraints (min 3 layers)

5. Execution Monitoring
- Real-time metrics tracking:
  * Tokens per second 
  * Memory utilization
  * Temperature thresholds
  * Power consumption patterns
- Automatic recovery strategies:
  * Cache clearing
  * Batch size reduction
  * Process termination

Key File Paths:
- dualgpuopt/optimizer.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/layer_balance.py
- dualgpuopt/memory/predictor.py
- dualgpuopt/commands/gpu_commands.py

The model execution flow implements specialized handling of LLM deployment across dual GPU configurations with emphasis on memory efficiency, performance optimization and automatic resource management.

$END$