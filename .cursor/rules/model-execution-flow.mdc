---
description: Documents core logic for LLM model execution, GPU resource allocation, backend selection, and command generation across llama.cpp and vLLM.
globs: **/optimizer.py,**/engine/**,**/launcher/**,**/gpu/**
alwaysApply: false
---


# model-execution-flow

The system implements a dual-GPU optimization pipeline for LLM model execution with the following core components:

## Model Execution Flow

1. Backend Selection and Initialization:
- `engine/backend.py` determines optimal backend (llama.cpp, vLLM) based on:
  * File format (.gguf -> llama.cpp)  
  * Model quantization (AWQ -> vLLM)
  * Architecture requirements (MoE -> vLLM)

2. GPU Resource Allocation:
- `optimizer.py` calculates optimal GPU splits:
  * Memory ratios based on VRAM capacity
  * Layer distribution for tensor parallelism
  * Safety margins for context expansion
  * Adjustments for model architecture (MoE requires 5% extra)

3. Command Generation:
- Generates framework-specific launch commands:
```python
# llama.cpp format
--main-gpu <gpu0_memory> --gpu-split <split_ratio>
# vLLM format  
--tensor-parallel-size <num_gpus> --gpu-memory-utilization <ratios>
```

4. Context Size Optimization:
- Dynamic context length calculation based on:
  * Available GPU memory
  * Model architecture 
  * KV cache requirements
  * Tensor parallelism overhead

5. Execution Monitoring:
- Real-time metrics tracking:
  * Tokens/second throughput
  * Memory pressure per GPU
  * Temperature thresholds
  * Error states and recovery

File structure:
```
dualgpuopt/
  engine/
    backend.py    # Backend selection logic
    pool.py       # Model instance management 
  optimizer.py    # GPU split calculations
  gpu/
    monitor.py    # Resource monitoring
```

Key components integrate through an event bus system that coordinates model loading, execution state changes, and resource monitoring events.

$END$