---
description: Defines large language model execution flows including command generation, resource allocation, and GPU orchestration
globs: **/*optimizer.py,**/*runner.py,**/*launcher.py,**/gpu_info.py,**/telemetry.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized handling for running large language models across multiple GPUs. Key components include:

1. Command Generation (optimizer.py, importance: 95)
- Framework-specific command generation for llama.cpp and vLLM:
  - Calculates optimal tensor parallel splits based on GPU memory ratios 
  - Sets environment variables for CUDA memory configuration
  - Configures NCCL communication parameters

2. Resource Allocation (gpu_info.py, importance: 90)
- Dynamic GPU memory management:
  - Calculates maximum safe context lengths per model architecture
  - Reserves memory quotas for attention cache vs model weights
  - Implements 20% overhead buffer for tensor parallel operations
  - Enforces model-specific memory thresholds

3. Layer Distribution (layer_balance.py, importance: 85)
- Adaptive layer balancing across GPUs:
  - Profiles layer execution times using 64/1024 token sequences
  - Distributes layers based on relative GPU performance
  - Maintains memory quotas during layer assignment
  - Handles specialized MoE model architectures

4. Execution Pipeline (runner.py, importance: 80)
- Model inference orchestration:
  - Initializes model shards across designated GPUs
  - Manages tensor parallel communication
  - Handles KV cache allocation and clearing
  - Provides fallback paths for OOM conditions

5. Telemetry Integration (telemetry.py, importance: 75)
- Runtime performance monitoring:
  - Tracks per-GPU memory utilization
  - Monitors PCIe bandwidth consumption
  - Detects execution bottlenecks
  - Provides metrics for dynamic rebalancing

The system focuses on optimizing execution of large language models by intelligently managing GPU resources and model distribution. Core emphasis is on safe memory management, balanced workload distribution, and framework-specific optimizations.

$END$