---
description: Defines LLM execution workflows, command generation, configuration and GPU allocation for dual-GPU optimization
globs: **/telemetry.py,**/memory/profiler.py,**/optimizer.py,**/gpu_info.py,**/batch/**,**/commands/**
alwaysApply: false
---


# model-execution-flow

Core model execution flow logic centers around orchestrating LLM inference across dual GPUs through several key components:

1. Smart Batching System (dualgpuopt/batch/smart_batch.py):
- Specialized length-aware inference scheduler optimized for dual GPU setups
- Implements dynamic batch size calculation based on sequence lengths and available memory
- Employs backpressure mechanism that auto-adjusts batch sizes based on OOM events
- Uses bucket policy to group similar-length sequences for optimal throughput
- Progressive recovery with 25% batch size reduction after OOM

2. Memory Profiling System (dualgpuopt/memory/profiler.py):
- Tracks memory patterns during inference sessions with configurable sampling
- Leak detection using linear regression analysis with 0.05 growth threshold
- Validates against minimum 5MB change to filter noise
- Memory spike detection using 20-sample sliding window
- Correlates spikes with inference activities and token processing

3. Layer Distribution Logic (dualgpuopt/optimizer.py):  
- Context size calculation considering model architecture and GPU memory 
- Dynamic split ratio determination based on available GPU memory
- Framework-specific command generation for llama.cpp and vLLM
- Memory allocation strategies with configurable safety margins
- Tensor parallelism calculation with overhead factor compensation

4. Execution Flow Manager (dualgpuopt/telemetry.py):
- Real-time GPU metrics collection during model execution 
- Multi-tier alerting based on memory/temperature/power thresholds
- Rolling 60-second metrics history for performance analysis
- Smart error recovery with exponential backoff
- Automatic failover to mock data after consecutive failures

Key execution workflows:
1. Pre-execution:
- Model memory requirement estimation
- GPU split ratio calculation 
- Command generation with optimized parameters

2. During execution:
- Real-time memory/performance monitoring
- Dynamic batch size adjustment
- Leak/spike detection
- Automatic recovery from OOM events

3. Post-execution:
- Session metric analysis
- Memory pattern evaluation
- Inference efficiency calculation
- Pattern-based recommendations

The system manages end-to-end model execution with focus on optimal resource utilization across dual GPUs while maintaining stability through comprehensive monitoring and recovery mechanisms.

$END$