---
description: Specifies how large language models are executed across multiple GPUs, including memory allocation, batch processing, and resource optimization.
globs: **/optimizer.py,**/launcher.py,**/memory_monitor.py,**/layer_balance.py,**/batch/*.py,**/commands/*.py
alwaysApply: false
---


# model-execution-flow

## Core Execution Components

1. Model Runtime Optimization System (launcher.py)
- Dynamic GPU resource allocation based on available VRAM/model size
- Intelligent layer distribution across GPUs using performance profiling
- Framework-specific command generation for llama.cpp and vLLM
- Implements automated parameter tuning based on model architecture

2. Memory Management System (memory_monitor.py)
- Model-specific memory profiles with pre-configured settings
- Predictive OOM prevention using historical usage patterns
- Multi-level memory alert system with progressive thresholds
- Automatic recovery strategies: cache clearing, batch reduction, offloading

3. Layer Distribution (layer_balance.py)
- Performance-based layer distribution across GPUs
- Balancing algorithm considers sequence length profiles
- Creates contiguous layer blocks to minimize inter-GPU communication
- Handles MoE architecture-specific layer allocation

4. Smart Batching System (batch/smart_batch.py)
- Length-aware batching with dynamic resizing
- Back-pressure mechanism prevents GPU memory overflow 
- Token ratio-based bucketing for workload distribution
- Handles asynchronous batch processing with future-based results

5. Command Generation (commands/gpu_commands.py)
- Framework-specific optimization parameters
- GPU split configuration for tensor parallelism
- Memory-aware command generation
- Environment variable optimization for multi-GPU setups

## Execution Flow

1. Model Startup
- Extract model parameters from filename
- Calculate optimal context size and batch parameters
- Distribute model layers across GPUs
- Configure framework-specific launch parameters

2. Runtime Optimization
- Monitor GPU memory usage and performance metrics
- Adjust batch sizes dynamically based on memory pressure
- Balance layer distribution based on runtime profiling
- Implement preventive OOM handling

3. Recovery Handling
- Progressive memory recovery strategies
- Dynamic batch size adjustment
- Automated cache management
- Process restart handling with state preservation

## Key Integration Points

- Memory Monitor ↔ Layer Balancer
- Smart Batcher ↔ Memory Monitor
- Command Generator ↔ Runtime Optimizer
- Framework Interface ↔ Memory Management

Importance Scores:
- Model Runtime Optimization: 95
- Memory Management: 90
- Layer Distribution: 85
- Smart Batching: 80
- Command Generation: 75

$END$