---
description: Handles model execution workflow orchestration for dual GPU systems, including command generation, GPU resource allocation, and monitoring.
globs: **/run_*.py,**/launcher/**,**/optimizer/**,**/gpu/**,**/telemetry.py
alwaysApply: false
---


# model-execution-flow

Model Execution Workflow Engine core components:

1. Launch Command Generation
- Located in: dualgpuopt/gui/launcher/launch_controller.py
- Generates optimized launch configurations for:
  * llama.cpp: GPU split ratios, tensor parallelism settings
  * vLLM: Device mapping, tensor parallel size configuration
- Handles model-specific parameter mapping:
  * Context length validation
  * Layer distribution across GPUs
  * Memory allocation strategy
  * KV cache configuration

2. GPU Resource Allocation Flow
- Located in: dualgpuopt/optimizer.py
- Memory distribution algorithm:
  * Calculates per-GPU VRAM quotas 
  * Reserves system overhead (2GB default)
  * Adjusts for model architecture requirements
  * Handles MoE models with expert count scaling
- Layer distribution logic:
  * Balances transformer layers across GPUs
  * Optimizes for latency using profiling data
  * Maintains architecture coherency requirements
  * Groups layers into contiguous blocks

3. Model Execution Monitoring
- Located in: dualgpuopt/telemetry.py
- Real-time metric collection:
  * GPU utilization tracking
  * Memory allocation monitoring
  * Temperature/power monitoring
  * Token throughput measurement
- Implements backpressure detection:
  * Memory pressure monitoring
  * OOM prevention system
  * Cache clearing triggers
  * Workload rebalancing

4. Execution Recovery System
- Located in: dualgpuopt/memory/recovery.py
- Progressive intervention strategy:
  * Cache clearing (Level 1)
  * Batch size reduction (Level 2)
  * Layer redistribution (Level 3)
  * Full model reload (Level 4)
- Memory optimization rules:
  * 85% memory utilization target
  * 20% tensor parallel overhead allowance
  * 5% minimum free memory requirement
  * Progressive batch size reduction (25% steps)

The system orchestrates model execution across dual GPU configurations with emphasis on optimal resource utilization and stability through continuous monitoring and adaptive intervention strategies.

$END$