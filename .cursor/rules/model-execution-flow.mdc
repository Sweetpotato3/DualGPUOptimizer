---
description: Documentation for managing LLM execution workflows, command generation, and GPU resource allocation patterns
globs: **/optimizer.py,**/runner.py,**/gui/launcher.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized handling of LLM deployments across multiple GPUs:

### Core Execution Components

1. Command Generation System
- Framework-specific command builders:
  ```
  llama.cpp: ./main -m {model} --gpu-split {ratio} --n-gpu-layers {count}
  vLLM: python -m vllm.entrypoints.api_server --model {path} --tensor-parallel-size {tp}
  ```
- Dynamic parameter calculation based on:
  - Available GPU memory
  - Model architecture (layers, heads)
  - KV cache requirements
  - MoE factors for expert models

2. Resource Allocation Flow
- Pre-execution GPU memory analysis:
  - Base memory footprint calculation
  - Dynamic KV cache sizing
  - Safety buffer allocation (10-20%)
- Layer distribution optimization:
  - Performance profiling with dual sequence lengths:
    - Short sequences (64 tokens) - 20% weight
    - Long sequences (1024 tokens) - 80% weight
  - Layer rebalancing based on empirical performance

3. Execution Pipeline
- Model loading phase:
  - Memory pressure monitoring
  - CUDA cache management
  - Layer distribution enforcement
- Inference optimization:
  - Batch size determination
  - Context length validation
  - Memory fragmention prevention

Relevant Files:
- dualgpuopt/optimizer.py - Core optimization logic
- dualgpuopt/gui/launcher.py - Launch orchestration
- dualgpuopt/commands/gpu_commands.py - Command generation
- dualgpuopt/batch/smart_batch.py - Batch optimization

$END$