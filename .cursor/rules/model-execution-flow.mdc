---
description: Specification for implementing and documenting LLM model execution workflows across multiple GPUs
globs: **/runner.py,**/optimizer.py,**/batch/*.py,**/layer_balance.py,**/mpolicy.py
alwaysApply: false
---


# model-execution-flow

The model execution flow consists of three main components:

1. Model Deployment Orchestration (Importance: 95)
- Generates framework-specific commands for llama.cpp and vLLM
- Configures GPU split ratios based on available VRAM
- Manages tensor parallelism settings for multi-GPU deployments 
- Handles environment variable setup for memory allocation

2. Layer Distribution System (Importance: 90)
- Implements adaptive layer balancing across GPUs:
  - Profiles layer performance with dual sequence testing (64/1024 tokens)
  - Uses weighted scoring (0.2 * short_seq + 0.8 * long_seq)
  - Distributes layers while respecting VRAM constraints
  - Places highest-latency layers optimally

3. Smart Batching Pipeline (Importance: 85)
- Length-aware inference scheduling with queue depth control
- Token-based batch size optimization 
- Automatic OOM recovery with cache clearing
- Power-of-two bucket grouping for sequences
- Token ratio-based bucketing for balanced workloads

Key File Paths:
- dualgpuopt/optimizer.py: GPU split calculations and command generation
- dualgpuopt/layer_balance.py: Layer distribution algorithms
- dualgpuopt/batch/smart_batch.py: Batching system implementation
- dualgpuopt/batch/heuristics.py: Batching strategies
- dualgpuopt/mpolicy.py: Mixed precision policies

The execution flow emphasizes optimal resource utilization across dual GPU setups while maintaining safe memory constraints and efficient workload distribution.

$END$