---
description: Documents the execution flow and orchestration of LLM models across multiple GPUs, including memory management and resource allocation.
globs: **/runner.py,**/optimizer.py,**/gpu_commands.py,**/smart_batch.py,**/layer_balance.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized orchestration for running large language models across multiple GPUs:

Core Execution Pipeline:

1. Model Resource Calculation
- Layer-aware distribution based on transformer architecture
- Memory requirement estimation using:
  * Base model weights
  * KV cache scaling factor (2.0x)
  * Tensor parallelism overhead (20%)
  * System overhead buffer (2GB)
- GPU split ratio optimization considering:
  * Available VRAM per device
  * Model architecture requirements
  * Minimum 20% allocation per GPU rule

2. Smart Batching System
- Length-aware inference scheduling using:
  * Token count bucketing (threshold: 256 tokens)
  * Dynamic batch sizing based on sequence lengths
  * Back-pressure system for OOM prevention
  * Automatic retry mechanism with configurable scale factor
- Two primary bucket strategies:
  * Power-of-two scaling for llama.cpp
  * Token ratio optimization for vLLM

3. Layer Distribution Algorithm
- Performance-weighted profiling:
  * Short sequences (64 tokens): 20% weight
  * Long sequences (1024 tokens): 80% weight
- Layer assignment rules:
  * Input embeddings to first GPU
  * Output components to second GPU
  * Contiguous block optimization
  * Cross-device transfer minimization

4. Framework-Specific Command Generation
- llama.cpp:
  * GPU split string configuration 
  * Context size parameters
  * Layer distribution mapping
- vLLM:
  * Tensor parallel size settings
  * Memory allocation ratios
  * Quantization configuration

File Paths:
- dualgpuopt/optimizer.py: Core optimization logic
- dualgpuopt/batch/smart_batch.py: Batching system
- dualgpuopt/layer_balance.py: Layer distribution
- dualgpuopt/commands/gpu_commands.py: Command generation

Importance Score: 95
Rationale: Core business logic for ML model execution orchestration across GPUs

$END$