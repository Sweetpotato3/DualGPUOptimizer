---
description: Handles LLM execution workflows including command generation, configuration, and GPU resource allocation for llama.cpp and vLLM frameworks.
globs: **/optimizer.py,**/gpu_info.py,**/runner.py,**/layer_balance.py,**/commands/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized command generation and resource allocation for running large language models across dual GPU configurations.

## Core Command Generation
[File: dualgpuopt/optimizer.py]

The command generation system outputs framework-specific launch commands based on model requirements and GPU configuration:

1. llama.cpp Command Flow:
- Calculates ngl (number of layers per GPU) based on model architecture
- Generates split string (e.g. "50,50") for balanced GPU allocation 
- Includes tensor split configuration for parallel inference
- Handles context window sizing based on available VRAM

2. vLLM Command Flow:
- Determines tensor parallel size from GPU count
- Configures GPU device mapping 
- Sets model-specific parameter values for:
  - Context length
  - Memory efficient attention 
  - Quantization mode
  - Seed value

## Layer Distribution System 
[File: dualgpuopt/layer_balance.py]

The layer balancer distributes transformer layers across available GPUs:

1. Layer Assignment Strategy:
- Weights short sequences (64 tokens) at 20% 
- Weights long sequences (1024 tokens) at 80%
- Assigns layers based on profiled execution times
- Forms blocks of at least 3 contiguous layers
- Places input embedding on first GPU
- Places norm/lm_head on last GPU

2. Memory Management:
- Maintains reserve ratio for each GPU
- Respects memory quotas during assignment
- Rebalances on OOM conditions
- Adjusts distribution based on model family (Llama, Mistral, etc.)

## Model Execution Flow
[File: dualgpuopt/runner.py]

The execution orchestrator manages the model launch process:

1. Initialization:
- Validates model path exists
- Checks GPU memory availability
- Sets framework-specific env vars
- Initializes logging streams

2. Launch Control:
- Non-blocking process execution
- Output streaming to queue
- Graceful termination support
- Error state monitoring

3. Failure Recovery:
- Clears GPU caches on OOM
- Retries with reduced batch size
- Falls back to CPU if needed
- Logs error conditions

The system focuses on optimizing dual GPU model execution through intelligent resource allocation and framework-specific launch configurations.

$END$