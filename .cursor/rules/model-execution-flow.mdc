---
description: Defines execution workflows for LLM models including command generation, GPU allocation and runtime optimization
globs: **/optimizer.py,**/runner.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

## Command Generation
The optimizer generates framework-specific commands for model execution:

- llama.cpp commands include GPU memory split configuration and context size parameters
- vLLM commands specify tensor parallel size and GPU allocation settings
- Environment configuration files are generated with CUDA and NCCL settings

## Layer Distribution
Implements latency-aware layer redistribution across GPUs:
- Profiles layer execution times
- Balances GPU memory usage while minimizing latency impact 
- Generates device mapping JSON for layer assignments
- Considers factors like number of layers, KV heads, head dimensions

## Batch Processing
SmartBatcher provides asynchronous request batching:
- Groups inference requests by input length using bucketing policies
- Implements back-pressure to prevent memory overflow
- Handles GPU OOM errors with automatic retry after cache clearing

## Command Pattern Implementation
Commands handle GPU operations with undo support:
- ApplyOverclockCommand manages GPU overclocking settings
- EnableMockGpuCommand toggles mock GPU mode
- Command results publish to event bus for monitoring

## Key File Paths
- `dualgpuopt/optimizer.py`: Command generation and optimization
- `dualgpuopt/batch/smart_batch.py`: Batching implementation
- `dualgpuopt/commands/gpu_commands.py`: GPU operation commands
- `dualgpuopt/layer_balance.py`: Layer distribution logic

Importance Scores:
- Command Generation: 95 - Core execution workflow
- Layer Distribution: 90 - Critical for multi-GPU performance
- Batch Processing: 85 - Key inference optimization
- Command Pattern: 75 - Important operational control

$END$