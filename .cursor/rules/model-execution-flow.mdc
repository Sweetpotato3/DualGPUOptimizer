---
description: Documents LLM execution workflows, command generation, configuration parameters, and GPU resource allocation strategies for machine learning model deployment.
globs: **/optimizer.py,**/commands/*.py,**/launcher/*.py,**/direct_launcher.py,**/services/*.py
alwaysApply: false
---


# model-execution-flow

## Core Model Execution Control 
Orchestrates model execution across dual GPUs, providing commands for both llama.cpp and vLLM frameworks. Key capabilities:
- **Model Loading:** Loads model weights with optimized memory splits across GPUs. Calculates optimal split ratios based on available VRAM and device capabilities.
- **Tensor Parallelization:** Auto-detects optimal tensor parallel configurations for both frameworks. For llama.cpp, generates layer distribution commands based on memory ratios. For vLLM, sets tensor parallel size and device mappings.
- **Command Generation:** Creates framework-specific commands with optimized parameters:
  - llama.cpp: `model_path`, `context_size`, `gpu_split`, and layer distribution options
  - vLLM: `model_path`, `tensor_parallel_size`, memory configurations

## Memory Management
- Real-time monitoring of GPU memory utilization
- Dynamic memory reclamation using CUDA cache clearing
- OOM prevention through batch size adjustment

## Configuration Parameters
- Model-specific configurations (context length, hidden size, layers, heads)
- GPU split recommendations based on device capabilities
- Environment variable generation for optimized execution

## Resource Allocation Flow
1. Model Parameter Detection
2. GPU Memory Analysis 
3. Split Ratio Calculation
4. Command Generation
5. Process Launch and Monitoring

Relevant File Paths:
```
dualgpuopt/optimizer.py - Core optimization logic
dualgpuopt/commands/command_base.py - Base command generation
dualgpuopt/commands/gpu_commands.py - GPU-specific commands
dualgpuopt/launcher/launch_controller.py - Process control
dualgpuopt/services/event_bus.py - Execution event coordination
```

$END$