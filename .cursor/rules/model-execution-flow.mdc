---
description: Defines model execution workflows and command generation for large language models across multiple GPUs.
globs: **/engine/**,**/launcher/**,**/executor/**,**/model_flow/**,**/execution/**
alwaysApply: false
---


# model-execution-flow

Core execution flow components:

1. Model Launch Orchestration
- Implements specialized launch configuration for multi-GPU deployments with customizable parameters
- Generates framework-specific commands for llama.cpp and vLLM backends
- Handles tensor parallel configurations based on available GPU memory
- Auto-detects model type (GGUF, AWQ, Hugging Face) to select appropriate launch parameters

2. Engine Pool Management
- Maintains LRU cache of loaded model engines with health monitoring 
- Implements automatic recovery system for failed engines with configurable retry attempts
- Tracks metrics for cache hits/misses and model load times
- Graceful degradation to mock engines when hardware access fails

3. Memory Split Calculator
- Dynamically computes GPU memory splits based on:
  - Available VRAM per GPU 
  - Model architecture (layers, KV heads, hidden size)
  - Context length requirements
  - Framework-specific overheads
- Generates split configurations for both llama.cpp and vLLM

4. Process Monitoring
- Implements OOM detection and recovery through CUDA cache clearing
- Provides graceful termination with fallback force kill after timeout
- Tracks process health metrics and alerts on failures
- Handles framework-specific exit codes and error states

5. Smart Batch Processing 
- Length-aware inference scheduling with back-pressure mechanisms
- Bucket policies for efficient request batching
- Auto-retry on OOM with exponential backoff
- Token stream aggregation and result ordering

Key File Paths:
- `dualgpuopt/engine/pool/core.py`: Engine caching and health monitoring
- `dualgpuopt/gui/launcher/process_monitor.py`: Process lifecycle management
- `dualgpuopt/batch/smart_batch.py`: Length-aware batch processing
- `dualgpuopt/optimizer.py`: Memory split optimization
- `dualgpuopt/commands/gpu_commands.py`: Command generation

The system focuses on optimizing model execution across dual GPU setups while providing robust monitoring, recovery, and resource management capabilities.

$END$