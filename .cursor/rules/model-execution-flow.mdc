---
description: Handles model execution workflows, command generation, GPU resource allocation and batch management for running LLM inference
globs: **/*llm*.py,**/*model*.py,**/*inference*.py,**/*exec*.py,**/runner.py,**/launcher*.py
alwaysApply: false
---


# model-execution-flow

## Launch Command Generation Flow
The command generation system handles customized launch commands for LLM inference across different model frameworks:

### Framework-Specific Command Building
- `llama.cpp` commands include GPU split parameters, context sizes, layer allocations
- `vLLM` commands configure tensor parallelism, GPU visibility, CUDA_VISIBLE_DEVICES
- Environment variable generation for NCCL configurations and memory splits

### Resource Configuration 
- Dynamically calculates maximum context lengths based on GPU memory availability
- Determines optimal layer splits and tensor parallel configurations
- Generates framework-specific model deployment parameters like batch size

File: `dualgpuopt/optimizer.py`

## Model Process Control Flow
The process control system manages LLM model execution:

### Process Lifecycle
- Maintains separate process tracking for llama.cpp and vLLM model execution
- Implements graceful shutdown with configurable timeouts (3s default)
- Handles process output streaming and error capture

### GPU Resource Management
- Allocates GPU memory based on framework requirements
- Configures tensor parallel deployments for vLLM
- Manages GPU split configurations for llama.cpp

File: `dualgpuopt/qt/launcher_tab.py`

## Batch Processing Flow
Smart batching system for optimized inference:

### Length-Aware Batching
- Groups similar-length sequences to optimize throughput
- Dynamic batch size adjustment based on GPU memory
- Automatic OOM recovery by reducing batch sizes

### Memory Management
- Memory usage tracking per batch
- Proactive cache clearing on OOM events
- Memory growth monitoring with alerts

File: `dualgpuopt/batch/smart_batch.py`

## Model Memory Optimization
Memory optimization system for dual GPU deployments:

### Memory Calculation
- Per-token memory estimation based on model architecture
- KV cache overhead calculation
- Layer distribution optimization across GPUs

### Split Configuration
- Optimal memory split ratios for dual GPUs
- Tensor parallelism overhead adjustments
- Memory split caching with timeout invalidation

File: `dualgpuopt/optimizer.py`

$END$