---
description: Defines how machine learning models are executed across dual GPUs, including layer distribution, memory management, and command generation.
globs: 
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized workflows for running large language models across dual GPU configurations:

1. Core Execution Pipeline

- Tensor Split Optimization:
  ```python
  # Calculates optimal memory ratios between GPUs
  bytes_per_token = model.kv_hidden_size * model.num_layers * 2
  mb_per_token = (bytes_per_token / (1024 * 1024)) * kv_cache_factor
  ```

- Layer Distribution Logic:
  - Places input embeddings on first GPU by default
  - Assigns output components (norm layer, language model head) to last GPU
  - Implements dynamic programming for optimal layer block allocation
  - Uses weighted profiling (20% short sequences, 80% long sequences)

2. GPU Memory Management

- Memory Prediction System:
  - Estimates usage based on model architecture and batch size
  - Projects memory growth over execution timeline
  - Triggers alerts at 75% (WARNING), 90% (CRITICAL), 95% (EMERGENCY)

- Recovery Mechanisms:
  - Cache clearing with PyTorch CUDA cache reset
  - Progressive batch size reduction (25% steps)
  - Automatic model offloading when thresholds exceeded

3. Command Generation

- Framework-Specific Parameters:
  - llama.cpp: GPU split ratios, layer distribution, thread count
  - vLLM: Tensor parallel size, memory utilization limits
  - Generates environment variables for each framework

4. Health Monitoring

- Engine Pool Management:
  - Tracks engine health with periodic checks
  - Implements auto-restart for failing engines
  - Maintains cache of loaded models with LRU eviction
  - Records performance metrics per model

Key file paths:
- dualgpuopt/engine/pool.py: Engine lifecycle and health monitoring
- dualgpuopt/memory/predictor.py: Memory usage prediction and alerts
- dualgpuopt/model/vram_fit.py: VRAM allocation optimization
- dualgpuopt/commands/gpu_commands.py: Framework-specific command generation

The system prioritizes stable model execution across dual GPUs while preventing out-of-memory conditions through predictive monitoring and automated recovery mechanisms.

$END$