---
description: Technical specification for GPU-accelerated model execution flows, optimization strategies, and resource management in multi-GPU systems.
globs: dualgpuopt/**/*.py,hooks/**/*.py,examples/**/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow centers around dual GPU optimization and workload distribution through several core components:

## Command Generation System
File: dualgpuopt/commands/gpu_commands.py

- Model Launch Generation:
  - llama.cpp: Generates GPU split strings and layer allocations based on calculated ratios
  - vLLM: Produces tensor parallelism configurations for multi-GPU setups
  - Context lengths dynamically calculated from GPU memory availability
  - Automatic thread allocation based on GPU core counts

## Model Runner
File: dualgpuopt/runner.py

- Launch Protocol:
  1. Pre-launch GPU memory reset and validation 
  2. Framework-specific environment setup (CUDA_VISIBLE_DEVICES, etc)
  3. Command execution with supervised process management
  4. Real-time output monitoring and alert system

## Resource Manager 
File: dualgpuopt/gpu_info.py

- VRAM Distribution:
  - Calculates memory splits based on total/free ratios
  - Reserves system memory overhead (2GB default)
  - Implements safety margins for OOM prevention
  - Manages tensor allocation across GPUs

## Model Profiles
File: dualgpuopt/model_profiles.py

Core execution profiles for different model architectures:
- Llama-2: Default 5-65% split across GPUs
- Mistral: 40-60% balanced configuration  
- Mixtral: MoE-aware splits with 8-expert distribution

## Layer Balancing
File: dualgpuopt/layer_balance.py

- Progressive Layer Distribution:
  1. Profile GPU performance characteristics
  2. Calculate optimal splits using 20/80 weighted ratios
  3. Apply minimum block size (3 layers)
  4. Generate layer maps for framework consumption

## Execution Monitoring
File: dualgpuopt/memory/monitor.py

- Real-time telemetry:
  - Memory pressure tracking
  - Temperature monitoring 
  - Utilization metrics
  - Auto-recovery on OOM conditions

The execution flow orchestrates these components to achieve optimal model deployment across dual GPU configurations while maintaining stability and performance monitoring.

$END$