---
description: Guidelines for implementing model execution workflows, command generation, and GPU resource management
globs: **/optimizer.py,**/runner.py,**/layer_balance.py,**/ctx_size.py,**/gpu_info.py
alwaysApply: false
---


# model-execution-flow

Model Execution Workflows:

1. Framework-Specific Command Generation (Importance: 95)
- Two primary execution paths:
  * llama.cpp: GPU layer distribution and split configuration
  * vLLM: Tensor parallel GPU allocation and batch optimization
- Dynamic environment configuration based on GPU configuration
- Adaptive command generation based on model architecture and GPU memory

2. Layer Distribution System (Importance: 90)
- Intelligent layer placement across GPUs based on:
  * Per-layer latency profiling (64 and 1024 token sequences)
  * Memory quota allocation per GPU
  * Layer performance characteristics
- Weighted scoring (20% short sequences, 80% long sequences)
- Priority assignment for performance-critical layers

3. Context Size Management (Importance: 85)
- Model-specific token window calculations considering:
  * Transformer architecture parameters
  * Available GPU memory distribution
  * MoE factor requirements
  * KV cache overhead
- Built-in parameters for common architectures (LLaMA-2, Mistral, Mixtral)

4. Memory Optimization (Importance: 85)
- Token-based memory allocation with:
  * System overhead (2GB base)
  * KV cache multiplier (2.0x)
  * Tensor parallelism overhead (20%)
  * Safety margin buffer (10%)
- Dynamic split ratio calculation for available GPU memory

Key Implementation Components:
- dualgpuopt/optimizer.py: Command generation and resource allocation
- dualgpuopt/layer_balance.py: Layer distribution algorithms
- dualgpuopt/ctx_size.py: Context window optimization
- dualgpuopt/gpu_info.py: GPU capability detection and monitoring

$END$