---
description: Specification for managing execution flows of LLM models across dual GPU configurations, including command generation and resource allocation
globs: **/memory/**,**/gpu/**,**/commands/**,**/launcher/**,**/optimizer/**,**/runner.py
alwaysApply: false
---


# model-execution-flow

Core components for managing LLM model execution across dual GPU configurations:

1. Execution Pipeline Controller (dualgpuopt/memory/profiler.py):
```python
Memory Management Flow:
- Pre-inference memory baseline capture
- Token-based memory tracking during inference
- Post-inference memory cleanup
- Leak detection using dual thresholds:
  - Spike threshold for rapid growth
  - Leak threshold for sustained growth
```

2. Command Generation System (dualgpuopt/commands/gpu_commands.py):
```python
Framework-specific command generation:
- llama.cpp: GPU layer distribution and split ratios
- vLLM: Tensor parallel size and memory settings
- Environment variables for CUDA optimizations
```

3. Smart Batch Processing (dualgpuopt/batch/smart_batch.py):
- Length-aware sequence grouping 
- Dynamic batch size optimization based on:
  - Available GPU memory
  - Model memory requirements
  - Token sequence lengths
- Backpressure system for OOM prevention

4. Memory Recovery (dualgpuopt/memory/recovery.py):
- Tiered recovery strategies:
  1. CACHE_ONLY: PyTorch cache clearing
  2. CLOCK_RESET: GPU clock management
  3. FULL_RESET: Complete memory reclamation
  4. SYSTEM_CMD: OS-level GPU reset

5. Layer Balance Optimization (dualgpuopt/layer_balance.py):
- Two-phase profiling approach:
  - Short sequences (64 tokens): 20% weight
  - Long sequences (1024 tokens): 80% weight
- Dynamic quota system for VRAM allocation
- Layer assignment prioritization by computation intensity

6. Process Management (dualgpuopt/runner.py):
- Non-blocking subprocess execution for model inference
- Real-time output streaming through queue system
- Graceful termination handling
- Resource cleanup on process completion

Flow Control Rules:
1. Memory thresholds trigger recovery actions
2. Batch sizes adjust dynamically based on OOM events
3. Layer distribution optimizes for longest sequences
4. Process termination follows staged approach:
   - Soft termination attempt (2.5s timeout)
   - Force kill if soft termination fails

File Paths:
- dualgpuopt/memory/profiler.py
- dualgpuopt/commands/gpu_commands.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/memory/recovery.py
- dualgpuopt/layer_balance.py
- dualgpuopt/runner.py

$END$