---
description: Defines business logic for executing AI models across multiple GPUs, including memory management, batching, and command generation.
globs: **/model_execution.py,**/launcher/**,**/commands/**,**/memory/**,**/batch/**
alwaysApply: false
---


# model-execution-flow

## Command Generation
The system implements specialized command generation for different ML frameworks:

1. llama.cpp Commands:
- Generates commands with GPU layer distribution parameters
- Handles tensor parallel split ratios across GPUs
- Sets context length and batch size based on available memory
- Configures thread count and model quantization

2. vLLM Commands: 
- Manages tensor parallel size configuration
- Sets maximum model lengths and batch sizes
- Configures swap space and tensor parallel degree
- Handles model loading parameters with quantization

## Memory Management
Memory optimization implements:

1. Profile Management:
- Default profiles for common models (Llama-2, Mistral, Mixtral)
- Per-model metrics for base usage, per-batch usage, growth rate
- Token-based memory estimation with caching
- Maximum batch/sequence length calculations

2. Recovery Strategies:
- Tiered recovery for OOM conditions:
  1. Cache clearing
  2. Batch size reduction
  3. Memory reset
  4. Process termination
- CUDA cache management with cleaning intervals
- Memory retention monitoring between inference runs

## Batch Processing
Smart batching system provides:

1. Length-aware Scheduling:
- Groups sequences by length thresholds
- Dynamic batch size optimization
- Memory quota management (80% max utilization)
- Sequence padding minimization

2. Backpressure Control:
- Queue depth monitoring
- Automatic batch size reduction on OOM
- Recovery cooldown periods
- Gradual batch size restoration

File Paths:
```
dualgpuopt/commands/gpu_commands.py - Command generation
dualgpuopt/memory/predictor.py - Memory profiling
dualgpuopt/batch/smart_batch.py - Batch scheduling
dualgpuopt/memory/recovery.py - OOM recovery
```

The system optimizes model execution across multiple GPUs through coordinated command generation, memory management, and batch processing optimizations.

$END$