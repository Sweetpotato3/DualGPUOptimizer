---
description: Specification for LLM model execution workflows, command generation, and GPU resource management
globs: **/commands/*.py,**/model/*.py,**/execution/*.py,**/launcher/*.py
alwaysApply: false
---


# model-execution-flow

Model execution and resource management system implementing specialized GPU allocation strategies for LLM inferencing.

Core Command Generation (dualgpuopt/commands/gpu_commands.py):
- Dynamic command-line parameter generation for llama.cpp and vLLM frameworks
- Context length optimization with model-specific calculation engines
- GPU split ratio formulas based on model architecture and memory constraints 
- Layer distribution algorithms for tensor parallelism

Framework-Specific Launch Logic:
1. llama.cpp:
- GPU layer splitting config generator
- Memory-per-layer calculations
- Context window size optimization
- Split ratio enforcement (must sum to ~1.0)

2. vLLM:
- Tensor parallel size configuration 
- Memory allocation ratios between GPUs
- Peer-to-peer communication setup
- Device visibility management

Memory Management (dualgpuopt/memory/predictor.py):
- Model-specific base memory requirements:
  - Llama2-7B: 7GB base, 50MB/batch, 3KB/token
  - Llama2-13B: 13GB base, 100MB/batch, 5KB/token
  - Llama2-70B: 35GB base, 350MB/batch, 18KB/token
  - Mistral-7B: 8GB base, 55MB/batch, 3KB/token
  - Mixtral-8x7B: 25GB base, 200MB/batch, 12KB/token

Resource Allocation Rules:
- Target 80% memory utilization across GPUs
- Batch size limits: 1-64 sequences 
- 20% overhead allocation for system operations
- 5% per-sequence model size estimation
- Automatic recovery triggers at 95% utilization

Critical Processes:
1. Memory allocation and model split calculations
2. Framework-specific command generation
3. Resource optimization and monitoring
4. Batch size and context window optimization
5. GPU layer distribution logic

Business Importance Scores:
- Command Generation Engine: 95
- Memory Management System: 90
- Resource Allocation Rules: 85
- Framework Integration Logic: 80
- Process Management: 75

$END$