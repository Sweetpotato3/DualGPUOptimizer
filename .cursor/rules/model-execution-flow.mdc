---
description: Handles model execution flow optimization, command generation and GPU memory allocation for dual-GPU language model inference
globs: dualgpuopt/runner.py,dualgpuopt/gpu_info.py,dualgpuopt/ctx_size.py,dualgpuopt/optimizer.py
alwaysApply: false
---


# model-execution-flow

Core execution flow logic revolves around:

1. Command Generation Pipeline (dualgpuopt/optimizer.py)
- Framework-specific parameter generation:
  * llama.cpp: GPU split ratios and layer distributions
  * vLLM: Tensor parallel sizing and memory allocation
- Automatic optimization based on detected GPU capabilities:
  * Memory availability checks
  * Layer performance profiling
  * Split ratios based on relative GPU power
  * Tensor parallel configuration for different model architectures

2. Context Size Management (dualgpuopt/ctx_size.py)
- Model-specific window size calculation incorporating:
  * Layer count and dimensions
  * KV head configuration 
  * MoE architecture factors
  * Per-token memory requirements
  * Safety margins and reserved memory
- Token window validation against GPU memory constraints

3. Layer Distribution System (dualgpuopt/runner.py)  
- Performance-based layer distribution:
  * Profiling of layer execution times
  * Weighted scoring based on sequence lengths 
  * Quota-based allocation with reserve margins
  * Priority assignment for high-impact layers
- Dynamic rebalancing based on runtime metrics

4. Memory Split Optimization (dualgpuopt/optimizer.py)
- Optimal memory distribution calculations:
  * KV cache requirements per token
  * Tensor parallelism overhead allocation
  * System reserve margins
  * Model-specific safety buffers
- Split configurations proportional to GPU capabilities

Key Domain Rules:
- Layer distribution must respect per-GPU memory quotas
- Split ratios account for relative GPU performance differences
- Memory allocation includes safety margins for stability
- Token windows validated against available GPU memory

Importance Scores:
- Command Generation Pipeline: 95 (Core execution optimization)
- Context Size Management: 90 (Critical for model stability)
- Layer Distribution: 85 (Key performance optimization)
- Memory Split Optimization: 90 (Essential resource management)

$END$