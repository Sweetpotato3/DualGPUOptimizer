---
description: Documents the flow of LLM model execution, GPU allocation, and command generation for llama.cpp and vLLM.
globs: **/gpu_info.py,**/optimizer.py,**/runner.py,**/gui.py
alwaysApply: false
---


# model-execution-flow

## Core Model Execution Components

### Command Generation Engine (Importance: 95)
`dual_gpu_optimizer/dualgpuopt/optimizer.py`
- Generates execution commands for llama.cpp and vLLM based on detected GPUs
- Creates environment files with GPU-specific configurations (CUDA_VISIBLE_DEVICES, NCCL_P2P_DISABLE)
- Calculates tensor fractions and GPU splits for optimal multi-GPU model distribution

### GPU Resource Management (Importance: 90)
`dual_gpu_optimizer/dualgpuopt/gpu_info.py`
- Detects available NVIDIA GPUs using NVML interface
- Monitors real-time GPU memory usage and device status
- Provides mock GPU data capability for testing environments

### Model Runner System (Importance: 85)
`dual_gpu_optimizer/dualgpuopt/runner.py`
- Controls background execution of model commands
- Manages real-time log output streaming
- Handles start/stop functionality for model processes

## Supporting Components

### Execution UI Controller (Importance: 75)
`dual_gpu_optimizer/dualgpuopt/gui.py`
- Manages model path and preset selection
- Provides context size adjustment interface
- Updates real-time GPU utilization charts
- Implements alert system for low GPU utilization

### Command Line Workflow (Importance: 70)
`dual_gpu_optimizer/dualgpuopt/__main__.py`
- Processes CLI arguments for model execution
- Generates optimization configurations
- Creates environment files for execution

$END$