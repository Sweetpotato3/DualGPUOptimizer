---
description: Technical specification for model execution flow including LLM command generation, configuration parameters and GPU resource allocation
globs: **/optimizer.py,**/launcher/**,**/batch/**,**/gpu/**,**/model_profiles.py
alwaysApply: false
---


# model-execution-flow

Core components managing model execution flow:

1. Model Launch Pipeline
- Command generation for multiple ML frameworks:
  ```python
  # llama.cpp command generation 
  def generate_llama_cpp_cmd(model_path, gpu_split, context_size):
    return f"llama.cpp --model {model_path} --gpu-split {gpu_split} --ctx-size {context_size}"
  
  # vLLM command generation
  def generate_vllm_cmd(model_path, tp_size, max_tokens):
    return f"vllm --model {model_path} --tensor-parallel-size {tp_size} --max-tokens {max_tokens}"
  ```
- Framework-specific parameter optimization:
  - Context length scaling based on GPU memory
  - Tensor parallel size for multi-GPU setups
  - Memory utilization limits per device

2. Batch Processing
- Length-aware sequence batching using SmartBatcher
- Implements back-pressure via queue depth limits
- Provides bucket policies for sequence grouping:
  - Power-of-two bucketing 
  - Token ratio bucketing
- OOM recovery with automatic cache clearing

3. Resource Allocation
- Calculates optimal GPU split ratios based on available memory
- Layer distribution optimization across multiple GPUs
- Memory reservation policies with fallback mechanisms

Key file paths:
```
dualgpuopt/
  optimizer.py - Core optimization logic
  launcher/
    launch_controller.py - Launch pipeline coordination  
    parameter_resolver.py - Parameter optimization
  batch/
    smart_batch.py - Batching implementation
  gpu/
    monitor.py - GPU resource monitoring
```

The system orchestrates model execution by:
1. Determining optimal configuration parameters
2. Generating framework-specific launch commands 
3. Managing batch processing and resource allocation
4. Monitoring execution and handling recovery

$END$