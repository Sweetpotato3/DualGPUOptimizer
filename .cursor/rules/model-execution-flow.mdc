---
description: Documents the execution flow and configuration of LLM models, focusing on command generation, memory allocation, and resource management.
globs: **/model/**,**/engine/**,**/gpu/**,**/memory/**
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized handling of LLM deployment across dual GPUs with focus on memory optimization and resource allocation.

## Command Generation
The launcher component generates framework-specific commands for model execution:

```python
# llama.cpp configuration
- GPU split ratios for memory allocation
- Layer distribution parameters
- Context size optimization

# vLLM configuration
- Tensor parallel configuration
- GPU device mapping
- Memory allocation strategy
```

## Memory Management Strategy
- Dynamic memory profiler tracks GPU allocation patterns during inference
- Implements sliding window analysis for leak detection
- Memory recovery with progressive strategy:
  - Cache clearing
  - Clock reset
  - Full reset
  - System command fallback

## Resource Allocation
- Calculates optimal tensor splits based on:
  - Available VRAM per GPU
  - Model architecture parameters
  - Context window requirements
  - Framework-specific overheads

## Process Lifecycle
1. Pre-execution:
   - VRAM reset
   - Memory pressure check
   - Resource allocation calculation

2. Execution:
   - Command generation with optimized parameters
   - Process launch with resource constraints
   - Output/error stream handling

3. Post-execution:
   - Memory cleanup
   - Resource release
   - Metrics collection

## Health Monitoring
- Real-time memory pressure monitoring
- Temperature threshold tracking
- Power consumption monitoring
- Automated recovery on resource exhaustion

## Key Files
- `dualgpuopt/engine/backend.py`: Framework-specific execution backends
- `dualgpuopt/memory/profiler.py`: Memory tracking and optimization
- `dualgpuopt/gpu_info.py`: Resource monitoring and allocation
- `dualgpuopt/runner.py`: Process execution management

$END$
