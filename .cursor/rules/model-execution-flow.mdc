---
description: Manages model execution workflows, command generation, and GPU resource allocation for LLM inference optimization
globs: **/gpu_info.py,**/optimizer.py,**/runner.py,**/telemetry.py,**/batch/*.py,**/commands/*.py
alwaysApply: false
---


# model-execution-flow

## Core Execution Components

### Command Generation
- Generates optimized execution commands for different model frameworks:
  ```python
  llama_command(model_path, ctx_size, gpu_split)  # For llama.cpp models
  vllm_command(model_path, tensor_parallel)       # For vLLM models
  ```
- Calculates optimal GPU splits based on available memory and model requirements
- Handles environment variable configuration for multi-GPU setups

### Batch Processing Engine
Located in `batch/smart_batch.py`:
- Implements length-aware dynamic batching for inference requests
- Uses configurable bucket policies (power-of-2, token ratio) for sequence batching
- Manages backlog limits to prevent memory overflow
- Handles GPU OOM errors with automatic retries

### GPU Resource Management
Located in `gpu_info.py`:
- Collects real-time GPU metrics (memory, utilization, temperature)
- Calculates tensor fractions for optimal model distribution
- Generates environment configurations for GPU visibility and communication

### Telemetry Pipeline
Located in `telemetry.py`:
- Streams real-time GPU performance metrics
- Implements middleware pattern for metrics processing
- Publishes metrics events for monitoring and optimization

### Model Execution Control
Located in `runner.py`:
- Manages model lifecycle (load, execute, unload)
- Handles framework-specific execution flows
- Coordinates GPU resource allocation
- Implements execution error recovery

## File Paths Summary
```
dual_gpu_optimizer/dualgpuopt/
  ├── optimizer.py      # Core optimization logic
  ├── runner.py        # Model execution management  
  ├── gpu_info.py      # GPU resource monitoring
  ├── telemetry.py     # Metrics collection
  └── batch/
      ├── smart_batch.py    # Request batching
      └── heuristics.py     # Batching policies
```

Importance Scores:
- Command Generation: 95 (Core business logic for model execution)
- Batch Processing: 90 (Critical for inference optimization)
- GPU Resource Management: 85 (Key for multi-GPU coordination)
- Telemetry Pipeline: 75 (Important for optimization decisions)
- Model Execution Control: 85 (Essential workflow management)

$END$