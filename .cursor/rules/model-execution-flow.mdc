---
description: Defines how machine learning models are executed across multiple GPUs, including command generation, configuration and resource allocation
globs: **/optimizer.py,**/runner.py,**/launcher.py,**/commands/*.py,**/batch/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow handles distributing and running machine learning models across multiple GPUs through several key components:

## Command Generation System (85)
File: dual_gpu_optimizer/dualgpuopt/commands/gpu_commands.py

Generates framework-specific commands for model execution:

1. llama.cpp Mode:
- Creates GPU split strings based on available memory ratios
- Configures context window sizes per GPU
- Sets thread count and batch size parameters

2. vLLM Mode: 
- Generates tensor parallel configurations
- Calculates GPU memory fraction allocations
- Sets environment variables for GPU visibility

## Batch Processing Engine (90)
File: dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py

Implements intelligent batching logic:
- Length-aware sequence bucketing using power-of-two strategy
- Automatic batch size adjustment based on GPU memory constraints
- Back-pressure handling with configurable queue depth limits
- OOM recovery through cache purging and retry mechanisms

## Resource Allocation Controller (95)
File: dual_gpu_optimizer/dualgpuopt/optimizer.py

Controls GPU resource distribution:
- Dynamically splits model layers across available GPUs
- Calculates optimal tensor parallel fractions
- Manages GPU memory allocation ratios
- Handles model-specific memory requirements
- Generates environment configurations for each framework

## Execution Flow Manager (85)
File: dual_gpu_optimizer/dualgpuopt/runner.py

Coordinates model execution:
- Initializes GPU contexts and CUDA streams
- Manages model loading and weight distribution
- Handles framework-specific execution paths
- Monitors GPU utilization and adjusts splits
- Provides graceful shutdown and cleanup

## Memory Management (80)
File: dual_gpu_optimizer/dualgpuopt/mpolicy.py

Implements memory policies:
- Mixed precision handling for different operations
- Context size calculations based on model architecture
- Memory safety buffers and threshold management
- Automatic tensor offloading when memory constrained

$END$