---
description: Documents LLM execution flow including command generation, configuration and GPU resource management
globs: dualgpuopt/**/*.py,dualgpuopt/commands/*.py,dualgpuopt/batch/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow centers around orchestrating LLM inference across multiple GPUs and frameworks.

Core Execution Components:

1. Command Generation System
- Framework-specific command builders:
  * llama.cpp: Generates GPU split commands with layer distribution
  * vLLM: Creates tensor parallel configurations
- Environment variable configuration for each framework
- Dynamic parameter generation based on model architecture:
  * Context length calculation 
  * Memory split ratios
  * GPU layer allocation

2. Resource Allocation Engine
- GPU memory allocation calculation:
  * Model size estimation
  * KV cache requirements per token
  * Safety buffer allocation (20%)
- Layer distribution optimization across GPUs:
  * Performance-based distribution
  * Memory constraint validation
  * Tensor parallel configuration

3. Model Execution Workflow
- Pre-execution configuration:
  * Environment setup with GPU assignments
  * Memory split validation
  * Process isolation setup
- Runtime monitoring:
  * Memory pressure tracking
  * OOM detection and recovery
  * Performance metrics collection
- Graceful termination handling with staged shutdown

Key File Paths:
```
dualgpuopt/commands/gpu_commands.py      # Command generation logic
dualgpuopt/batch/smart_batch.py          # Batch optimization system  
dualgpuopt/optimizer.py                  # Core GPU split optimization
dualgpuopt/memory/profiler.py           # Memory monitoring system
dualgpuopt/runner.py                    # Process execution management
```

Importance Scores:
- Command Generation: 95 (Critical for model execution)
- Resource Allocation: 90 (Core optimization logic) 
- Execution Workflow: 85 (Key runtime management)
- Memory Management: 80 (Supporting optimization)

$END$