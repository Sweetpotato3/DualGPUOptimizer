---
description: Specification for LLM model execution workflows, GPU resource allocation, and command generation for llama.cpp and vLLM
globs: **/optimizer.py,**/gpu_info.py,**/runner.py,**/simple_launcher.py
alwaysApply: false
---


# model-execution-flow

Core Model Execution Components:

1. GPU Resource Allocation (Importance: 95)
- Implements dual GPU memory splitting based on available VRAM ratios
- Calculates tensor fractions for parallel model distribution
- Supports dynamic GPU memory overrides through environment variables
- Enforces 90% memory utilization policy for vLLM deployments

2. Framework-Specific Command Generation (Importance: 90)
- Generates optimized commands for llama.cpp and vLLM deployments
- Calculates framework-specific parameters based on GPU configurations
- Handles tensor parallel size determination for vLLM
- Manages context size settings per model type

3. Model Deployment Workflow (Importance: 85)
- Validates minimum GPU requirements (2 GPUs required)
- Implements automatic layer distribution across available GPUs
- Manages preset model configurations for different AI models
- Supports mock mode for development/testing scenarios

Key Files:
- dual_gpu_optimizer/dualgpuopt/optimizer.py - Core optimization logic
- dual_gpu_optimizer/dualgpuopt/runner.py - Model execution handling
- dual_gpu_optimizer/dualgpuopt/gpu_info.py - GPU resource management
- simple_launcher.py - Framework command generation

Business Rules:
- Minimum 2 GPU requirement for model execution
- Memory splitting based on relative GPU capacities
- Automatic tensor parallel size calculation
- Framework-specific memory utilization policies
- Support for up to 8 GPUs in parallel configuration

$END$