---
description: Defines and documents how LLM model execution flows through the system including command generation and GPU allocation
globs: */gpu_info.py,*/optimizer.py,*/layer_balance.py,*/commands/*.py,*/batch/*.py
alwaysApply: false
---


# model-execution-flow

The system implements a sophisticated model execution workflow spanning multiple components:

## Command Generation (Importance: 95)
Located in `commands/gpu_commands.py`:

1. Framework-specific command builders:
- llama.cpp: Generates split configuration strings for GPU layer distribution
- vLLM: Creates tensor parallel deployment parameters
- Handles environment variable generation for GPU memory allocation

2. Parameter Generation:
- Context size calculation based on available GPU memory
- Tensor split ratios for multi-GPU deployment
- Framework-specific flags and options

## Layer Distribution (Importance: 90)
Located in `layer_balance.py`:

1. Adaptive layer redistribution using:
- Short sequence profiling (64 tokens) - 20% weight
- Long sequence profiling (1024 tokens) - 80% weight
- Per-GPU VRAM quota enforcement
- Dynamic device mapping based on execution times

2. Distribution formulas:
- Weighted execution time: final_time = 0.2 * short_seq + 0.8 * long_seq
- Memory-aware layer assignment per GPU
- MoE-specific handling for mixture models

## Batch Processing (Importance: 85)
Located in `batch/smart_batch.py`:

1. Length-aware sequence batching:
- Groups similar length sequences
- Maintains configurable backlog limits
- Implements OOM protection with auto-retry
- Token-based size management

2. Two bucketing strategies:
- Power-of-two for llama.cpp
- Token ratio for proportional sizing

## Resource Management (Importance: 80)
Located in `gpu_info.py`:

1. Memory allocation:
- 2GB default operational overhead
- Model-specific scaling factors
- Architecture-based parameter tuning
- Dynamic VRAM quota calculation

2. Resource monitoring:
- Real-time memory tracking
- Multi-GPU coordination
- Temperature and power monitoring
- Utilization metrics collection

The execution flow prioritizes optimal resource utilization while maintaining model performance through intelligent layer distribution and batching strategies.

$END$