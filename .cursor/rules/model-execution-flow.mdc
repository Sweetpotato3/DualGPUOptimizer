---
description: Handles model execution workflows for LLMs including command generation, resource allocation, and GPU optimization
globs: **/dualgpuopt/services/*.py,**/dualgpuopt/gui/launcher.py,**/dualgpuopt/commands/*.py
alwaysApply: false
---


# model-execution-flow

Core execution workflow components that manage LLM model deployments across dual GPU configurations:

1. Framework-Specific Command Generation System (dualgpuopt/gui/launcher.py)
- Generates optimized launch commands for:
  * llama.cpp: GPU split configuration with memory ratios
  * vLLM: Tensor parallel deployment with GPU count optimization
- Dynamically adjusts parameters based on:
  * Available GPU memory
  * Model context size requirements 
  * Framework-specific optimization constraints

Importance Score: 95 (Core model execution logic)

2. GPU Resource Allocation Flow (dualgpuopt/services/state_service.py)
- Manages tensor split ratios across available GPUs
- Tracks per-GPU memory allocation states
- Controls idle detection thresholds for reallocation
- Handles dynamic memory rebalancing during execution

Importance Score: 90 (Critical resource management)

3. Model Execution Event Pipeline (dualgpuopt/services/event_bus.py)
- Processes GPU-specific execution events:
  * Optimization adjustments
  * Memory utilization changes
  * Temperature/power thresholds
- Priority-based execution event handling

Importance Score: 85 (Key execution monitoring)

4. Smart Batching System (dualgpuopt/batch/smart_batch.py)
- Length-aware inference scheduling
- Token-based batch size management
- Automatic OOM recovery mechanisms
- Request grouping based on sequence lengths

Importance Score: 80 (Execution optimization)

Key Execution Rules:
1. GPU memory splits must account for model-specific requirements
2. Tensor parallelism configuration varies by framework
3. Execution commands include safety limits for GPU resources
4. Batch processing adjusts based on available GPU memory

$END$