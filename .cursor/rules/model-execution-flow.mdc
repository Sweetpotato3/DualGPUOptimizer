---
description: Documentation of LLM execution workflows, GPU command generation, configuration and resource allocation
globs: **/dualgpuopt/engine/**,**/dualgpuopt/batch/**,**/dualgpuopt/commands/**,**/dualgpuopt/telemetry.py
alwaysApply: false
---


# model-execution-flow

Core Execution Components:

1. Model Launch Orchestration
- Dedicated launch controller manages model execution across multiple GPUs 
- Framework-specific parameter optimization:
  * llama.cpp: GPU split string generation for memory distribution
  * vLLM: Tensor parallel size configuration and distribution
- Automatic resource allocation based on GPU availability and model requirements
- Handles process lifecycle with OOM recovery and cache management

2. Batch Processing System
- Length-aware sequence batching optimized for GPU memory constraints
- Dynamic batch size calculation based on:
  * Available GPU memory
  * Model architecture parameters
  * Token count limits (16384 per batch)
- Implements backpressure mechanism with configurable backlog limits
- Automatic recovery from OOM conditions with 25% batch reduction

3. GPU Command Generation 
- Framework-specific command templating:
  * llama.cpp: Split ratios, context size, thread allocation
  * vLLM: Tensor parallel config, memory allocation
- Model-specific parameter generation:
  * Mixtral: 60/40 split with distribution 0-15:GPU0, 16-31:GPU1
  * 70B models: 70/30 split with layer distribution 0-39:GPU0, 40-79:GPU1
  * 13B models: 50/50 split with even layer distribution
- Environment variable generation for cross-platform deployments

4. Resource Monitoring
- Real-time telemetry collection:
  * Memory utilization tracking
  * GPU temperature monitoring  
  * Power consumption analysis
  * PCIe bandwidth utilization
- Custom alerting thresholds:
  * Emergency: Memory ≥95%, Temperature ≥90°C
  * Critical: Memory ≥90%, Temperature ≥80°C
  * Warning: Memory ≥75%, Temperature ≥70°C

5. Memory Recovery System
- Tiered recovery strategy:
  * Cache clearing (least disruptive)
  * Batch size reduction
  * Memory offloading
  * Process termination (last resort)
- Automatic strategy escalation on failure
- GPU-specific monitoring and recovery logging

Key Implementation Paths:
- dualgpuopt/commands/gpu_commands.py: Command generation
- dualgpuopt/batch/smart_batch.py: Batch optimization
- dualgpuopt/telemetry.py: Resource monitoring
- dualgpuopt/memory/recovery.py: Memory management

Model execution focuses on optimizing GPU resource utilization while maintaining stable operation through sophisticated monitoring and recovery systems.

$END$