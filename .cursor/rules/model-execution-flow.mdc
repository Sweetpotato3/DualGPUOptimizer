---
description: Defines execution flow and coordination of LLM workloads across multiple GPUs, including memory management and hardware optimization
globs: 
alwaysApply: false
---


# model-execution-flow

The model execution flow implements several key components for orchestrating LLM inference across multiple GPUs:

## Command Generation (Importance: 95)
Location: dualgpuopt/optimizer.py

Generates specialized command configurations for different LLM frameworks:
- llama.cpp: GPU split parameters, memory allocation ratios
- vLLM: Tensor parallelism degree, per-GPU memory quotas

Key parameters managed:
- Context size allocation
- GPU memory splits
- Tensor parallel fractions

## Mixed Precision Policy (Importance: 90)
Location: dualgpuopt/mpolicy.py

Manages precision for different operation types:
- FP32: LayerNorm, Softmax, Residual adds
- FP16/BF16: Matrix multiplications
- INT8/FP4: Weight storage and lookup
- Custom dtype mapping for PyTorch ops

## Batching System (Importance: 85)
Location: dualgpuopt/batch/smart_batch.py

Implements length-aware sequence batching:
- Power-of-two bucketing strategy
- Token ratio bucketing with size ratios
- Back-pressure mechanism with configurable limits
- Automatic retry after OOM with cache purging

## Layer Distribution (Importance: 80) 
Location: dualgpuopt/layer_balance.py

Controls model layer placement:
- Weighted profiling (20% short / 80% long sequences)
- Memory quota enforcement
- Latency-based distribution
- MoE routing optimization

## Command Execution (Importance: 75)
Location: dualgpuopt/runner.py

Manages model execution lifecycle:
- Framework-specific launch sequences
- GPU resource allocation/deallocation
- Error handling and recovery
- Metric collection and monitoring

$END$