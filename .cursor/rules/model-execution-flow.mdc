---
description: Documents LLM execution workflows, command generation, and GPU resource allocation patterns
globs: **/optimizer.py,**/runner.py,**/gpu_commands.py,**/layer_balance.py
alwaysApply: false
---


# model-execution-flow

Core Execution Flow Components:

1. Framework-Specific Command Generation
- Generates optimized execution commands for:
  * llama.cpp with GPU split parameters and context size settings 
  * vLLM with tensor parallel configurations
  * Memory allocation specifications for each framework
- File: dualgpuopt/optimizer.py

2. Layer Distribution Logic 
- Implements weighted profiling approach for layer assignment:
  * 20% weight for short sequences (64 tokens)
  * 80% weight for long sequences (1024 tokens)
- Balances layers across GPUs based on:
  * Individual layer latency profiles
  * Available GPU memory quotas
  * Performance characteristics
- File: dualgpuopt/layer_balance.py

3. GPU Resource Allocation
- Calculates optimal memory splits:
  * Core tensor parallelism overhead (20%)
  * System overhead reservation (2GB)
  * KV cache estimation (2.0x factor)
  * Global safety margin (10%)
- File: dualgpuopt/optimizer.py

4. Model Execution Management
- Non-blocking subprocess management for LLM workloads
- Coordinates resource allocation between frameworks
- Implements startup/shutdown sequences
- Handles graceful process termination
- File: dualgpuopt/runner.py

The system prioritizes efficient distribution of LLM workloads across dual GPU setups while maintaining framework-specific optimizations and safety constraints.

Importance Scores:
- Command Generation: 95 (Core execution logic)
- Layer Distribution: 90 (Critical optimization)
- Resource Allocation: 85 (Key infrastructure)
- Execution Management: 80 (Essential orchestration)

$END$