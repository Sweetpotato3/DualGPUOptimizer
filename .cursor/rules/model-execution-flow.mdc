---
description: Documents the workflow and execution patterns for LLM inference across multiple GPUs, including command generation and resource allocation.
globs: 
alwaysApply: false
---


# model-execution-flow

The model execution flow consists of several key components:

## Command Generation System
Located in `dualgpuopt/optimizer.py`:

1. Framework-Specific Command Generation:
- llama.cpp: 
  - Generates GPU split parameters based on available VRAM
  - Sets tensor allocation across multiple GPUs
  - Configures context size and layer distribution
- vLLM:
  - Calculates tensor parallel size based on GPU count
  - Sets memory utilization thresholds
  - Configures batch size based on model architecture

2. Layer Distribution (`dualgpuopt/layer_balance.py`):
- Implements weighted performance profiling using 64 and 1024 token sequences
- Assigns layers based on performance quotas calculated from profiling results
- Creates contiguous layer blocks to minimize cross-device communication
- Handles architecture-specific placement rules for embeddings and output layers

3. Context Size Management (`dualgpuopt/ctx_size.py`):
- Calculates safe context lengths based on:
  - Model architecture parameters (layers, KV heads, dimensions)
  - Available GPU memory
  - MoE factors for mixture models
- Implements model-specific context size limits:
```python
llama2-7b: max_ctx = min(32768, available_mem / bytes_per_token)
llama2-13b: max_ctx = min(16384, available_mem / bytes_per_token)
mistral-7b: max_ctx = min(32768, available_mem / bytes_per_token)
mixtral-8x7b: max_ctx = min(32768, available_mem / bytes_per_token * moe_factor)
```

4. Resource Management (`dualgpuopt/memory_monitor.py`):
- Implements multi-stage VRAM management:
  1. Cache clearing
  2. Clock/compute mode reset
  3. Full memory cleanup
  4. System-level optimization
- Tracks per-model memory requirements:
  - Base memory footprint
  - Per-batch scaling factors
  - Token-level memory usage
  - Growth rate projections

5. Batch Processing (`dualgpuopt/batch/smart_batch.py`):
- Length-aware sequence batching
- Dynamic batch size calculation based on:
  - Available GPU memory
  - Model architecture constraints
  - Current memory pressure
- Implements backpressure mechanism:
  - Reduces batch size by 25% after OOM events
  - Gradually recovers after 5 successful batches

The execution flow prioritizes stable multi-GPU inference while preventing out-of-memory conditions through careful resource management and dynamic adjustments.

$END$