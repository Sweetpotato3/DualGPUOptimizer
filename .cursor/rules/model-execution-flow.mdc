---
description: System for managing and optimizing LLM execution workflows across multiple GPUs with dynamic resource allocation.
globs: **/*.py,**/*.json
alwaysApply: false
---


# model-execution-flow

Core model execution workflow consists of two parallel paths optimized for llama.cpp and vLLM frameworks:

1. Command Generation System
- Located in `dualgpuopt/optimizer.py`
- Generates framework-specific execution commands with:
  * GPU memory split configurations for llama.cpp
  * Tensor parallelism settings for vLLM
  * Dynamic context size allocation
  * Model path integration

2. Resource Allocation Flow
- Located in `dualgpuopt/runner.py`
- Calculates tensor fractions based on GPU memory capacity
- Handles process lifecycle for model execution
- Manages GPU memory distribution
- Implements fallback mechanisms for resource constraints

3. Framework-Specific Optimizations
- llama.cpp execution path:
  * Split ratios for memory distribution
  * Context size partitioning
  * Multi-GPU coordination
- vLLM execution path:
  * Tensor parallel configurations
  * Memory utilization parameters
  * GPU device mapping

4. Model Preset Management
- Located in `dualgpuopt/gui/optimizer_tab.py`
- Maintains predefined configurations for common model architectures
- Handles memory requirement calculations
- Manages framework compatibility mapping

Importance Scores:
- Command Generation System: 95 (Core business logic for model execution)
- Resource Allocation Flow: 90 (Critical GPU optimization logic)
- Framework-Specific Optimizations: 85 (Key integration components)
- Model Preset Management: 75 (Important workflow support)

$END$