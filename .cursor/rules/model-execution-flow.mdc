---
description: Documents how model execution is coordinated across GPUs, including command generation and resource allocation flows
globs: **/optimizer.py,**/runner.py,**/launcher*.py,**/commands/*.py,**/gpu/*.py
alwaysApply: false
---


# model-execution-flow

The model execution flow implements specialized logic for running large language models across multiple GPUs:

## Command Generation System
- Generates framework-specific commands for model execution:
  ```python
  # llama.cpp format
  ./main -m <model> --split {gpu_splits} --mul-mat-q --gpu-layers {layer_count}
  
  # vLLM format 
  python -m vllm.entrypoints.api_server --model <model> --tensor-parallel-size {gpu_count}
  ```

- Layer distribution calculation splits model across GPUs:
  ```python
  total_memory = sum(gpu.mem_free_mb for gpu in gpus)
  split = [int((gpu.mem_free_mb / total_memory) * 100) for gpu in gpus]
  ```

## Resource Management
- Memory split optimization assigns memory quotas based on:
  - Available VRAM per GPU
  - Model size and architecture
  - Context length requirements
  - Batch size configuration

- Dynamic adjustment of tensor parallel size based on:
  - GPU count
  - Per-GPU memory availability  
  - Model architecture constraints

## Model Launch Controller
- Validates hardware requirements:
  - Minimum 2GB free memory per GPU
  - Compatible GPU architectures
  - Valid model checkpoint path

- Handles framework-specific launch flows:
  - llama.cpp: Layer distribution, memory splits
  - vLLM: Tensor parallelism, memory efficiency

## Flow Orchestration 
1. Hardware validation and resource analysis
2. Memory quota calculation and GPU split determination
3. Framework-specific parameter generation
4. Launch command construction
5. Model execution monitoring

File paths containing core execution logic:
- dualgpuopt/optimizer.py
- dualgpuopt/runner.py
- dualgpuopt/commands/gpu_commands.py
- dualgpuopt/gui/launcher.py

$END$