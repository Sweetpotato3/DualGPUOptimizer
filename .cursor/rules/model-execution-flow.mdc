---
description: Handles model loading, GPU allocation, execution flow and resource management for large language model inference
globs: **/model/*.py,**/engine/*.py,**/gpu/*.py,**/optimizer*.py,**/vram*.py
alwaysApply: false
---


# model-execution-flow

Core Model Execution Components:

1. Engine Pool Management (dualgpuopt/engine/pool.py)
- Manages active model instances with LRU cache (2 model maximum)
- Implements health monitoring with 3-failure threshold 
- Auto-restarts failed model instances
- Tracks performance metrics and status
- Graceful model switching and unloading

2. Mixed Precision Execution (dualgpuopt/mpolicy.py)
- FP32 precision for critical operations (LayerNorm, softmax)
- Manages tensor parallelism across dual GPUs
- Context-aware autocast selection
- Adaptive gradient scaling

3. GPU Memory Split Logic (dualgpuopt/vram_reset.py)
- Four-tier memory reset strategy:
  * CACHE_ONLY: PyTorch cache clearing
  * CLOCK_RESET: GPU clock manipulation
  * FULL_RESET: Total memory reclamation
  * SYSTEM_CMD: OS-level recovery
- Platform-specific implementations
- Selective device targeting
- Memory reclamation tracking

4. Model Launch Workflow (dualgpuopt/optimizer.py)
- GPU memory split optimization with model-specific ratios:
  * 70B models: 70/30 split, 40 layers
  * Mixtral: 60/40 split, 16 layers
  * 13B models: 50/50 split, 20 layers
  * Standard: Single GPU, full layers
- Dynamic context size limits:
  * 70B: 4096 max
  * Mixtral: 8192 max  
  * 13B: 12288 max
  * Standard: 16384 max
- Command generation for:
  * llama.cpp deployment
  * vLLM tensor parallel setup

5. Smart Batch Processing (dualgpuopt/batch/smart_batch.py)
- Length-aware batch scheduling
- OOM recovery with 25% reduction
- 5-batch window for stability monitoring
- Gradual recovery up to 95% capacity
- 16,384 token batch limit
- 256 token sequence threshold

File paths containing core logic:
- dualgpuopt/engine/pool.py
- dualgpuopt/optimizer.py 
- dualgpuopt/vram_reset.py
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/mpolicy.py

$END$