---
<<<<<<< HEAD
description: Documents the execution flow and orchestration of language models across multiple GPUs, including command generation and resource allocation
globs: **/optimizer.py,**/runner.py,**/commands/*.py,**/batch/*.py
=======
description: Specification for model execution flows, command generation, and GPU resource allocation across llama.cpp and vLLM frameworks
globs: **/optimizer.py,**/runner.py,**/gpu_commands.py,**/launcher.py
>>>>>>> 199829b (Update documentation for DualGPUOptimizer to provide a high-level overview of GPU optimization and model inference systems. Organized content into key components: Core GPU Management, Model Optimization Engine, Command System, Monitoring Dashboard, and State Management. Enhanced glob patterns for improved file matching and clarified key implementation files, ensuring comprehensive coverage of system functionalities and integration points.)
alwaysApply: false
---


# model-execution-flow

<<<<<<< HEAD
## Core Execution Components

### Command Generation System
Located in `dualgpuopt/optimizer.py`:
- Generates framework-specific deployment commands:
  - llama.cpp: GPU split ratios and memory allocation parameters
  - vLLM: Tensor parallelism configuration and GPU distribution
- Implements dynamic parameter calculation based on available GPU memory
- Handles model-specific configuration (context size, layer distribution)

### Batch Processing Pipeline 
Located in `dualgpuopt/batch/smart_batch.py`:
- Length-aware sequence batching with dynamic sizing
- Implements backpressure mechanism for memory management
- Automatic retry logic for OOM conditions
- Token-based batch composition with configurable limits

### Model Runner Orchestration
Located in `dualgpuopt/runner.py`:
- Controls model lifecycle across multiple GPUs
- Manages process execution and monitoring
- Handles framework-specific launch configurations:
  - Memory splits for llama.cpp
  - Tensor parallel sizes for vLLM
- Real-time output monitoring with non-blocking queues

### Resource Allocation Flow
Located in `dualgpuopt/layer_balance.py`:
- Adaptive layer distribution based on GPU capabilities
- Dual-length profiling (64/1024 tokens) for balanced optimization
- Memory quota management with configurable reserve ratios
- Dynamic redistribution based on performance metrics

### GPU Command Pipeline
Located in `dualgpuopt/commands/gpu_commands.py`:
- Framework-specific command string generation
- Environment configuration for multi-GPU setups
- Tensor parallelism parameter calculation
- Model preset handling for common configurations

### Execution Safety Controls
- Minimum 2 GPU requirement enforcement
- 90% memory utilization cap for stability
- Automatic failover for hardware errors
- GPU capability validation before execution

Importance Scores:
- Command Generation: 95 (core optimization logic)
- Batch Processing: 90 (critical for execution efficiency)
- Runner Orchestration: 85 (key execution control)
- Resource Allocation: 90 (essential optimization)
- GPU Command Pipeline: 80 (framework integration)
- Safety Controls: 75 (operational stability)
=======
The core model execution flow consists of several key components:

## Command Generation System
- Generates specific execution commands for different ML frameworks:
  - llama.cpp commands with GPU split configurations and context size parameters
  - vLLM commands with tensor parallelism and resource allocation settings
- Handles environment variable generation for GPU configurations including CUDA device visibility and NCCL settings

## Framework-Specific Execution
- llama.cpp execution:
  - Dynamic GPU memory split calculation based on available VRAM
  - Tensor fraction computation for optimal layer distribution
  - Context size validation against GPU memory constraints

- vLLM execution:
  - Asynchronous batch processing with smart scheduling
  - Length-aware inference queuing system
  - Automatic retry logic for OOM conditions

## Resource Management
- GPU memory allocation flow:
  1. Calculate maximum safe context length based on model parameters
  2. Determine optimal GPU split ratios
  3. Configure tensor parallelism settings
  4. Apply resource constraints to command generation

## Model Launch Control
- Framework selection logic between llama.cpp and vLLM
- Dynamic command construction based on:
  - Selected framework requirements
  - Model path and parameters
  - GPU resource availability
- Process management for model execution with log capture

## Batch Processing System
- Smart batching logic based on sequence lengths
- Bucket policies for request grouping:
  - Power-of-two bucketing
  - Token-ratio based grouping
- Back-pressure mechanisms for queue depth control

Relevant files:
- `dual_gpu_optimizer/dualgpuopt/optimizer.py`
- `dual_gpu_optimizer/dualgpuopt/runner.py`
- `dual_gpu_optimizer/dualgpuopt/gpu_commands.py`
- `dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py`
- `dual_gpu_optimizer/dualgpuopt/gui/launcher.py`
>>>>>>> 199829b (Update documentation for DualGPUOptimizer to provide a high-level overview of GPU optimization and model inference systems. Organized content into key components: Core GPU Management, Model Optimization Engine, Command System, Monitoring Dashboard, and State Management. Enhanced glob patterns for improved file matching and clarified key implementation files, ensuring comprehensive coverage of system functionalities and integration points.)

$END$