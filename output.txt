
===== DualGPUOptimizer - LLM Workload Optimization v0.2.0 =====

Detecting GPUs...
INFO: Using mock GPU data instead of real hardware

Detected GPUs:
  GPU 0: NVIDIA GeForce RTX 3090 - 24576 MiB total, 20480 MiB free
  GPU 1: NVIDIA GeForce RTX 3080 - 10240 MiB total, 8192 MiB free

Recommended GPU Split: 24,10

Generated Commands:
  llama.cpp: ./main -m C:/models/llama-7b.gguf --gpu-split 24,10 --n-gpu-layers 999 --ctx-size 65536
  vLLM: python -m vllm.entrypoints.openai.api_server --model C:/models/llama-7b.gguf --dtype float16 --tensor-parallel-size 2 --gpu-memory-utilization 0.9
