# Training presets for French legal LLM fine-tuning

# A100 configuration for full training
a100_bf16_large:
  base_model: mistralai/Mistral-7B-Instruct-v0.2
  tokenizer: tokenizer_frqc
  dataset: datasets/qc_legal_clean.jsonl
  out: checkpoints/qc_legal
  epochs: 3
  bs: 4
  model_bytes: 14500000000 # fp16 checkpoint size (estimated)
  gpus:
    - memory_total: 40960 # A100 40 GB

# Configuration for dual GPU (8GB + 8GB) setup
dual_gpu_small:
  base_model: mistralai/Mistral-7B-Instruct-v0.2
  tokenizer: tokenizer_frqc
  dataset: datasets/qc_legal_clean.jsonl
  out: checkpoints/qc_legal_small
  epochs: 2
  bs: 1
  model_bytes: 14500000000
  gpus:
    - memory_total: 8192 # First 8GB GPU
    - memory_total: 8192 # Second 8GB GPU

# Configuration for single consumer GPU + CPU offload
rtx_cpu_offload:
  base_model: mistralai/Mistral-7B-Instruct-v0.2
  tokenizer: tokenizer_frqc
  dataset: datasets/qc_legal_clean.jsonl
  out: checkpoints/qc_legal_offload
  epochs: 2
  bs: 1
  model_bytes: 14500000000
  gpus:
    - memory_total: 8192 # RTX 3070 8GB
  cpu_offload: true
  disk_offload: true

# Smaller model configuration
small_model:
  base_model: mistralai/Mistral-7B-Instruct-v0.2
  tokenizer: tokenizer_frqc
  dataset: datasets/qc_legal_clean.jsonl
  out: checkpoints/qc_legal_small
  epochs: 3
  bs: 2
  model_bytes: 6500000000 # Using quantized model
  quantization: int8 # Apply int8 quantization
  gpus:
    - memory_total: 8192 # 8GB consumer GPU

# Auto-detecting heterogeneous GPU setup
auto_hetero:
  base_model: mistralai/Mistral-7B-Instruct-v0.2
  tokenizer: tokenizer_frqc
  dataset: datasets/qc_legal_clean.jsonl
  out: checkpoints/qc_legal_auto
  epochs: 2
  bs: 1 # will be **auto-scaled** by adaptive trainer
  model_bytes: 14500000000
  gpus: auto # ‚Üê magic word triggers detection
