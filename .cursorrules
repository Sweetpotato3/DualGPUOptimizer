
START SPECIFICATION:
---
description: Create high-level overview documentation for projects focused on GPU optimization and management, particularly those handling multi-GPU configurations and model execution workflows
globs: *.py,*.code-workspace
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system provides specialized GPU management and optimization for machine learning model execution across multiple GPUs. The system consists of three core business domains:

### GPU Resource Management (Importance: 95)
- GPU detection and telemetry collection through NVML integration
- Dynamic memory allocation calculations for multi-GPU workloads
- Idle state detection and resource optimization notifications
- Memory utilization tracking and threshold monitoring

### Model Execution Optimization (Importance: 90)
- Automated workload distribution across available GPUs
- Specialized command generation for llama.cpp and vLLM execution
- Tensor parallel size calculations for optimal model splitting
- Environment configuration generation for GPU-specific parameters

### Configuration and Monitoring (Importance: 75)
- GPU-specific color visualization for performance metrics
- Threshold-based alerting for memory and utilization
- Theme-based visualization of GPU metrics
- Mock GPU support for testing and demonstration

Key Integration Points:
1. GPU Information Retrieval -> Optimization Engine
2. Optimization Engine -> Command Generation
3. Telemetry Collection -> Idle Detection
4. Configuration Validation -> GPU Management

Core Workflow:
1. Detect available GPUs and collect capabilities
2. Calculate optimal workload distribution
3. Generate execution commands with GPU-specific parameters
4. Monitor performance and provide resource optimization alerts

$END$
END SPECIFICATION