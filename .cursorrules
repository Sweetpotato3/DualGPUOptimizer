
START SPECIFICATION:
---
description: Creates high-level documentation for dual GPU optimization systems focused on machine learning workload distribution and GPU resource management
globs: *.py,*.code-workspace
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The Dual GPU Optimizer manages GPU resources and optimizes workload distribution across multiple GPUs for machine learning applications. The system consists of several key business components:

### Core GPU Management (Importance: 95)
- Real-time GPU telemetry collection and monitoring
- Automated detection of GPU specifications and capabilities
- Dynamic workload distribution calculations based on GPU memory and utilization
- Located in: `dual_gpu_optimizer/dualgpuopt/gpu_info.py`, `dual_gpu_optimizer/dualgpuopt/telemetry.py`

### Optimization Engine (Importance: 90)
- Calculates optimal tensor fractions for GPU memory distribution
- Generates environment configurations for ML frameworks
- Produces optimized command-line arguments for model execution
- Located in: `dual_gpu_optimizer/dualgpuopt/optimizer.py`

### System Monitoring (Importance: 85)
- GPU idle detection with configurable thresholds
- System tray integration for real-time status monitoring 
- Utilization alerts when GPUs remain underutilized
- Located in: `dual_gpu_optimizer/dualgpuopt/tray.py`

### ML Framework Integration (Importance: 80)
- Specialized command generation for llama.cpp and vLLM
- GPU memory split calculations for optimal model deployment
- Dynamic tensor parallel size configuration
- Located in: `simple_launcher.py`

### Application Control Flow (Importance: 75)
- Dual-mode operation (CLI and GUI) for flexibility
- GPU configuration validation and requirement checking
- Environment variable management for framework compatibility
- Located in: `dual_gpu_optimizer/dualgpuopt/__main__.py`

$END$
END SPECIFICATION