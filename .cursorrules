
START SPECIFICATION:
---
description: Apply this documentation for high-level technical overviews of dual GPU optimization systems focused on machine learning model deployment and monitoring. Use when documenting systems that manage GPU resource allocation, model inference optimization, and real-time performance tracking.
globs: *.py,*.code-workspace
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system consists of three core business domains:

1. GPU Resource Optimization (Importance: 95)
- Intelligent memory splitting calculation for multi-GPU deployments
- Dynamic tensor distribution based on GPU capabilities
- Framework-specific configuration generation for llama.cpp and vLLM
- Adaptive layer balancing with quota-based workload distribution

Key files:
- `dual_gpu_optimizer/dualgpuopt/optimizer.py`
- `dual_gpu_optimizer/dualgpuopt/layer_balance.py`

2. GPU Performance Monitoring (Importance: 85)
- Real-time telemetry collection across multiple GPUs
- Comprehensive metrics tracking: utilization, memory, temperature, power
- Rolling 60-sample history window for trend analysis
- Event-driven metric distribution system

Key files:
- `dual_gpu_optimizer/dualgpuopt/telemetry.py`
- `dual_gpu_optimizer/dualgpuopt/metrics.py`

3. Model Deployment Management (Importance: 80)
- Context length optimization using GPU-aware heuristics
- Smart batching system with OOM recovery
- Overclocking management with safety constraints
- Framework-specific launch orchestration

Key files:
- `dual_gpu_optimizer/dualgpuopt/ctx_size.py`
- `dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py`

The system implements specialized optimization logic for large language model inference across dual GPU setups, with particular focus on:
- Memory allocation optimization
- Workload distribution
- Performance monitoring
- Resource utilization
- Safe overclocking management

Integration points between components are handled through an event bus system that coordinates GPU metric collection, optimization decisions, and deployment configurations.

$END$
END SPECIFICATION