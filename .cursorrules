
START SPECIFICATION:
---
description: Apply this overview documentation for dual GPU optimization systems that manage and coordinate machine learning model execution across multiple GPUs, including performance monitoring, resource allocation, and hardware state management.
globs: *.py
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system implements specialized GPU optimization for large language models through several key business components:

Core GPU Management:
- Layer balancing engine that distributes model layers across GPUs based on latency profiles and VRAM constraints
- Context size calculator that determines maximum safe sequence lengths considering model architecture and GPU memory
- Memory policy manager implementing mixed precision strategies while maintaining stability
- Overclocking management system with hardware-safe constraints and profile persistence

Resource Optimization:
- Smart batching system with length-aware scheduling and OOM recovery
- Real-time GPU telemetry tracking power usage, temperatures, and memory utilization
- Idle resource detection alerting on underutilized GPUs (threshold: 30% for 5+ minutes)

Model Execution:
- Framework-specific optimizers for llama.cpp and vLLM
- Split ratio computation for tensor distribution across GPUs
- Atomic overclocking operations with rollback capabilities
- GPU-specific overclocking profiles and safety constraints

Key files:
- `dualgpuopt/layer_balance.py`: Core layer distribution logic
- `dualgpuopt/ctx_size.py`: Context size calculation
- `dualgpuopt/batch/smart_batch.py`: Batching optimization
- `dualgpuopt/commands/gpu_commands.py`: GPU control operations

The system prioritizes optimal resource utilization for dual-GPU machine learning workloads while maintaining hardware safety and operational stability through configurable thresholds and failover mechanisms.

$END$
END SPECIFICATION