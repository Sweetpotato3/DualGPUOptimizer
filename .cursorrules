---
description: Create a high-level overview documentation for projects focused on GPU optimization and management, particularly when dealing with multi-GPU setups for machine learning workloads and model execution
globs: *.py,*.json
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer is a specialized application for managing and optimizing dual GPU setups, with core business functionality organized around three main areas:

## GPU Management and Monitoring (Importance: 95)

- Probes and validates GPU configurations through NVML integration
- Collects comprehensive GPU metrics including memory, utilization, PCIe throughput, power usage
- Implements continuous telemetry streaming for real-time GPU performance monitoring
- Provides mock GPU functionality for testing and development

Key files:

- `dualgpuopt/gpu_info.py`
- `dualgpuopt/telemetry.py`

## Model Optimization Engine (Importance: 90)

- Generates optimized GPU split configurations based on available GPU memory
- Creates environment configurations for CUDA and NCCL optimizations
- Produces framework-specific command strings for llama.cpp and vLLM
- Manages model presets and configurations for common ML models

Key files:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/gui/optimizer_tab.py`

## Execution Management (Importance: 85)

- Controls model execution across multiple GPUs
- Manages process lifecycle and logging for running models
- Provides real-time monitoring through an interactive dashboard
- Implements idle detection and resource optimization alerts

Key files:

- `dualgpuopt/gui/launcher.py`
- `dualgpuopt/gui/dashboard.py`
- `dualgpuopt/tray.py`

## Configuration and Theme Management (Importance: 75)

- Handles GPU-specific overclocking settings and persistence
- Manages application themes with support for multiple color schemes
- Maintains user preferences and GPU configurations across sessions

Key files:

- `dualgpuopt/gui/settings.py`
- `dualgpuopt/gui/theme.py`

The application integrates these components through a GUI interface that provides real-time monitoring, optimization controls, and model execution management, specifically designed for machine learning workloads on multi-GPU systems.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Create high-level technical documentation focused on GPU optimization, monitoring, and control logic, particularly for dual GPU systems with complex business workflows
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

## Core Business Logic

### GPU Optimization Engine

Key system for optimizing dual GPU performance through:

- Adaptive layer redistribution between GPUs based on performance profiles
- Context size calculation considering GPU memory and model parameters
- Dynamic mixed precision policies for optimized GPU memory usage
- Telemetry collection pipeline for real-time GPU metrics

### GPU Control and Management

Central system components:

- Overclocking control with safety checks and configuration persistence
- Fan speed management with automatic and manual control modes
- Power limit adjustments with validation
- Temperature monitoring and throttling protection

### Event-Driven Monitoring

Real-time monitoring architecture:

- GPU metrics collection including utilization, memory, temperature, power
- Historical data tracking for performance analysis
- Priority-based event dispatch system
- Idle detection and notification system

### Model Execution Optimization

Specialized logic for model deployment:

- Smart batch processing with length-aware scheduling
- GPU memory split calculations for optimal model distribution
- Framework-specific command generation (llama.cpp, vLLM)
- Environment configuration generation

### Dashboard System

Real-time visualization components:

- Multi-GPU metrics display with color-coded indicators
- Performance history graphs with trend analysis
- PCIe bandwidth monitoring and formatting
- Temperature and power threshold management

Critical Paths:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/layer_balance.py`
- `dualgpuopt/batch/smart_batch.py`
- `dualgpuopt/telemetry.py`

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Create a high-level overview for projects that require dual GPU optimization, specifically targeting machine learning workloads and real-time GPU monitoring. Apply when documentation needs to capture core business logic for managing GPU resources, model optimization, and memory tracking.
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer implements specialized GPU optimization and monitoring logic across several core components:

## GPU Telemetry System

Located in `dualgpuopt/telemetry.py`, the telemetry system provides:

- Real-time monitoring of dual GPU configurations with 60-second metric history
- Critical metrics tracking for utilization, memory pressure, temperature, power draw
- Alert classification system with emergency/critical/warning/normal states based on GPU health indicators
- GPU differentiation logic for high-end vs mid-range cards with specific monitoring profiles
- Auto-recovery mechanisms with exponential backoff for GPU monitoring failures

## Memory Management System

The memory monitoring and optimization system in `dualgpuopt/memory/profiler.py` handles:

- Real-time memory monitoring during LLM inference across dual GPUs
- Leak and spike detection with custom thresholds (5% sustained growth, 10% rapid growth)
- Token-to-memory correlation tracking for inference optimization
- Custom linear regression for memory growth trend analysis
- Memory pattern analysis for transformer model behavior

## GPU Optimization Engine

Core optimization logic in `dualgpuopt/optimizer.py`:

- GPU memory split calculation for optimal tensor distribution
- Model parameter validation specific to LLM architectures
- Framework-specific command generation (llama.cpp, vLLM)
- Dynamic context size calculation based on model architecture
- Built-in model preset configurations

## Layer Balancing System

Layer distribution logic in `dualgpuopt/layer_balance.py`:

- Weighted profiling using short/long sequences (20%/80% split)
- Block consolidation logic to minimize cross-GPU transitions
- Position-aware performance modeling for transformer layers
- Fallback strategies using estimated performance patterns
- Dynamic adjustment based on layer position and memory constraints

## Recovery Management

Error handling and recovery system in `dualgpuopt/error_handler/recovery.py`:

- Tiered GPU memory reclamation strategies from cache clearing to system-level reset
- Custom error categories for GPU operations with specific recovery paths
- Automatic fallback to mock data after consecutive failures
- Platform-specific memory reclamation techniques

The system focuses on optimizing large language model deployment across dual GPUs while providing comprehensive monitoring, error recovery, and memory management capabilities.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Generate a high-level business logic overview for dual GPU optimization applications focused on ML model deployment and monitoring. Apply when documenting the core system architecture and domain-specific implementations.
globs: *.py,*.md
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer implements specialized GPU optimization and monitoring for machine learning model deployment across two GPUs. The core business logic is organized into several key domains:

1. GPU Telemetry & Performance Monitoring

- Real-time monitoring of dual GPU setups with domain-specific alert thresholds:
  - Emergency: Memory ≥95%, Temperature ≥90°C
  - Critical: Memory ≥90%, Temperature ≥80°C
  - Warning: Memory ≥75%, Temperature ≥70°C
- Maintains 60-second rolling metrics history with specialized aggregation
Path: dualgpuopt/telemetry.py

2. Memory Analysis & Optimization

- Leak detection through sustained growth rate analysis
- Memory spike identification using linear regression
- Inference session memory retention tracking
- Adaptive memory timeline with 3600 sample limit
Path: dualgpuopt/memory/profiler.py

3. GPU Resource Distribution

- Model layer balancing across GPUs based on profiling
- GPU memory split calculation for optimal distribution
- Framework-specific command generation for llama.cpp and vLLM
Path: dualgpuopt/optimizer.py

4. Model Execution Management

- Intelligent model caching with LRU eviction
- Health monitoring with auto-restart capability  
- Process isolation for model execution
Path: dualgpuopt/engine/pool/core.py

5. Recovery System

- Progressive recovery with exponential backoff
- Mock data fallback after 3 consecutive failures
- Configurable recovery attempts via environment
Path: dualgpuopt/error_handler/recovery.py

The system provides comprehensive dual GPU optimization focusing on memory management, performance monitoring, and automated recovery mechanisms. The implementation emphasizes reliable model deployment while maintaining optimal resource utilization across both GPUs.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Generate high-level overview documentation for projects focused on GPU optimization and monitoring, especially those handling dual GPU configurations, memory management, and performance telemetry. This applies when documenting core business logic related to GPU workload distribution and performance tracking.
globs: *.py,*.cpp,*.h
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

Core Domain: GPU Optimization and Monitoring System

Key Business Components:

1. GPU Memory Management

- Advanced memory profiling across dual GPU configurations
- Real-time memory usage tracking with 60-second rolling window
- Specialized alert system with severity levels:
  - EMERGENCY: Memory ≥95%, Temperature ≥90°C
  - CRITICAL: Memory ≥90%, Temperature ≥80°C
  - WARNING: Memory ≥75%, Temperature ≥70°C
- Memory reclamation strategies for OOM prevention

2. Workload Distribution

- Layer balancing algorithms for transformer models
- Dynamic memory split optimization for dual GPUs
- Real-time workload distribution based on GPU capabilities
- Framework-specific command generation for llama.cpp and vLLM

3. Telemetry System

- Real-time GPU metrics collection and monitoring
- Custom metrics pipeline for dual GPU correlation
- Threshold-based alerting with configurable parameters
- Rolling metrics history with automatic pruning

4. Model Execution Management

- LRU caching system for model engines
- Automatic health monitoring with failure thresholds
- Progressive recovery with exponential backoff
- Framework-specific launch optimization

Core Files:

- dualgpuopt/memory/profiler.py: Memory profiling and leak detection
- dualgpuopt/telemetry.py: GPU metrics collection
- dualgpuopt/optimizer.py: Workload distribution
- dualgpuopt/layer_balance.py: Layer distribution algorithm
- dualgpuopt/engine/pool.py: Model execution management

The system focuses on optimizing GPU resource utilization through intelligent workload distribution, proactive memory management, and comprehensive performance monitoring. It handles specialized cases like transformer model layer distribution and framework-specific optimizations.

$END$
END SPECIFICATION

START SPECIFICATION:
---
description: Generate a high-level overview for the dualgpuopt project, focusing on GPU optimization and monitoring functionality while excluding standard utilities and framework-specific code. Apply when documenting key business logic organization.
globs: *.py,*.ini,*.cfg,*.toml
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer project implements a GPU resource management and optimization system centered around four core business domains:

1. GPU Telemetry Management (dualgpuopt/telemetry.py) [Score: 95]
- Real-time monitoring of dual GPU configurations
- Custom alert classification system based on memory, temperature, and power thresholds  
- Rolling 60-second metrics history with differential tracking
- Specialized GPU load characterization and mock data generation

2. Memory Profiling System (dualgpuopt/memory/profiler.py) [Score: 90]
- Session-based GPU memory tracking with inference-specific monitoring
- Custom leak detection algorithm using linear regression
- Per-inference memory pattern analysis with token correlation
- Memory timeline analysis with 20-point sliding window
- Event classification specific to ML model memory usage

3. Multi-GPU Management Interface (dualgpuopt/gui/main_app.py) [Score: 85]
- Implements dual GPU optimization interface with real-time metrics
- Adaptive UI scaling based on window dimensions
- Fault-tolerant tab management architecture
- Event bus coordination for GPU metrics and configurations

4. GPU Resource Monitoring (dualgpuopt/qt/app_window.py) [Score: 80]
- Custom thresholding system for GPU metrics
- Engine pool management with model caching
- Alert service with severity-based notifications
- Real-time cache statistics monitoring
- Mock mode for testing/development

The system integrates these components through:

- Event-driven architecture for real-time GPU metrics
- Centralized telemetry service for metrics aggregation
- Thread-safe memory management across GPUs  
- Fault tolerance with graceful degradation

Core file structure:

/dualgpuopt
  /memory/ - Memory profiling and optimization
  /gui/    - Multi-GPU management interface
  /qt/     - Qt-based monitoring components  
  /telemetry/ - GPU metrics collection

The business logic focuses on optimizing GPU resource utilization for machine learning workloads while providing comprehensive monitoring and alerting capabilities.

$END$
END SPECIFICATION