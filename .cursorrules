
START SPECIFICATION
---

description: Generate high-level overview documentation for GPU optimization and model inference systems, focusing on core business logic organizing GPU resource management, model execution, and telemetry collection
globs: *.py,*.code-workspace
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer system implements specialized GPU optimization and model inference capabilities through several key business components:

## Core GPU Management (Importance: 95)

- GPU discovery and configuration management for dual GPU setups
- Intelligent memory split calculations between GPUs
- GPU telemetry collection and monitoring system
- Mock GPU mode for testing/development
- Advanced error handling for GPU-specific failures

Key files:

- `dualgpuopt/gpu_info.py`
- `dualgpuopt/telemetry.py`

## Model Optimization Engine (Importance: 90)

- Context size optimization based on GPU memory
- Layer balancing across multiple GPUs
- Dynamic tensor distribution
- Mixed precision policy management
- Smart batching with length-aware scheduling

Key files:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/batch/smart_batch.py`

## Command System (Importance: 85)

- GPU-specific command generation for model execution
- Framework-specific optimizations for llama.cpp and vLLM
- Environment configuration generation
- Command history with undo capabilities

Key files:

- `dualgpuopt/commands/gpu_commands.py`

## Monitoring Dashboard (Importance: 80)

- Real-time GPU metrics visualization
- Historical utilization tracking
- Event-driven updates for GPU status
- Temperature and power monitoring

Key files:

- `dualgpuopt/gui/dashboard.py`
- `dualgpuopt/gui/event_dashboard.py`

## State Management (Importance: 75)

- Centralized configuration storage
- Event-driven state updates
- GPU settings persistence
- Theme and UI state management

Key files:

- `dualgpuopt/services/state_service.py`

The system uses an event-driven architecture to coordinate between components, with specialized services handling configuration, errors, and state management. The GUI provides multiple specialized views including an optimizer tab, launcher interface, and GPU monitoring dashboard.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Create high-level technical documentation focused on GPU optimization, monitoring, and control logic, particularly for dual GPU systems with complex business workflows
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

## Core Business Logic

### GPU Optimization Engine

Key system for optimizing dual GPU performance through:

- Adaptive layer redistribution between GPUs based on performance profiles
- Context size calculation considering GPU memory and model parameters
- Dynamic mixed precision policies for optimized GPU memory usage
- Telemetry collection pipeline for real-time GPU metrics

### GPU Control and Management

Central system components:

- Overclocking control with safety checks and configuration persistence
- Fan speed management with automatic and manual control modes
- Power limit adjustments with validation
- Temperature monitoring and throttling protection

### Event-Driven Monitoring

Real-time monitoring architecture:

- GPU metrics collection including utilization, memory, temperature, power
- Historical data tracking for performance analysis
- Priority-based event dispatch system
- Idle detection and notification system

### Model Execution Optimization

Specialized logic for model deployment:

- Smart batch processing with length-aware scheduling
- GPU memory split calculations for optimal model distribution
- Framework-specific command generation (llama.cpp, vLLM)
- Environment configuration generation

### Dashboard System

Real-time visualization components:

- Multi-GPU metrics display with color-coded indicators
- Performance history graphs with trend analysis
- PCIe bandwidth monitoring and formatting
- Temperature and power threshold management

Critical Paths:

- `dual_gpu_optimizer/dualgpuopt/optimizer.py`
- `dual_gpu_optimizer/dualgpuopt/layer_balance.py`
- `dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py`
- `dual_gpu_optimizer/dualgpuopt/telemetry.py`

$END$
END SPECIFICATION

START SPECIFICATION:
---
description: Create a high-level overview of systems that manage and optimize GPU resources, focusing on dual-GPU configurations and LLM deployment. Use when documenting core optimization logic, resource management, and GPU-specific algorithms.
globs: *.py,*.cpp,*.hpp
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system implements specialized GPU resource management and optimization for large language models across multiple GPUs.

Core Business Components:

1. GPU Memory Management System
- Heuristic algorithms for context length calculation
- Model parameter mapping for LLM architectures
- Architecture-specific memory scaling factors
- Safety reserves and operational overhead handling
- Located in: dual_gpu_optimizer/dualgpuopt/ctx_size.py

2. Layer Distribution Engine
- Adaptive latency-aware layer redistribution
- Weighted profiling methodology (20% short/80% long sequences)
- Dynamic device mapping based on execution times
- Located in: dual_gpu_optimizer/dualgpuopt/layer_balance.py

3. GPU Optimization Service
- Tensor parallel fraction calculations
- Split strategy implementation for multi-GPU deployments
- Model-specific memory configuration
- Located in: dual_gpu_optimizer/dualgpuopt/optimizer.py

4. Advanced Event System
- GPU-specific event hierarchy
- Metrics tracking for utilization, memory, temperature
- Priority-based event handling (LOW to CRITICAL)
- Located in: dual_gpu_optimizer/dualgpuopt/services/event_bus.py

5. Smart Batching System
- Length-aware sequence batching
- Memory-based batch size optimization
- Token-based workload distribution
- Located in: dual_gpu_optimizer/dualgpuopt/batch/smart_batch.py

6. Telemetry Collection
- Real-time GPU metrics monitoring
- Hardware-specific data collection
- Performance metric normalization
- Located in: dual_gpu_optimizer/dualgpuopt/telemetry.py

Key Integration Points:

1. Command Generation
- Framework-specific optimizations (llama.cpp, vLLM)
- GPU split configuration
- Memory allocation parameters
- Located in: dual_gpu_optimizer/dualgpuopt/commands/gpu_commands.py

2. State Management
- GPU configuration persistence
- Model parameter tracking
- Resource allocation state
- Located in: dual_gpu_optimizer/dualgpuopt/services/state_service.py

The system architecture centers around optimizing large language model deployment across multiple GPUs, with specialized handling of memory distribution, layer balancing, and resource monitoring.

$END$
END SPECIFICATION