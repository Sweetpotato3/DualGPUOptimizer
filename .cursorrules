
START SPECIFICATION:
---
description: Creates high-level overview documentation for GPU optimization systems focused on dual-GPU configurations and AI model deployment, specifically when dealing with GPU resource management and model deployment optimization
globs: *.py,*.code-workspace
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer implements specialized GPU resource management and optimization for AI model deployment across multiple GPUs.

Core Domain Components:

1. GPU Resource Management (Importance: 95)
- Multi-GPU visualization and monitoring system
- Historical GPU load tracking in 2-minute windows
- Dynamic GPU memory override capabilities
- Minimum 2 GPU requirement with support for up to 8 GPUs
- Real-time utilization monitoring with configurable thresholds

2. Model Deployment Optimization (Importance: 90)
- Preset configurations for AI models (e.g., Mixtral)
- Context size management and GPU split configurations
- Dual framework support: llama.cpp and vLLM
- Tensor parallelism calculations based on GPU memory ratios
- Framework-specific command generation with memory splits

3. Performance Monitoring System (Importance: 85)
- Real-time GPU metrics tracking via NVML
- PCIe bandwidth monitoring (RX/TX)
- Utilization alerts based on configurable thresholds
- Idle detection logic for resource optimization
- Continuous metric streaming system

4. Resource Optimization Engine (Importance: 80)
- Memory splitting algorithms for model distribution
- GPU-specific environment configurations
- Tensor fraction calculations for relative memory distribution
- Alert system for underutilized resources
- Custom parameter calculation for multi-GPU deployments

Key Business Rules:
- Dual GPU minimum requirement
- Memory utilization policy at 90% for vLLM
- Alert thresholds between 5-80%
- Alert duration settings: 60-900 seconds
- Idle detection threshold at 30% utilization

Critical Files:
- dual_gpu_optimizer/dualgpuopt/optimizer.py
- dual_gpu_optimizer/dualgpuopt/gpu_info.py
- dual_gpu_optimizer/dualgpuopt/telemetry.py

$END$
END SPECIFICATION