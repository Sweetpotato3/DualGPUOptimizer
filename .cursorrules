---
description: Create a high-level overview documentation for projects focused on GPU optimization and management, particularly when dealing with multi-GPU setups for machine learning workloads and model execution
globs: *.py,*.json
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer is a specialized application for managing and optimizing dual GPU setups, with core business functionality organized around three main areas:

## GPU Management and Monitoring (Importance: 95)
- Probes and validates GPU configurations through NVML integration
- Collects comprehensive GPU metrics including memory, utilization, PCIe throughput, power usage
- Implements continuous telemetry streaming for real-time GPU performance monitoring
- Provides mock GPU functionality for testing and development

Key files:
- `dualgpuopt/gpu_info.py`
- `dualgpuopt/telemetry.py`

## Model Optimization Engine (Importance: 90)
- Generates optimized GPU split configurations based on available GPU memory
- Creates environment configurations for CUDA and NCCL optimizations
- Produces framework-specific command strings for llama.cpp and vLLM
- Manages model presets and configurations for common ML models

Key files:
- `dualgpuopt/optimizer.py`
- `dualgpuopt/gui/optimizer_tab.py`

## Execution Management (Importance: 85)
- Controls model execution across multiple GPUs
- Manages process lifecycle and logging for running models
- Provides real-time monitoring through an interactive dashboard
- Implements idle detection and resource optimization alerts

Key files:
- `dualgpuopt/gui/launcher.py`
- `dualgpuopt/gui/dashboard.py`
- `dualgpuopt/tray.py`

## Configuration and Theme Management (Importance: 75)
- Handles GPU-specific overclocking settings and persistence
- Manages application themes with support for multiple color schemes
- Maintains user preferences and GPU configurations across sessions

Key files:
- `dualgpuopt/gui/settings.py`
- `dualgpuopt/gui/theme.py`

The application integrates these components through a GUI interface that provides real-time monitoring, optimization controls, and model execution management, specifically designed for machine learning workloads on multi-GPU systems.

$END$
END SPECIFICATION

START SPECIFICATION
---
description: Create high-level technical documentation focused on GPU optimization, monitoring, and control logic, particularly for dual GPU systems with complex business workflows
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

## Core Business Logic

### GPU Optimization Engine

Key system for optimizing dual GPU performance through:

- Adaptive layer redistribution between GPUs based on performance profiles
- Context size calculation considering GPU memory and model parameters
- Dynamic mixed precision policies for optimized GPU memory usage
- Telemetry collection pipeline for real-time GPU metrics

### GPU Control and Management

Central system components:

- Overclocking control with safety checks and configuration persistence
- Fan speed management with automatic and manual control modes
- Power limit adjustments with validation
- Temperature monitoring and throttling protection

### Event-Driven Monitoring

Real-time monitoring architecture:

- GPU metrics collection including utilization, memory, temperature, power
- Historical data tracking for performance analysis
- Priority-based event dispatch system
- Idle detection and notification system

### Model Execution Optimization

Specialized logic for model deployment:

- Smart batch processing with length-aware scheduling
- GPU memory split calculations for optimal model distribution
- Framework-specific command generation (llama.cpp, vLLM)
- Environment configuration generation

### Dashboard System

Real-time visualization components:

- Multi-GPU metrics display with color-coded indicators
- Performance history graphs with trend analysis
- PCIe bandwidth monitoring and formatting
- Temperature and power threshold management

Critical Paths:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/layer_balance.py`
- `dualgpuopt/batch/smart_batch.py`
- `dualgpuopt/telemetry.py`

$END$
END SPECIFICATION

START SPECIFICATION:
---
description: Create high-level business logic overview documentation for GPU optimization software focused on dual GPU configurations and machine learning model deployment
globs: *.py,*.cpp
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system implements specialized logic for managing and optimizing large language model inference across dual GPU configurations. The core business functionality is organized into four main domains:

## Memory Management & Profiling (Importance: 95)
Core component managing GPU memory allocation and optimization:
- Session-based memory tracking system for dual GPU inference
- Memory leak detection using linear regression with configurable thresholds
- Per-GPU metrics tracking for utilization, temperature, power draw
- Memory timeline analysis with categorized events (allocation, growth spikes, leaks)

Key files:
- dualgpuopt/memory/profiler.py
- dualgpuopt/memory/predictor.py

## Model Optimization Engine (Importance: 90)
Handles model distribution and execution across GPUs:
- Calculates optimal tensor splits based on GPU memory characteristics
- Dynamic layer balancing with profiling-based distribution
- Context size optimization considering model architecture and memory constraints
- Framework-specific command generation for llama.cpp and vLLM

Key files:
- dualgpuopt/optimizer.py
- dualgpuopt/layer_balance.py

## GPU Telemetry System (Importance: 85)
Real-time monitoring and metrics collection:
- Custom metrics collection pipeline for dual GPU configurations
- Event-based architecture for metric propagation
- Automatic fallback to mock data for testing/development
- Temperature and power threshold monitoring

Key files:
- dualgpuopt/telemetry.py
- dualgpuopt/gpu/monitor.py

## Smart Batching System (Importance: 80)
Optimizes workload distribution across GPUs:
- Length-aware batch scheduling for efficient GPU utilization
- Dynamic batch size adjustment based on memory pressure
- Token-based workload distribution with quota enforcement
- Automatic recovery from out-of-memory conditions

Key files:
- dualgpuopt/batch/smart_batch.py
- dualgpuopt/batch/heuristics.py

The system prioritizes efficient GPU resource utilization while providing comprehensive monitoring and optimization capabilities specifically designed for large language model inference across dual GPU configurations.

$END$
END SPECIFICATION
