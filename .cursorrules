---
description: Create a high-level overview documentation for projects focused on GPU optimization and management, particularly when dealing with multi-GPU setups for machine learning workloads and model execution
globs: *.py,*.json
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer is a specialized application for managing and optimizing dual GPU setups, with core business functionality organized around three main areas:

## GPU Management and Monitoring (Importance: 95)

- Probes and validates GPU configurations through NVML integration
- Collects comprehensive GPU metrics including memory, utilization, PCIe throughput, power usage
- Implements continuous telemetry streaming for real-time GPU performance monitoring
- Provides mock GPU functionality for testing and development

Key files:

- `dualgpuopt/gpu_info.py`
- `dualgpuopt/telemetry.py`

## Model Optimization Engine (Importance: 90)

- Generates optimized GPU split configurations based on available GPU memory
- Creates environment configurations for CUDA and NCCL optimizations
- Produces framework-specific command strings for llama.cpp and vLLM
- Manages model presets and configurations for common ML models

Key files:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/gui/optimizer_tab.py`

## Execution Management (Importance: 85)

- Controls model execution across multiple GPUs
- Manages process lifecycle and logging for running models
- Provides real-time monitoring through an interactive dashboard
- Implements idle detection and resource optimization alerts

Key files:

- `dualgpuopt/gui/launcher.py`
- `dualgpuopt/gui/dashboard.py`
- `dualgpuopt/tray.py`

## Configuration and Theme Management (Importance: 75)

- Handles GPU-specific overclocking settings and persistence
- Manages application themes with support for multiple color schemes
- Maintains user preferences and GPU configurations across sessions

Key files:

- `dualgpuopt/gui/settings.py`
- `dualgpuopt/gui/theme.py`

The application integrates these components through a GUI interface that provides real-time monitoring, optimization controls, and model execution management, specifically designed for machine learning workloads on multi-GPU systems.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Create high-level technical documentation focused on GPU optimization, monitoring, and control logic, particularly for dual GPU systems with complex business workflows
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

## Core Business Logic

### GPU Optimization Engine

Key system for optimizing dual GPU performance through:

- Adaptive layer redistribution between GPUs based on performance profiles
- Context size calculation considering GPU memory and model parameters
- Dynamic mixed precision policies for optimized GPU memory usage
- Telemetry collection pipeline for real-time GPU metrics

### GPU Control and Management

Central system components:

- Overclocking control with safety checks and configuration persistence
- Fan speed management with automatic and manual control modes
- Power limit adjustments with validation
- Temperature monitoring and throttling protection

### Event-Driven Monitoring

Real-time monitoring architecture:

- GPU metrics collection including utilization, memory, temperature, power
- Historical data tracking for performance analysis
- Priority-based event dispatch system
- Idle detection and notification system

### Model Execution Optimization

Specialized logic for model deployment:

- Smart batch processing with length-aware scheduling
- GPU memory split calculations for optimal model distribution
- Framework-specific command generation (llama.cpp, vLLM)
- Environment configuration generation

### Dashboard System

Real-time visualization components:

- Multi-GPU metrics display with color-coded indicators
- Performance history graphs with trend analysis
- PCIe bandwidth monitoring and formatting
- Temperature and power threshold management

Critical Paths:

- `dualgpuopt/optimizer.py`
- `dualgpuopt/layer_balance.py`
- `dualgpuopt/batch/smart_batch.py`
- `dualgpuopt/telemetry.py`

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Create a high-level overview for projects that require dual GPU optimization, specifically targeting machine learning workloads and real-time GPU monitoring. Apply when documentation needs to capture core business logic for managing GPU resources, model optimization, and memory tracking.
globs: *.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer implements specialized GPU optimization and monitoring logic across several core components:

## GPU Telemetry System

Located in `dualgpuopt/telemetry.py`, the telemetry system provides:

- Real-time monitoring of dual GPU configurations with 60-second metric history
- Critical metrics tracking for utilization, memory pressure, temperature, power draw
- Alert classification system with emergency/critical/warning/normal states based on GPU health indicators
- GPU differentiation logic for high-end vs mid-range cards with specific monitoring profiles
- Auto-recovery mechanisms with exponential backoff for GPU monitoring failures

## Memory Management System

The memory monitoring and optimization system in `dualgpuopt/memory/profiler.py` handles:

- Real-time memory monitoring during LLM inference across dual GPUs
- Leak and spike detection with custom thresholds (5% sustained growth, 10% rapid growth)
- Token-to-memory correlation tracking for inference optimization
- Custom linear regression for memory growth trend analysis
- Memory pattern analysis for transformer model behavior

## GPU Optimization Engine

Core optimization logic in `dualgpuopt/optimizer.py`:

- GPU memory split calculation for optimal tensor distribution
- Model parameter validation specific to LLM architectures
- Framework-specific command generation (llama.cpp, vLLM)
- Dynamic context size calculation based on model architecture
- Built-in model preset configurations

## Layer Balancing System

Layer distribution logic in `dualgpuopt/layer_balance.py`:

- Weighted profiling using short/long sequences (20%/80% split)
- Block consolidation logic to minimize cross-GPU transitions
- Position-aware performance modeling for transformer layers
- Fallback strategies using estimated performance patterns
- Dynamic adjustment based on layer position and memory constraints

## Recovery Management

Error handling and recovery system in `dualgpuopt/error_handler/recovery.py`:

- Tiered GPU memory reclamation strategies from cache clearing to system-level reset
- Custom error categories for GPU operations with specific recovery paths
- Automatic fallback to mock data after consecutive failures
- Platform-specific memory reclamation techniques

The system focuses on optimizing large language model deployment across dual GPUs while providing comprehensive monitoring, error recovery, and memory management capabilities.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Generate a high-level business logic overview for dual GPU optimization applications focused on ML model deployment and monitoring. Apply when documenting the core system architecture and domain-specific implementations.
globs: *.py,*.md
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer implements specialized GPU optimization and monitoring for machine learning model deployment across two GPUs. The core business logic is organized into several key domains:

1. GPU Telemetry & Performance Monitoring

- Real-time monitoring of dual GPU setups with domain-specific alert thresholds:
  - Emergency: Memory â¥95%, Temperature â¥90Â°C
  - Critical: Memory â¥90%, Temperature â¥80Â°C
  - Warning: Memory â¥75%, Temperature â¥70Â°C
- Maintains 60-second rolling metrics history with specialized aggregation
Path: dualgpuopt/telemetry.py

2. Memory Analysis & Optimization

- Leak detection through sustained growth rate analysis
- Memory spike identification using linear regression
- Inference session memory retention tracking
- Adaptive memory timeline with 3600 sample limit
Path: dualgpuopt/memory/profiler.py

3. GPU Resource Distribution

- Model layer balancing across GPUs based on profiling
- GPU memory split calculation for optimal distribution
- Framework-specific command generation for llama.cpp and vLLM
Path: dualgpuopt/optimizer.py

4. Model Execution Management

- Intelligent model caching with LRU eviction
- Health monitoring with auto-restart capability  
- Process isolation for model execution
Path: dualgpuopt/engine/pool/core.py

5. Recovery System

- Progressive recovery with exponential backoff
- Mock data fallback after 3 consecutive failures
- Configurable recovery attempts via environment
Path: dualgpuopt/error_handler/recovery.py

The system provides comprehensive dual GPU optimization focusing on memory management, performance monitoring, and automated recovery mechanisms. The implementation emphasizes reliable model deployment while maintaining optimal resource utilization across both GPUs.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Generate high-level overview documentation for projects focused on GPU optimization and monitoring, especially those handling dual GPU configurations, memory management, and performance telemetry. This applies when documenting core business logic related to GPU workload distribution and performance tracking.
globs: *.py,*.cpp,*.h
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

Core Domain: GPU Optimization and Monitoring System

Key Business Components:

1. GPU Memory Management

- Advanced memory profiling across dual GPU configurations
- Real-time memory usage tracking with 60-second rolling window
- Specialized alert system with severity levels:
  - EMERGENCY: Memory â¥95%, Temperature â¥90Â°C
  - CRITICAL: Memory â¥90%, Temperature â¥80Â°C
  - WARNING: Memory â¥75%, Temperature â¥70Â°C
- Memory reclamation strategies for OOM prevention

2. Workload Distribution

- Layer balancing algorithms for transformer models
- Dynamic memory split optimization for dual GPUs
- Real-time workload distribution based on GPU capabilities
- Framework-specific command generation for llama.cpp and vLLM

3. Telemetry System

- Real-time GPU metrics collection and monitoring
- Custom metrics pipeline for dual GPU correlation
- Threshold-based alerting with configurable parameters
- Rolling metrics history with automatic pruning

4. Model Execution Management

- LRU caching system for model engines
- Automatic health monitoring with failure thresholds
- Progressive recovery with exponential backoff
- Framework-specific launch optimization

Core Files:

- dualgpuopt/memory/profiler.py: Memory profiling and leak detection
- dualgpuopt/telemetry.py: GPU metrics collection
- dualgpuopt/optimizer.py: Workload distribution
- dualgpuopt/layer_balance.py: Layer distribution algorithm
- dualgpuopt/engine/pool.py: Model execution management

The system focuses on optimizing GPU resource utilization through intelligent workload distribution, proactive memory management, and comprehensive performance monitoring. It handles specialized cases like transformer model layer distribution and framework-specific optimizations.

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Generate high-level overview documentation focused on DualGPUOptimizer's core business logic, organization and key systems. Applies when documenting project-level architecture and critical business workflows.
globs: *.py,*.md
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

DualGPUOptimizer System Architecture

Core Business Systems:

1. GPU Metrics & Telemetry

- Comprehensive GPU performance monitoring
- Real-time metric collection: utilization, memory, temperature, power
- Custom AlertLevel system with four severity tiers
- 60-second rolling history with multi-GPU aggregation
- Automatic failover to mock data after 5 consecutive errors
- File: dualgpuopt/telemetry.py

2. Memory Profiling & Management

- Specialized dual-GPU memory profiling system
- Session-based memory tracking with token count correlation
- Critical event detection: leaks, spikes, allocation patterns
- Linear regression analysis for growth rate calculation
- Retention analysis with 10MB threshold for leak detection
- File: dualgpuopt/memory/profiler.py

3. Layer Distribution Optimization

- Dynamic layer allocation across dual GPUs
- Performance profiling with weighted sequence analysis
- Automated layer distribution based on GPU capabilities
- Memory quota enforcement with balancing algorithms
- File: dualgpuopt/layer_balance.py

4. Model Execution Management

- Framework-specific launch command generation
- Tensor parallel size optimization
- Dynamic batch size calculation
- Memory-based GPU split optimization
- File: dualgpuopt/commands/gpu_commands.py

5. State & Configuration Management

- Central application state management
- GPU overclock profile persistence
- Theme and UI customization settings
- Idle detection and power management
- File: dualgpuopt/services/state_service.py

Key Integration Points:

- GPU telemetry feeds into memory profiling system
- Layer distribution uses telemetry data for optimization
- State service coordinates settings across components
- Model execution integrates with layer distribution

Business Value:
The system provides comprehensive dual-GPU optimization through:

- Real-time performance monitoring and alerting
- Memory leak detection and prevention
- Intelligent workload distribution
- Framework-agnostic model execution
- Persistent configuration management

$END$
END SPECIFICATION

START SPECIFICATION
---

description: Apply this overview documentation when analyzing the main architectural structure and core business logic of a dual GPU optimization system. Use when documenting high-level project organization and key business components.
globs: *.py,src/*.py,dualgpuopt/**/*.py
alwaysApply: false
---

# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.

The DualGPUOptimizer implements specialized logic for managing and optimizing machine learning model deployment across multiple GPUs.

Core Business Components:

1. GPU Telemetry System (dualgpuopt/telemetry.py):

- Comprehensive GPU health monitoring with metrics collection
- Four-tier alert classification system (EMERGENCY, CRITICAL, WARNING, NORMAL)  
- Progressive recovery strategy for monitoring failures
- 60-second rolling metrics history with aggregate statistics

2. Memory Optimization (dualgpuopt/memory/):

- Memory event detection system for anomalies and leaks
- Inference session memory tracking and analysis
- Timeline-based memory pattern analysis
- Custom pattern recognition for common memory issues
- Dedicated memory profiling for dual GPU setups

3. Model Distribution System (dualgpuopt/optimizer.py):

- Dynamic GPU split calculation based on model architecture
- Memory-aware model layer distribution
- Framework-specific command generation
- Model size and parameter estimation
- Tensor parallel optimization for multi-GPU deployment

4. Launch Management (dualgpuopt/gui/launcher/):

- Model launch configuration generation
- Process monitoring and health checks
- OOM error detection and recovery
- Automatic model reloading on failures
- Framework-specific parameter optimization

Key Integration Points:

1. Telemetry Integration:

- Real-time GPU metrics collection
- Memory utilization tracking
- Temperature and power monitoring
- Performance data aggregation

2. Memory Management:

- Memory profiling and leak detection
- Inference optimization
- Pattern analysis and alerting
- Timeline tracking per GPU

3. Model Deployment:

- GPU split optimization
- Layer distribution
- Command generation
- Process management

The system focuses on efficient distribution and monitoring of machine learning models across dual GPU setups, with emphasis on memory optimization, telemetry collection, and automated recovery from failures.

$END$
END SPECIFICATION

START SPECIFICATION:
---
description: Create high-level overview documentation for software systems focused on GPU optimization, hardware monitoring, memory management, and machine learning model execution. This rule applies when documenting systems that handle GPU resource allocation, hardware telemetry, and model deployment across multiple GPUs.
globs: *.py,*.md
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer application implements specialized GPU monitoring, memory management, and model optimization across multiple GPUs.

## Core Architecture

### GPU Telemetry System (Score: 95)
File: dualgpuopt/telemetry.py
- Comprehensive GPU metrics monitoring with 4 severity levels (NORMAL, WARNING, CRITICAL, EMERGENCY)
- Rolling 60-second metrics history per GPU device
- Implements exponential backoff for NVML failures with mock data fallback
- Custom alert thresholds for memory, temperature, and power usage

### Memory Management (Score: 92)
File: dualgpuopt/memory/profiler.py
- Real-time memory leak detection using sliding window analysis
- Session-based memory tracking during model inference
- Delta analysis between inference starts/ends
- Memory pattern classification (ALLOCATION, DEALLOCATION, GROWTH_SPIKE, LEAK_DETECTED)

### Model Optimization Engine (Score: 90)
File: dualgpuopt/optimizer.py
- Custom algorithm for calculating optimal memory distribution across dual GPUs
- Model-specific parameter mapping and memory profiling
- Framework-specific command generation for llama.cpp and vLLM
- Tensor parallelism optimization for multi-GPU setups

### Layer Distribution System (Score: 88)
File: dualgpuopt/layer_balance.py
- Adaptive latency-aware layer distribution across GPUs
- Two-phase profiling approach with weighted sequence lengths
- Memory quota-aware distribution with performance optimization
- Dynamic block merging for contiguous layer arrangements

### Engine Pool Management (Score: 85)
File: dualgpuopt/engine/pool.py
- LRU caching for model instances with health monitoring
- Automatic model recovery and reloading on health check failures
- Backend-specific health check implementations
- Dynamic metrics recording and performance tracking

### Legal Document Processing (Score: 82)
File: dualgpuopt/serve/legal_api.py
- Quebec-French legal prompt formatting with context injection
- RAG integration for legal document retrieval
- Legal context template management
- Domain-specific response streaming for legal content

### Smart Batching System (Score: 80)
File: dualgpuopt/batch/smart_batch.py
- Length-aware sequence batching for legal text processing
- Token-based backpressure system
- Model-specific memory optimization
- OOM recovery for long legal documents

This architecture provides comprehensive GPU resource management and optimization for machine learning model deployment, with specialized components for legal document processing and memory management.

$END$
END SPECIFICATION