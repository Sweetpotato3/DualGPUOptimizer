
START SPECIFICATION:
---
description: Apply when documenting high-level architecture and business logic organization for GPU optimization and model execution systems, particularly those handling dual GPU configurations and LLM workloads
globs: *.py,*.code-workspace
alwaysApply: false
---


# main-overview

## Development Guidelines

- Only modify code directly relevant to the specific request. Avoid changing unrelated functionality.
- Never replace code with placeholders like `# ... rest of the processing ...`. Always include complete code.
- Break problems into smaller steps. Think through each step separately before implementing.
- Always provide a complete PLAN with REASONING based on evidence from code and logs before making changes.
- Explain your OBSERVATIONS clearly, then provide REASONING to identify the exact issue. Add console logs when needed to gather more information.


The DualGPUOptimizer system orchestrates Large Language Model (LLM) workloads across multiple GPUs through several core business components:

### GPU Management and Optimization (Importance: 95)
- GPU detection and information retrieval using NVML
- Mock GPU capabilities for testing and demonstration
- Intelligent GPU split calculation for optimal resource distribution
- Real-time GPU utilization monitoring and telemetry collection

### Model Execution Control (Importance: 90)
- Command generation for both llama.cpp and vLLM implementations
- Dynamic context size adjustment for GPU split optimization
- Environment file generation for model execution
- Background process management with real-time logging

### Resource Monitoring and Alerting (Importance: 85)
- Real-time GPU utilization visualization
- Configurable alert thresholds for low GPU utilization
- Continuous telemetry streaming for performance metrics
- System tray integration for resource monitoring

### Model Management (Importance: 80)
- Model path and preset handling
- Context size management for optimization
- Command generation based on model characteristics
- Integration with multiple model execution frameworks

Core Files:
```
dual_gpu_optimizer/dualgpuopt/
├── gpu_info.py      # GPU detection and information management
├── optimizer.py     # GPU split optimization and command generation
├── telemetry.py     # Real-time GPU monitoring
└── runner.py        # Model execution management
```

The system implements a comprehensive workflow for LLM execution optimization:
1. GPU detection and capability assessment
2. Resource split calculation based on model requirements
3. Command generation for optimal execution
4. Real-time monitoring and alerting
5. Automated resource management recommendations

$END$
END SPECIFICATION